{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica 2 - Estudiantes",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmilianoLS/DiTellaML/blob/main/Deep%20Learning/Practica_2_Estudiantes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZSWq2lIEnbB"
      },
      "source": [
        "# Práctica 2: Introducción a PyTorch\n",
        "\n",
        "## a) Calculando el gradiente mediante Autograd\n",
        "\n",
        "En primer lugar, vamos a calcular del gradiente para el perceptrón simple con función de activación sigmoidea que vimos en la teoría. Pero esta vez, en lugar de realizar manualmente el proceso de backpropagation, vamos a usar el módulo `autograd` de PyTorch.\n",
        "\n",
        "La función $f(x, w)$ a la cual queremos encontrarle el gradiente es:\n",
        "\n",
        "> $f(\\mathbf{x}, \\mathbf{w}) = \\frac{1}{1 + e^{-(w_0 x_0 + w_1 x_1 + w_2)}}$\n",
        "\n",
        "Definimos entonces la función utilizando `torch.tensor` (recordar usar el parámetro `requires_grad = True` para que PyTorch guarde los gradientes) y realizamos la pasada \"forward\" para los siguientes valores de x y w:\n",
        "\n",
        "> $\\mathbf{x} = (-1, -2)$\n",
        "\n",
        "> $\\mathbf{w} = (2, -3, -3)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UczyYh5Nj2u5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a814e922-0d82-4064-ea22-1ec1c3007730"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Declaro los tensores x y w\n",
        "x = torch.tensor([-1.,-2.], requires_grad = True)\n",
        "w = torch.tensor([2.,-3., -3.], requires_grad = True)\n",
        "\n",
        "# Defino la funcion sinusoide\n",
        "f = 1/(1 + torch.exp(-(w[0]*x[0] + w[1]*x[1] + w[2])))\n",
        "\n",
        "print(f)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7311, grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkrbxHMukzHQ"
      },
      "source": [
        "Ahora, utilizando la función `f.backward()` computamos los gradientes $\\frac{\\partial f}{ \\partial \\mathbf{x}}$ y $\\frac{\\partial f}{ \\partial \\mathbf{w}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q477bpjr77xp"
      },
      "source": [
        "f.backward()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXewlL_8YHMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "543d5aa1-7748-41c0-ccf2-0b06ce07a81a"
      },
      "source": [
        "print(\"Gradiente df/dx = \" + str(x.grad))\n",
        "print(\"Gradiente df/dw = \" + str(w.grad))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradiente df/dx = tensor([ 0.3932, -0.5898])\n",
            "Gradiente df/dw = tensor([-0.1966, -0.3932,  0.1966])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsXTQ9wJnK2j"
      },
      "source": [
        "## b) Minimizando una función con Gradient Descent\n",
        "\n",
        "Ahora, vamos a implementar usar el algorítmo de gradiente descendiente (utilizando Autograd para computar el gradiente) para minimizar la función cuadrática $$f(x) = 2x^2 + x + 4$$\n",
        "\n",
        "Utilizaremos la implementación `torch.optim.SGD` de gradiente descendiente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKc75VsMYS4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa652e42-1e04-413b-e072-e473ec0efba9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Definir la variable que será el parámetro a optimizar\n",
        "x = torch.tensor([-3.], requires_grad = True)\n",
        "\n",
        "# Definir el optimizador, indicando el parámetro a optimizar y el learning rate\n",
        "optimizer = torch.optim.SGD([x], lr = 0.001)\n",
        "\n",
        "# Acumuladores que usaremos para guardar los valores sucesivos de x, y\n",
        "f_values = []\n",
        "x_values = []\n",
        "\n",
        "# Loop de optimización\n",
        "for i in range(1000):\n",
        "\n",
        "    # Setemos en 0 los gradientes de todos los elementos\n",
        "    optimizer.zero_grad()\n",
        "       \n",
        "    # Pasada forward: ejecutar la función a minimizar\n",
        "    f = 2 * x ** 2 + x + 4\n",
        "\n",
        "    print(\"X = \" + str(x) + \", f(x) = \" + str(f))\n",
        "\n",
        "    # Pasada backward: computar los gradientes\n",
        "    f.backward()\n",
        "\n",
        "    # Actualizar los pesos dando un paso de gradiente descendiente\n",
        "    optimizer.step()\n",
        "\n",
        "    # Guardar los valores para luego plotearlos\n",
        "    f_values.append(f.data.item())\n",
        "    x_values.append(x.data.item())\n",
        "\n",
        "# Ploteo los valores\n",
        "plt.title(\"Optimizando la función f = 2 * x**2 + x + 4\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.plot(x_values,f_values)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X = tensor([-3.], requires_grad=True), f(x) = tensor([19.], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9890], requires_grad=True), f(x) = tensor([18.8792], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9780], requires_grad=True), f(x) = tensor([18.7594], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9671], requires_grad=True), f(x) = tensor([18.6406], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9563], requires_grad=True), f(x) = tensor([18.5227], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9454], requires_grad=True), f(x) = tensor([18.4058], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9347], requires_grad=True), f(x) = tensor([18.2898], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9239], requires_grad=True), f(x) = tensor([18.1747], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9132], requires_grad=True), f(x) = tensor([18.0605], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9026], requires_grad=True), f(x) = tensor([17.9472], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8920], requires_grad=True), f(x) = tensor([17.8349], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8814], requires_grad=True), f(x) = tensor([17.7234], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8709], requires_grad=True), f(x) = tensor([17.6129], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8604], requires_grad=True), f(x) = tensor([17.5032], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8499], requires_grad=True), f(x) = tensor([17.3944], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8395], requires_grad=True), f(x) = tensor([17.2864], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8292], requires_grad=True), f(x) = tensor([17.1794], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8189], requires_grad=True), f(x) = tensor([17.0731], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8086], requires_grad=True), f(x) = tensor([16.9678], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7984], requires_grad=True), f(x) = tensor([16.8632], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7882], requires_grad=True), f(x) = tensor([16.7595], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7780], requires_grad=True), f(x) = tensor([16.6567], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7679], requires_grad=True), f(x) = tensor([16.5546], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7578], requires_grad=True), f(x) = tensor([16.4534], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7478], requires_grad=True), f(x) = tensor([16.3530], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7378], requires_grad=True), f(x) = tensor([16.2533], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7279], requires_grad=True), f(x) = tensor([16.1545], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7179], requires_grad=True), f(x) = tensor([16.0565], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7081], requires_grad=True), f(x) = tensor([15.9592], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6982], requires_grad=True), f(x) = tensor([15.8627], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6884], requires_grad=True), f(x) = tensor([15.7670], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6787], requires_grad=True), f(x) = tensor([15.6721], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6690], requires_grad=True), f(x) = tensor([15.5779], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6593], requires_grad=True), f(x) = tensor([15.4845], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6497], requires_grad=True), f(x) = tensor([15.3918], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6401], requires_grad=True), f(x) = tensor([15.2998], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6305], requires_grad=True), f(x) = tensor([15.2086], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6210], requires_grad=True), f(x) = tensor([15.1181], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6115], requires_grad=True), f(x) = tensor([15.0283], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6021], requires_grad=True), f(x) = tensor([14.9393], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5926], requires_grad=True), f(x) = tensor([14.8510], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5833], requires_grad=True), f(x) = tensor([14.7633], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5739], requires_grad=True), f(x) = tensor([14.6764], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5646], requires_grad=True), f(x) = tensor([14.5902], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5554], requires_grad=True), f(x) = tensor([14.5046], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5462], requires_grad=True), f(x) = tensor([14.4197], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5370], requires_grad=True), f(x) = tensor([14.3356], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5278], requires_grad=True), f(x) = tensor([14.2520], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5187], requires_grad=True), f(x) = tensor([14.1692], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5096], requires_grad=True), f(x) = tensor([14.0870], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5006], requires_grad=True), f(x) = tensor([14.0055], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4916], requires_grad=True), f(x) = tensor([13.9246], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4826], requires_grad=True), f(x) = tensor([13.8443], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4737], requires_grad=True), f(x) = tensor([13.7648], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4648], requires_grad=True), f(x) = tensor([13.6858], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4560], requires_grad=True), f(x) = tensor([13.6075], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4471], requires_grad=True), f(x) = tensor([13.5298], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4383], requires_grad=True), f(x) = tensor([13.4527], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4296], requires_grad=True), f(x) = tensor([13.3762], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4209], requires_grad=True), f(x) = tensor([13.3003], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4122], requires_grad=True), f(x) = tensor([13.2251], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4035], requires_grad=True), f(x) = tensor([13.1504], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3949], requires_grad=True), f(x) = tensor([13.0764], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3863], requires_grad=True), f(x) = tensor([13.0029], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3778], requires_grad=True), f(x) = tensor([12.9301], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3693], requires_grad=True), f(x) = tensor([12.8578], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3608], requires_grad=True), f(x) = tensor([12.7860], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3524], requires_grad=True), f(x) = tensor([12.7149], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3440], requires_grad=True), f(x) = tensor([12.6443], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3356], requires_grad=True), f(x) = tensor([12.5743], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3272], requires_grad=True), f(x) = tensor([12.5048], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3189], requires_grad=True), f(x) = tensor([12.4359], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3107], requires_grad=True), f(x) = tensor([12.3676], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3024], requires_grad=True), f(x) = tensor([12.2998], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2942], requires_grad=True), f(x) = tensor([12.2325], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2860], requires_grad=True), f(x) = tensor([12.1658], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2779], requires_grad=True), f(x) = tensor([12.0996], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2698], requires_grad=True), f(x) = tensor([12.0339], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2617], requires_grad=True), f(x) = tensor([11.9688], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2536], requires_grad=True), f(x) = tensor([11.9042], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2456], requires_grad=True), f(x) = tensor([11.8401], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2376], requires_grad=True), f(x) = tensor([11.7765], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2297], requires_grad=True), f(x) = tensor([11.7134], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2218], requires_grad=True), f(x) = tensor([11.6508], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2139], requires_grad=True), f(x) = tensor([11.5887], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2060], requires_grad=True), f(x) = tensor([11.5271], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1982], requires_grad=True), f(x) = tensor([11.4660], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1904], requires_grad=True), f(x) = tensor([11.4054], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1827], requires_grad=True), f(x) = tensor([11.3453], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1749], requires_grad=True), f(x) = tensor([11.2857], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1672], requires_grad=True), f(x) = tensor([11.2265], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1596], requires_grad=True), f(x) = tensor([11.1678], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1519], requires_grad=True), f(x) = tensor([11.1096], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1443], requires_grad=True), f(x) = tensor([11.0518], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1367], requires_grad=True), f(x) = tensor([10.9945], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1292], requires_grad=True), f(x) = tensor([10.9377], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1217], requires_grad=True), f(x) = tensor([10.8813], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1142], requires_grad=True), f(x) = tensor([10.8254], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1067], requires_grad=True), f(x) = tensor([10.7699], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0993], requires_grad=True), f(x) = tensor([10.7148], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0919], requires_grad=True), f(x) = tensor([10.6602], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0845], requires_grad=True), f(x) = tensor([10.6060], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0772], requires_grad=True), f(x) = tensor([10.5523], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0699], requires_grad=True), f(x) = tensor([10.4990], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0626], requires_grad=True), f(x) = tensor([10.4461], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0554], requires_grad=True), f(x) = tensor([10.3936], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0481], requires_grad=True), f(x) = tensor([10.3416], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0409], requires_grad=True), f(x) = tensor([10.2900], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0338], requires_grad=True), f(x) = tensor([10.2387], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0266], requires_grad=True), f(x) = tensor([10.1879], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0195], requires_grad=True), f(x) = tensor([10.1375], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0125], requires_grad=True), f(x) = tensor([10.0875], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0054], requires_grad=True), f(x) = tensor([10.0379], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9984], requires_grad=True), f(x) = tensor([9.9887], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9914], requires_grad=True), f(x) = tensor([9.9399], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9844], requires_grad=True), f(x) = tensor([9.8915], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9775], requires_grad=True), f(x) = tensor([9.8435], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9706], requires_grad=True), f(x) = tensor([9.7958], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9637], requires_grad=True), f(x) = tensor([9.7485], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9568], requires_grad=True), f(x) = tensor([9.7016], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9500], requires_grad=True), f(x) = tensor([9.6551], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9432], requires_grad=True), f(x) = tensor([9.6090], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9364], requires_grad=True), f(x) = tensor([9.5632], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9297], requires_grad=True), f(x) = tensor([9.5178], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9230], requires_grad=True), f(x) = tensor([9.4727], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9163], requires_grad=True), f(x) = tensor([9.4280], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9096], requires_grad=True), f(x) = tensor([9.3837], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9030], requires_grad=True), f(x) = tensor([9.3397], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8964], requires_grad=True), f(x) = tensor([9.2961], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8898], requires_grad=True), f(x) = tensor([9.2528], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8832], requires_grad=True), f(x) = tensor([9.2099], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8767], requires_grad=True), f(x) = tensor([9.1673], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8702], requires_grad=True), f(x) = tensor([9.1250], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8637], requires_grad=True), f(x) = tensor([9.0831], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8573], requires_grad=True), f(x) = tensor([9.0415], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8508], requires_grad=True), f(x) = tensor([9.0003], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8444], requires_grad=True), f(x) = tensor([8.9593], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8380], requires_grad=True), f(x) = tensor([8.9188], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8317], requires_grad=True), f(x) = tensor([8.8785], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8254], requires_grad=True), f(x) = tensor([8.8385], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8191], requires_grad=True), f(x) = tensor([8.7989], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8128], requires_grad=True), f(x) = tensor([8.7596], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8065], requires_grad=True), f(x) = tensor([8.7206], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8003], requires_grad=True), f(x) = tensor([8.6819], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7941], requires_grad=True), f(x) = tensor([8.6435], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7879], requires_grad=True), f(x) = tensor([8.6055], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7818], requires_grad=True), f(x) = tensor([8.5677], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7757], requires_grad=True), f(x) = tensor([8.5302], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7695], requires_grad=True), f(x) = tensor([8.4931], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7635], requires_grad=True), f(x) = tensor([8.4562], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7574], requires_grad=True), f(x) = tensor([8.4196], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7514], requires_grad=True), f(x) = tensor([8.3833], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7454], requires_grad=True), f(x) = tensor([8.3473], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7394], requires_grad=True), f(x) = tensor([8.3116], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7334], requires_grad=True), f(x) = tensor([8.2762], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7275], requires_grad=True), f(x) = tensor([8.2411], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7216], requires_grad=True), f(x) = tensor([8.2062], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7157], requires_grad=True), f(x) = tensor([8.1716], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7098], requires_grad=True), f(x) = tensor([8.1373], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7040], requires_grad=True), f(x) = tensor([8.1033], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6982], requires_grad=True), f(x) = tensor([8.0695], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6924], requires_grad=True), f(x) = tensor([8.0360], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6866], requires_grad=True), f(x) = tensor([8.0028], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6809], requires_grad=True), f(x) = tensor([7.9699], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6752], requires_grad=True), f(x) = tensor([7.9372], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6695], requires_grad=True), f(x) = tensor([7.9047], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6638], requires_grad=True), f(x) = tensor([7.8726], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6581], requires_grad=True), f(x) = tensor([7.8406], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6525], requires_grad=True), f(x) = tensor([7.8090], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6469], requires_grad=True), f(x) = tensor([7.7776], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6413], requires_grad=True), f(x) = tensor([7.7464], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6357], requires_grad=True), f(x) = tensor([7.7155], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6302], requires_grad=True), f(x) = tensor([7.6848], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6247], requires_grad=True), f(x) = tensor([7.6544], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6192], requires_grad=True), f(x) = tensor([7.6243], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6137], requires_grad=True), f(x) = tensor([7.5943], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6082], requires_grad=True), f(x) = tensor([7.5646], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6028], requires_grad=True), f(x) = tensor([7.5352], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5974], requires_grad=True), f(x) = tensor([7.5059], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5920], requires_grad=True), f(x) = tensor([7.4770], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5866], requires_grad=True), f(x) = tensor([7.4482], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5813], requires_grad=True), f(x) = tensor([7.4197], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5760], requires_grad=True), f(x) = tensor([7.3914], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5707], requires_grad=True), f(x) = tensor([7.3633], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5654], requires_grad=True), f(x) = tensor([7.3354], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5601], requires_grad=True), f(x) = tensor([7.3078], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5549], requires_grad=True), f(x) = tensor([7.2804], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5497], requires_grad=True), f(x) = tensor([7.2532], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5445], requires_grad=True), f(x) = tensor([7.2262], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5393], requires_grad=True), f(x) = tensor([7.1995], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5341], requires_grad=True), f(x) = tensor([7.1729], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5290], requires_grad=True), f(x) = tensor([7.1466], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5239], requires_grad=True), f(x) = tensor([7.1205], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5188], requires_grad=True), f(x) = tensor([7.0946], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5137], requires_grad=True), f(x) = tensor([7.0689], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5086], requires_grad=True), f(x) = tensor([7.0434], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5036], requires_grad=True), f(x) = tensor([7.0181], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4986], requires_grad=True), f(x) = tensor([6.9930], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4936], requires_grad=True), f(x) = tensor([6.9681], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4886], requires_grad=True), f(x) = tensor([6.9434], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4837], requires_grad=True), f(x) = tensor([6.9189], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4787], requires_grad=True), f(x) = tensor([6.8946], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4738], requires_grad=True), f(x) = tensor([6.8705], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4689], requires_grad=True), f(x) = tensor([6.8466], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4641], requires_grad=True), f(x) = tensor([6.8228], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4592], requires_grad=True), f(x) = tensor([6.7993], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4544], requires_grad=True), f(x) = tensor([6.7760], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4495], requires_grad=True), f(x) = tensor([6.7528], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4447], requires_grad=True), f(x) = tensor([6.7298], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4400], requires_grad=True), f(x) = tensor([6.7070], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4352], requires_grad=True), f(x) = tensor([6.6844], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4305], requires_grad=True), f(x) = tensor([6.6620], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4257], requires_grad=True), f(x) = tensor([6.6397], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4210], requires_grad=True), f(x) = tensor([6.6177], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4164], requires_grad=True), f(x) = tensor([6.5958], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4117], requires_grad=True), f(x) = tensor([6.5740], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4070], requires_grad=True), f(x) = tensor([6.5525], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4024], requires_grad=True), f(x) = tensor([6.5311], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3978], requires_grad=True), f(x) = tensor([6.5099], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3932], requires_grad=True), f(x) = tensor([6.4889], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3886], requires_grad=True), f(x) = tensor([6.4680], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3841], requires_grad=True), f(x) = tensor([6.4473], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3796], requires_grad=True), f(x) = tensor([6.4268], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3750], requires_grad=True), f(x) = tensor([6.4064], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3705], requires_grad=True), f(x) = tensor([6.3862], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3661], requires_grad=True), f(x) = tensor([6.3661], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3616], requires_grad=True), f(x) = tensor([6.3462], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3571], requires_grad=True), f(x) = tensor([6.3265], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3527], requires_grad=True), f(x) = tensor([6.3069], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3483], requires_grad=True), f(x) = tensor([6.2875], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3439], requires_grad=True), f(x) = tensor([6.2683], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3395], requires_grad=True), f(x) = tensor([6.2492], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3352], requires_grad=True), f(x) = tensor([6.2302], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3308], requires_grad=True), f(x) = tensor([6.2114], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3265], requires_grad=True), f(x) = tensor([6.1927], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3222], requires_grad=True), f(x) = tensor([6.1742], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3179], requires_grad=True), f(x) = tensor([6.1559], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3136], requires_grad=True), f(x) = tensor([6.1377], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3094], requires_grad=True), f(x) = tensor([6.1196], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3052], requires_grad=True), f(x) = tensor([6.1017], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3009], requires_grad=True), f(x) = tensor([6.0839], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2967], requires_grad=True), f(x) = tensor([6.0663], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2925], requires_grad=True), f(x) = tensor([6.0488], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2884], requires_grad=True), f(x) = tensor([6.0314], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2842], requires_grad=True), f(x) = tensor([6.0142], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2801], requires_grad=True), f(x) = tensor([5.9971], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2760], requires_grad=True), f(x) = tensor([5.9802], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2719], requires_grad=True), f(x) = tensor([5.9634], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2678], requires_grad=True), f(x) = tensor([5.9467], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2637], requires_grad=True), f(x) = tensor([5.9302], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2596], requires_grad=True), f(x) = tensor([5.9138], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2556], requires_grad=True), f(x) = tensor([5.8975], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2516], requires_grad=True), f(x) = tensor([5.8813], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2476], requires_grad=True), f(x) = tensor([5.8653], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2436], requires_grad=True), f(x) = tensor([5.8494], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2396], requires_grad=True), f(x) = tensor([5.8337], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2357], requires_grad=True), f(x) = tensor([5.8180], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2317], requires_grad=True), f(x) = tensor([5.8025], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2278], requires_grad=True), f(x) = tensor([5.7871], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2239], requires_grad=True), f(x) = tensor([5.7718], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2200], requires_grad=True), f(x) = tensor([5.7567], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2161], requires_grad=True), f(x) = tensor([5.7417], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2122], requires_grad=True), f(x) = tensor([5.7268], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2084], requires_grad=True), f(x) = tensor([5.7120], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2045], requires_grad=True), f(x) = tensor([5.6973], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2007], requires_grad=True), f(x) = tensor([5.6828], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1969], requires_grad=True), f(x) = tensor([5.6683], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1931], requires_grad=True), f(x) = tensor([5.6540], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1894], requires_grad=True), f(x) = tensor([5.6398], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1856], requires_grad=True), f(x) = tensor([5.6257], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1819], requires_grad=True), f(x) = tensor([5.6118], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1781], requires_grad=True), f(x) = tensor([5.5979], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1744], requires_grad=True), f(x) = tensor([5.5841], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1707], requires_grad=True), f(x) = tensor([5.5705], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1670], requires_grad=True), f(x) = tensor([5.5569], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1634], requires_grad=True), f(x) = tensor([5.5435], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1597], requires_grad=True), f(x) = tensor([5.5302], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1561], requires_grad=True), f(x) = tensor([5.5170], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1525], requires_grad=True), f(x) = tensor([5.5039], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1489], requires_grad=True), f(x) = tensor([5.4909], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1453], requires_grad=True), f(x) = tensor([5.4780], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1417], requires_grad=True), f(x) = tensor([5.4652], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1381], requires_grad=True), f(x) = tensor([5.4525], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1346], requires_grad=True), f(x) = tensor([5.4399], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1310], requires_grad=True), f(x) = tensor([5.4274], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1275], requires_grad=True), f(x) = tensor([5.4150], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1240], requires_grad=True), f(x) = tensor([5.4027], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1205], requires_grad=True), f(x) = tensor([5.3905], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1170], requires_grad=True), f(x) = tensor([5.3784], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1135], requires_grad=True), f(x) = tensor([5.3664], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1101], requires_grad=True), f(x) = tensor([5.3545], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1066], requires_grad=True), f(x) = tensor([5.3427], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1032], requires_grad=True), f(x) = tensor([5.3310], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0998], requires_grad=True), f(x) = tensor([5.3193], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0964], requires_grad=True), f(x) = tensor([5.3078], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0930], requires_grad=True), f(x) = tensor([5.2964], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0896], requires_grad=True), f(x) = tensor([5.2850], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0863], requires_grad=True), f(x) = tensor([5.2738], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0829], requires_grad=True), f(x) = tensor([5.2626], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0796], requires_grad=True), f(x) = tensor([5.2515], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0763], requires_grad=True), f(x) = tensor([5.2405], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0730], requires_grad=True), f(x) = tensor([5.2296], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0697], requires_grad=True), f(x) = tensor([5.2188], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0664], requires_grad=True), f(x) = tensor([5.2081], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0632], requires_grad=True), f(x) = tensor([5.1974], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0599], requires_grad=True), f(x) = tensor([5.1869], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0567], requires_grad=True), f(x) = tensor([5.1764], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0534], requires_grad=True), f(x) = tensor([5.1660], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0502], requires_grad=True), f(x) = tensor([5.1557], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0470], requires_grad=True), f(x) = tensor([5.1455], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0438], requires_grad=True), f(x) = tensor([5.1353], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0407], requires_grad=True), f(x) = tensor([5.1253], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0375], requires_grad=True), f(x) = tensor([5.1153], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0343], requires_grad=True), f(x) = tensor([5.1054], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0312], requires_grad=True), f(x) = tensor([5.0956], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0281], requires_grad=True), f(x) = tensor([5.0858], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0250], requires_grad=True), f(x) = tensor([5.0761], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0219], requires_grad=True), f(x) = tensor([5.0666], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0188], requires_grad=True), f(x) = tensor([5.0570], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0157], requires_grad=True), f(x) = tensor([5.0476], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0126], requires_grad=True), f(x) = tensor([5.0382], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0096], requires_grad=True), f(x) = tensor([5.0290], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0066], requires_grad=True), f(x) = tensor([5.0197], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0035], requires_grad=True), f(x) = tensor([5.0106], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0005], requires_grad=True), f(x) = tensor([5.0015], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9975], requires_grad=True), f(x) = tensor([4.9925], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9945], requires_grad=True), f(x) = tensor([4.9836], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9915], requires_grad=True), f(x) = tensor([4.9748], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9886], requires_grad=True), f(x) = tensor([4.9660], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9856], requires_grad=True), f(x) = tensor([4.9573], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9827], requires_grad=True), f(x) = tensor([4.9486], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9797], requires_grad=True), f(x) = tensor([4.9401], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9768], requires_grad=True), f(x) = tensor([4.9316], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9739], requires_grad=True), f(x) = tensor([4.9231], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9710], requires_grad=True), f(x) = tensor([4.9148], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9681], requires_grad=True), f(x) = tensor([4.9065], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9653], requires_grad=True), f(x) = tensor([4.8982], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9624], requires_grad=True), f(x) = tensor([4.8901], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9596], requires_grad=True), f(x) = tensor([4.8819], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9567], requires_grad=True), f(x) = tensor([4.8739], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9539], requires_grad=True), f(x) = tensor([4.8659], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9511], requires_grad=True), f(x) = tensor([4.8580], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9483], requires_grad=True), f(x) = tensor([4.8502], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9455], requires_grad=True), f(x) = tensor([4.8424], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9427], requires_grad=True), f(x) = tensor([4.8347], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9399], requires_grad=True), f(x) = tensor([4.8270], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9372], requires_grad=True), f(x) = tensor([4.8194], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9344], requires_grad=True), f(x) = tensor([4.8119], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9317], requires_grad=True), f(x) = tensor([4.8044], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9290], requires_grad=True), f(x) = tensor([4.7970], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9262], requires_grad=True), f(x) = tensor([4.7896], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9235], requires_grad=True), f(x) = tensor([4.7823], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9208], requires_grad=True), f(x) = tensor([4.7751], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9182], requires_grad=True), f(x) = tensor([4.7679], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9155], requires_grad=True), f(x) = tensor([4.7607], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9128], requires_grad=True), f(x) = tensor([4.7537], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9102], requires_grad=True), f(x) = tensor([4.7467], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9075], requires_grad=True), f(x) = tensor([4.7397], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9049], requires_grad=True), f(x) = tensor([4.7328], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9023], requires_grad=True), f(x) = tensor([4.7259], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8997], requires_grad=True), f(x) = tensor([4.7191], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8971], requires_grad=True), f(x) = tensor([4.7124], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8945], requires_grad=True), f(x) = tensor([4.7057], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8919], requires_grad=True), f(x) = tensor([4.6991], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8893], requires_grad=True), f(x) = tensor([4.6925], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8868], requires_grad=True), f(x) = tensor([4.6860], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8842], requires_grad=True), f(x) = tensor([4.6795], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8817], requires_grad=True), f(x) = tensor([4.6731], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8792], requires_grad=True), f(x) = tensor([4.6667], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8767], requires_grad=True), f(x) = tensor([4.6604], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8741], requires_grad=True), f(x) = tensor([4.6541], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8717], requires_grad=True), f(x) = tensor([4.6479], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8692], requires_grad=True), f(x) = tensor([4.6417], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8667], requires_grad=True), f(x) = tensor([4.6356], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8642], requires_grad=True), f(x) = tensor([4.6295], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8618], requires_grad=True), f(x) = tensor([4.6235], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8593], requires_grad=True), f(x) = tensor([4.6175], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8569], requires_grad=True), f(x) = tensor([4.6116], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8545], requires_grad=True), f(x) = tensor([4.6057], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8520], requires_grad=True), f(x) = tensor([4.5999], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8496], requires_grad=True), f(x) = tensor([4.5941], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8472], requires_grad=True), f(x) = tensor([4.5884], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8448], requires_grad=True), f(x) = tensor([4.5827], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8425], requires_grad=True), f(x) = tensor([4.5770], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8401], requires_grad=True), f(x) = tensor([4.5714], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8377], requires_grad=True), f(x) = tensor([4.5659], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8354], requires_grad=True), f(x) = tensor([4.5603], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8330], requires_grad=True), f(x) = tensor([4.5549], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8307], requires_grad=True), f(x) = tensor([4.5494], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8284], requires_grad=True), f(x) = tensor([4.5441], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8261], requires_grad=True), f(x) = tensor([4.5387], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8238], requires_grad=True), f(x) = tensor([4.5334], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8215], requires_grad=True), f(x) = tensor([4.5282], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8192], requires_grad=True), f(x) = tensor([4.5229], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8169], requires_grad=True), f(x) = tensor([4.5178], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8146], requires_grad=True), f(x) = tensor([4.5126], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8124], requires_grad=True), f(x) = tensor([4.5075], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8101], requires_grad=True), f(x) = tensor([4.5025], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8079], requires_grad=True), f(x) = tensor([4.4975], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8057], requires_grad=True), f(x) = tensor([4.4925], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8034], requires_grad=True), f(x) = tensor([4.4876], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8012], requires_grad=True), f(x) = tensor([4.4827], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7990], requires_grad=True), f(x) = tensor([4.4778], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7968], requires_grad=True), f(x) = tensor([4.4730], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7946], requires_grad=True), f(x) = tensor([4.4683], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7925], requires_grad=True), f(x) = tensor([4.4635], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7903], requires_grad=True), f(x) = tensor([4.4588], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7881], requires_grad=True), f(x) = tensor([4.4542], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7860], requires_grad=True), f(x) = tensor([4.4495], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7838], requires_grad=True), f(x) = tensor([4.4449], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7817], requires_grad=True), f(x) = tensor([4.4404], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7796], requires_grad=True), f(x) = tensor([4.4359], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7774], requires_grad=True), f(x) = tensor([4.4314], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7753], requires_grad=True), f(x) = tensor([4.4270], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7732], requires_grad=True), f(x) = tensor([4.4226], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7711], requires_grad=True), f(x) = tensor([4.4182], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7691], requires_grad=True), f(x) = tensor([4.4138], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7670], requires_grad=True), f(x) = tensor([4.4095], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7649], requires_grad=True), f(x) = tensor([4.4053], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7629], requires_grad=True), f(x) = tensor([4.4010], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7608], requires_grad=True), f(x) = tensor([4.3968], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7588], requires_grad=True), f(x) = tensor([4.3927], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7567], requires_grad=True), f(x) = tensor([4.3885], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7547], requires_grad=True), f(x) = tensor([4.3844], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7527], requires_grad=True), f(x) = tensor([4.3804], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7507], requires_grad=True), f(x) = tensor([4.3763], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7487], requires_grad=True), f(x) = tensor([4.3723], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7467], requires_grad=True), f(x) = tensor([4.3684], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7447], requires_grad=True), f(x) = tensor([4.3644], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7427], requires_grad=True), f(x) = tensor([4.3605], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7407], requires_grad=True), f(x) = tensor([4.3566], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7388], requires_grad=True), f(x) = tensor([4.3528], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7368], requires_grad=True), f(x) = tensor([4.3490], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7349], requires_grad=True), f(x) = tensor([4.3452], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7329], requires_grad=True), f(x) = tensor([4.3414], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7310], requires_grad=True), f(x) = tensor([4.3377], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7291], requires_grad=True), f(x) = tensor([4.3340], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7272], requires_grad=True), f(x) = tensor([4.3304], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7253], requires_grad=True), f(x) = tensor([4.3267], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7233], requires_grad=True), f(x) = tensor([4.3231], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7215], requires_grad=True), f(x) = tensor([4.3195], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7196], requires_grad=True), f(x) = tensor([4.3160], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7177], requires_grad=True), f(x) = tensor([4.3125], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7158], requires_grad=True), f(x) = tensor([4.3090], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7140], requires_grad=True), f(x) = tensor([4.3055], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7121], requires_grad=True), f(x) = tensor([4.3021], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7103], requires_grad=True), f(x) = tensor([4.2987], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7084], requires_grad=True), f(x) = tensor([4.2953], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7066], requires_grad=True), f(x) = tensor([4.2919], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7048], requires_grad=True), f(x) = tensor([4.2886], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7029], requires_grad=True), f(x) = tensor([4.2853], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7011], requires_grad=True), f(x) = tensor([4.2820], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6993], requires_grad=True), f(x) = tensor([4.2788], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6975], requires_grad=True), f(x) = tensor([4.2755], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6957], requires_grad=True), f(x) = tensor([4.2724], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6939], requires_grad=True), f(x) = tensor([4.2692], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6922], requires_grad=True), f(x) = tensor([4.2660], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6904], requires_grad=True), f(x) = tensor([4.2629], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6886], requires_grad=True), f(x) = tensor([4.2598], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6869], requires_grad=True), f(x) = tensor([4.2567], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6851], requires_grad=True), f(x) = tensor([4.2537], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6834], requires_grad=True), f(x) = tensor([4.2507], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6817], requires_grad=True), f(x) = tensor([4.2477], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6799], requires_grad=True), f(x) = tensor([4.2447], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6782], requires_grad=True), f(x) = tensor([4.2417], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6765], requires_grad=True), f(x) = tensor([4.2388], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6748], requires_grad=True), f(x) = tensor([4.2359], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6731], requires_grad=True), f(x) = tensor([4.2330], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6714], requires_grad=True), f(x) = tensor([4.2302], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6697], requires_grad=True), f(x) = tensor([4.2273], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6680], requires_grad=True), f(x) = tensor([4.2245], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6664], requires_grad=True), f(x) = tensor([4.2217], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6647], requires_grad=True), f(x) = tensor([4.2190], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6630], requires_grad=True), f(x) = tensor([4.2162], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6614], requires_grad=True), f(x) = tensor([4.2135], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6597], requires_grad=True), f(x) = tensor([4.2108], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6581], requires_grad=True), f(x) = tensor([4.2081], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6565], requires_grad=True), f(x) = tensor([4.2054], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6549], requires_grad=True), f(x) = tensor([4.2028], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6532], requires_grad=True), f(x) = tensor([4.2002], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6516], requires_grad=True), f(x) = tensor([4.1976], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6500], requires_grad=True), f(x) = tensor([4.1950], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6484], requires_grad=True), f(x) = tensor([4.1925], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6468], requires_grad=True), f(x) = tensor([4.1899], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6452], requires_grad=True), f(x) = tensor([4.1874], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6437], requires_grad=True), f(x) = tensor([4.1849], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6421], requires_grad=True), f(x) = tensor([4.1824], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6405], requires_grad=True), f(x) = tensor([4.1800], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6389], requires_grad=True), f(x) = tensor([4.1776], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6374], requires_grad=True), f(x) = tensor([4.1751], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6358], requires_grad=True), f(x) = tensor([4.1727], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6343], requires_grad=True), f(x) = tensor([4.1704], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6328], requires_grad=True), f(x) = tensor([4.1680], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6312], requires_grad=True), f(x) = tensor([4.1657], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6297], requires_grad=True), f(x) = tensor([4.1634], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6282], requires_grad=True), f(x) = tensor([4.1610], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6267], requires_grad=True), f(x) = tensor([4.1588], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6252], requires_grad=True), f(x) = tensor([4.1565], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6237], requires_grad=True), f(x) = tensor([4.1543], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6222], requires_grad=True), f(x) = tensor([4.1520], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6207], requires_grad=True), f(x) = tensor([4.1498], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6192], requires_grad=True), f(x) = tensor([4.1476], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6177], requires_grad=True), f(x) = tensor([4.1454], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6163], requires_grad=True), f(x) = tensor([4.1433], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6148], requires_grad=True), f(x) = tensor([4.1411], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6133], requires_grad=True), f(x) = tensor([4.1390], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6119], requires_grad=True), f(x) = tensor([4.1369], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6104], requires_grad=True), f(x) = tensor([4.1348], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6090], requires_grad=True), f(x) = tensor([4.1327], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6075], requires_grad=True), f(x) = tensor([4.1307], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6061], requires_grad=True), f(x) = tensor([4.1286], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6047], requires_grad=True), f(x) = tensor([4.1266], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6033], requires_grad=True), f(x) = tensor([4.1246], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6019], requires_grad=True), f(x) = tensor([4.1226], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6005], requires_grad=True), f(x) = tensor([4.1206], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5991], requires_grad=True), f(x) = tensor([4.1187], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5977], requires_grad=True), f(x) = tensor([4.1167], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5963], requires_grad=True), f(x) = tensor([4.1148], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5949], requires_grad=True), f(x) = tensor([4.1129], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5935], requires_grad=True), f(x) = tensor([4.1110], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5921], requires_grad=True), f(x) = tensor([4.1091], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5908], requires_grad=True), f(x) = tensor([4.1072], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5894], requires_grad=True), f(x) = tensor([4.1054], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5880], requires_grad=True), f(x) = tensor([4.1035], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5867], requires_grad=True), f(x) = tensor([4.1017], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5853], requires_grad=True), f(x) = tensor([4.0999], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5840], requires_grad=True), f(x) = tensor([4.0981], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5827], requires_grad=True), f(x) = tensor([4.0963], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5813], requires_grad=True), f(x) = tensor([4.0946], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5800], requires_grad=True), f(x) = tensor([4.0928], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5787], requires_grad=True), f(x) = tensor([4.0911], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5774], requires_grad=True), f(x) = tensor([4.0893], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5761], requires_grad=True), f(x) = tensor([4.0876], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5748], requires_grad=True), f(x) = tensor([4.0859], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5735], requires_grad=True), f(x) = tensor([4.0843], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5722], requires_grad=True), f(x) = tensor([4.0826], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5709], requires_grad=True), f(x) = tensor([4.0809], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5696], requires_grad=True), f(x) = tensor([4.0793], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5683], requires_grad=True), f(x) = tensor([4.0776], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5670], requires_grad=True), f(x) = tensor([4.0760], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5658], requires_grad=True), f(x) = tensor([4.0744], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5645], requires_grad=True), f(x) = tensor([4.0728], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5633], requires_grad=True), f(x) = tensor([4.0713], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5620], requires_grad=True), f(x) = tensor([4.0697], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5608], requires_grad=True), f(x) = tensor([4.0681], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5595], requires_grad=True), f(x) = tensor([4.0666], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5583], requires_grad=True), f(x) = tensor([4.0651], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5570], requires_grad=True), f(x) = tensor([4.0635], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5558], requires_grad=True), f(x) = tensor([4.0620], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5546], requires_grad=True), f(x) = tensor([4.0605], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5534], requires_grad=True), f(x) = tensor([4.0591], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5522], requires_grad=True), f(x) = tensor([4.0576], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5509], requires_grad=True), f(x) = tensor([4.0561], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5497], requires_grad=True), f(x) = tensor([4.0547], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5485], requires_grad=True), f(x) = tensor([4.0533], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5473], requires_grad=True), f(x) = tensor([4.0518], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5462], requires_grad=True), f(x) = tensor([4.0504], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5450], requires_grad=True), f(x) = tensor([4.0490], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5438], requires_grad=True), f(x) = tensor([4.0476], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5426], requires_grad=True), f(x) = tensor([4.0463], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5414], requires_grad=True), f(x) = tensor([4.0449], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5403], requires_grad=True), f(x) = tensor([4.0435], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5391], requires_grad=True), f(x) = tensor([4.0422], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5380], requires_grad=True), f(x) = tensor([4.0408], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5368], requires_grad=True), f(x) = tensor([4.0395], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5357], requires_grad=True), f(x) = tensor([4.0382], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5345], requires_grad=True), f(x) = tensor([4.0369], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5334], requires_grad=True), f(x) = tensor([4.0356], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5323], requires_grad=True), f(x) = tensor([4.0343], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5311], requires_grad=True), f(x) = tensor([4.0331], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5300], requires_grad=True), f(x) = tensor([4.0318], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5289], requires_grad=True), f(x) = tensor([4.0305], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5278], requires_grad=True), f(x) = tensor([4.0293], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5267], requires_grad=True), f(x) = tensor([4.0281], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5255], requires_grad=True), f(x) = tensor([4.0268], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5244], requires_grad=True), f(x) = tensor([4.0256], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5233], requires_grad=True), f(x) = tensor([4.0244], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5223], requires_grad=True), f(x) = tensor([4.0232], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5212], requires_grad=True), f(x) = tensor([4.0221], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5201], requires_grad=True), f(x) = tensor([4.0209], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5190], requires_grad=True), f(x) = tensor([4.0197], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5179], requires_grad=True), f(x) = tensor([4.0186], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5168], requires_grad=True), f(x) = tensor([4.0174], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5158], requires_grad=True), f(x) = tensor([4.0163], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5147], requires_grad=True), f(x) = tensor([4.0152], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5137], requires_grad=True), f(x) = tensor([4.0140], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5126], requires_grad=True), f(x) = tensor([4.0129], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5116], requires_grad=True), f(x) = tensor([4.0118], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5105], requires_grad=True), f(x) = tensor([4.0107], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5095], requires_grad=True), f(x) = tensor([4.0096], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5084], requires_grad=True), f(x) = tensor([4.0086], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5074], requires_grad=True), f(x) = tensor([4.0075], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5064], requires_grad=True), f(x) = tensor([4.0064], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5053], requires_grad=True), f(x) = tensor([4.0054], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5043], requires_grad=True), f(x) = tensor([4.0044], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5033], requires_grad=True), f(x) = tensor([4.0033], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5023], requires_grad=True), f(x) = tensor([4.0023], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5013], requires_grad=True), f(x) = tensor([4.0013], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5003], requires_grad=True), f(x) = tensor([4.0003], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4993], requires_grad=True), f(x) = tensor([3.9993], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4983], requires_grad=True), f(x) = tensor([3.9983], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4973], requires_grad=True), f(x) = tensor([3.9973], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4963], requires_grad=True), f(x) = tensor([3.9963], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4953], requires_grad=True), f(x) = tensor([3.9954], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4943], requires_grad=True), f(x) = tensor([3.9944], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4934], requires_grad=True), f(x) = tensor([3.9934], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4924], requires_grad=True), f(x) = tensor([3.9925], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4914], requires_grad=True), f(x) = tensor([3.9916], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4904], requires_grad=True), f(x) = tensor([3.9906], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4895], requires_grad=True), f(x) = tensor([3.9897], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4885], requires_grad=True), f(x) = tensor([3.9888], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4876], requires_grad=True), f(x) = tensor([3.9879], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4866], requires_grad=True), f(x) = tensor([3.9870], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4857], requires_grad=True), f(x) = tensor([3.9861], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4847], requires_grad=True), f(x) = tensor([3.9852], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4838], requires_grad=True), f(x) = tensor([3.9843], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4829], requires_grad=True), f(x) = tensor([3.9834], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4819], requires_grad=True), f(x) = tensor([3.9826], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4810], requires_grad=True), f(x) = tensor([3.9817], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4801], requires_grad=True), f(x) = tensor([3.9809], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4792], requires_grad=True), f(x) = tensor([3.9800], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4782], requires_grad=True), f(x) = tensor([3.9792], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4773], requires_grad=True), f(x) = tensor([3.9783], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4764], requires_grad=True), f(x) = tensor([3.9775], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4755], requires_grad=True), f(x) = tensor([3.9767], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4746], requires_grad=True), f(x) = tensor([3.9759], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4737], requires_grad=True), f(x) = tensor([3.9751], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4728], requires_grad=True), f(x) = tensor([3.9743], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4719], requires_grad=True), f(x) = tensor([3.9735], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4710], requires_grad=True), f(x) = tensor([3.9727], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4701], requires_grad=True), f(x) = tensor([3.9719], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4693], requires_grad=True), f(x) = tensor([3.9712], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4684], requires_grad=True), f(x) = tensor([3.9704], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4675], requires_grad=True), f(x) = tensor([3.9696], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4666], requires_grad=True), f(x) = tensor([3.9689], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4658], requires_grad=True), f(x) = tensor([3.9681], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4649], requires_grad=True), f(x) = tensor([3.9674], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4641], requires_grad=True), f(x) = tensor([3.9666], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4632], requires_grad=True), f(x) = tensor([3.9659], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4623], requires_grad=True), f(x) = tensor([3.9652], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4615], requires_grad=True), f(x) = tensor([3.9645], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4607], requires_grad=True), f(x) = tensor([3.9637], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4598], requires_grad=True), f(x) = tensor([3.9630], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4590], requires_grad=True), f(x) = tensor([3.9623], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4581], requires_grad=True), f(x) = tensor([3.9616], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4573], requires_grad=True), f(x) = tensor([3.9609], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4565], requires_grad=True), f(x) = tensor([3.9603], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4556], requires_grad=True), f(x) = tensor([3.9596], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4548], requires_grad=True), f(x) = tensor([3.9589], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4540], requires_grad=True), f(x) = tensor([3.9582], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4532], requires_grad=True), f(x) = tensor([3.9576], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4524], requires_grad=True), f(x) = tensor([3.9569], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4516], requires_grad=True), f(x) = tensor([3.9563], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4508], requires_grad=True), f(x) = tensor([3.9556], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4500], requires_grad=True), f(x) = tensor([3.9550], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4492], requires_grad=True), f(x) = tensor([3.9543], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4484], requires_grad=True), f(x) = tensor([3.9537], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4476], requires_grad=True), f(x) = tensor([3.9531], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4468], requires_grad=True), f(x) = tensor([3.9524], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4460], requires_grad=True), f(x) = tensor([3.9518], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4452], requires_grad=True), f(x) = tensor([3.9512], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4444], requires_grad=True), f(x) = tensor([3.9506], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4436], requires_grad=True), f(x) = tensor([3.9500], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4429], requires_grad=True), f(x) = tensor([3.9494], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4421], requires_grad=True), f(x) = tensor([3.9488], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4413], requires_grad=True), f(x) = tensor([3.9482], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4406], requires_grad=True), f(x) = tensor([3.9476], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4398], requires_grad=True), f(x) = tensor([3.9471], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4390], requires_grad=True), f(x) = tensor([3.9465], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4383], requires_grad=True), f(x) = tensor([3.9459], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4375], requires_grad=True), f(x) = tensor([3.9453], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4368], requires_grad=True), f(x) = tensor([3.9448], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4360], requires_grad=True), f(x) = tensor([3.9442], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4353], requires_grad=True), f(x) = tensor([3.9437], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4346], requires_grad=True), f(x) = tensor([3.9431], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4338], requires_grad=True), f(x) = tensor([3.9426], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4331], requires_grad=True), f(x) = tensor([3.9420], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4323], requires_grad=True), f(x) = tensor([3.9415], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4316], requires_grad=True), f(x) = tensor([3.9410], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4309], requires_grad=True), f(x) = tensor([3.9404], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4302], requires_grad=True), f(x) = tensor([3.9399], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4294], requires_grad=True), f(x) = tensor([3.9394], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4287], requires_grad=True), f(x) = tensor([3.9389], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4280], requires_grad=True), f(x) = tensor([3.9384], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4273], requires_grad=True), f(x) = tensor([3.9379], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4266], requires_grad=True), f(x) = tensor([3.9374], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4259], requires_grad=True), f(x) = tensor([3.9369], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4252], requires_grad=True), f(x) = tensor([3.9364], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4245], requires_grad=True), f(x) = tensor([3.9359], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4238], requires_grad=True), f(x) = tensor([3.9354], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4231], requires_grad=True), f(x) = tensor([3.9349], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4224], requires_grad=True), f(x) = tensor([3.9344], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4217], requires_grad=True), f(x) = tensor([3.9340], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4210], requires_grad=True), f(x) = tensor([3.9335], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4203], requires_grad=True), f(x) = tensor([3.9330], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4197], requires_grad=True), f(x) = tensor([3.9326], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4190], requires_grad=True), f(x) = tensor([3.9321], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4183], requires_grad=True), f(x) = tensor([3.9317], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4176], requires_grad=True), f(x) = tensor([3.9312], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4170], requires_grad=True), f(x) = tensor([3.9308], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4163], requires_grad=True), f(x) = tensor([3.9303], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4156], requires_grad=True), f(x) = tensor([3.9299], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4150], requires_grad=True), f(x) = tensor([3.9294], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4143], requires_grad=True), f(x) = tensor([3.9290], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4136], requires_grad=True), f(x) = tensor([3.9286], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4130], requires_grad=True), f(x) = tensor([3.9281], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4123], requires_grad=True), f(x) = tensor([3.9277], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4117], requires_grad=True), f(x) = tensor([3.9273], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4110], requires_grad=True), f(x) = tensor([3.9269], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4104], requires_grad=True), f(x) = tensor([3.9265], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4098], requires_grad=True), f(x) = tensor([3.9260], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4091], requires_grad=True), f(x) = tensor([3.9256], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4085], requires_grad=True), f(x) = tensor([3.9252], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4078], requires_grad=True), f(x) = tensor([3.9248], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4072], requires_grad=True), f(x) = tensor([3.9244], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4066], requires_grad=True), f(x) = tensor([3.9240], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4060], requires_grad=True), f(x) = tensor([3.9236], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4053], requires_grad=True), f(x) = tensor([3.9233], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4047], requires_grad=True), f(x) = tensor([3.9229], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4041], requires_grad=True), f(x) = tensor([3.9225], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4035], requires_grad=True), f(x) = tensor([3.9221], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4029], requires_grad=True), f(x) = tensor([3.9217], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4023], requires_grad=True), f(x) = tensor([3.9214], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4016], requires_grad=True), f(x) = tensor([3.9210], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4010], requires_grad=True), f(x) = tensor([3.9206], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4004], requires_grad=True), f(x) = tensor([3.9203], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3998], requires_grad=True), f(x) = tensor([3.9199], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3992], requires_grad=True), f(x) = tensor([3.9195], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3986], requires_grad=True), f(x) = tensor([3.9192], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3980], requires_grad=True), f(x) = tensor([3.9188], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3975], requires_grad=True), f(x) = tensor([3.9185], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3969], requires_grad=True), f(x) = tensor([3.9181], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3963], requires_grad=True), f(x) = tensor([3.9178], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3957], requires_grad=True), f(x) = tensor([3.9175], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3951], requires_grad=True), f(x) = tensor([3.9171], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3945], requires_grad=True), f(x) = tensor([3.9168], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3939], requires_grad=True), f(x) = tensor([3.9164], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3934], requires_grad=True), f(x) = tensor([3.9161], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3928], requires_grad=True), f(x) = tensor([3.9158], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3922], requires_grad=True), f(x) = tensor([3.9155], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3917], requires_grad=True), f(x) = tensor([3.9151], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3911], requires_grad=True), f(x) = tensor([3.9148], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3905], requires_grad=True), f(x) = tensor([3.9145], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3900], requires_grad=True), f(x) = tensor([3.9142], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3894], requires_grad=True), f(x) = tensor([3.9139], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3888], requires_grad=True), f(x) = tensor([3.9136], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3883], requires_grad=True), f(x) = tensor([3.9132], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3877], requires_grad=True), f(x) = tensor([3.9129], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3872], requires_grad=True), f(x) = tensor([3.9126], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3866], requires_grad=True), f(x) = tensor([3.9123], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3861], requires_grad=True), f(x) = tensor([3.9120], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3855], requires_grad=True), f(x) = tensor([3.9117], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3850], requires_grad=True), f(x) = tensor([3.9115], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3845], requires_grad=True), f(x) = tensor([3.9112], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3839], requires_grad=True), f(x) = tensor([3.9109], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3834], requires_grad=True), f(x) = tensor([3.9106], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3829], requires_grad=True), f(x) = tensor([3.9103], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3823], requires_grad=True), f(x) = tensor([3.9100], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3818], requires_grad=True), f(x) = tensor([3.9097], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3813], requires_grad=True), f(x) = tensor([3.9095], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3807], requires_grad=True), f(x) = tensor([3.9092], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3802], requires_grad=True), f(x) = tensor([3.9089], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3797], requires_grad=True), f(x) = tensor([3.9086], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3792], requires_grad=True), f(x) = tensor([3.9084], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3787], requires_grad=True), f(x) = tensor([3.9081], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3782], requires_grad=True), f(x) = tensor([3.9078], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3776], requires_grad=True), f(x) = tensor([3.9076], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3771], requires_grad=True), f(x) = tensor([3.9073], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3766], requires_grad=True), f(x) = tensor([3.9071], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3761], requires_grad=True), f(x) = tensor([3.9068], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3756], requires_grad=True), f(x) = tensor([3.9066], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3751], requires_grad=True), f(x) = tensor([3.9063], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3746], requires_grad=True), f(x) = tensor([3.9061], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3741], requires_grad=True), f(x) = tensor([3.9058], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3736], requires_grad=True), f(x) = tensor([3.9056], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3731], requires_grad=True), f(x) = tensor([3.9053], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3726], requires_grad=True), f(x) = tensor([3.9051], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3721], requires_grad=True), f(x) = tensor([3.9048], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3716], requires_grad=True), f(x) = tensor([3.9046], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3712], requires_grad=True), f(x) = tensor([3.9044], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3707], requires_grad=True), f(x) = tensor([3.9041], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3702], requires_grad=True), f(x) = tensor([3.9039], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3697], requires_grad=True), f(x) = tensor([3.9037], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3692], requires_grad=True), f(x) = tensor([3.9034], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3688], requires_grad=True), f(x) = tensor([3.9032], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3683], requires_grad=True), f(x) = tensor([3.9030], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3678], requires_grad=True), f(x) = tensor([3.9028], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3673], requires_grad=True), f(x) = tensor([3.9025], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3669], requires_grad=True), f(x) = tensor([3.9023], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3664], requires_grad=True), f(x) = tensor([3.9021], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3659], requires_grad=True), f(x) = tensor([3.9019], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3655], requires_grad=True), f(x) = tensor([3.9017], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3650], requires_grad=True), f(x) = tensor([3.9015], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3645], requires_grad=True), f(x) = tensor([3.9012], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3641], requires_grad=True), f(x) = tensor([3.9010], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3636], requires_grad=True), f(x) = tensor([3.9008], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3632], requires_grad=True), f(x) = tensor([3.9006], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3627], requires_grad=True), f(x) = tensor([3.9004], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3623], requires_grad=True), f(x) = tensor([3.9002], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3618], requires_grad=True), f(x) = tensor([3.9000], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3614], requires_grad=True), f(x) = tensor([3.8998], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3609], requires_grad=True), f(x) = tensor([3.8996], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3605], requires_grad=True), f(x) = tensor([3.8994], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3600], requires_grad=True), f(x) = tensor([3.8992], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3596], requires_grad=True), f(x) = tensor([3.8990], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3592], requires_grad=True), f(x) = tensor([3.8988], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3587], requires_grad=True), f(x) = tensor([3.8986], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3583], requires_grad=True), f(x) = tensor([3.8985], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3579], requires_grad=True), f(x) = tensor([3.8983], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3574], requires_grad=True), f(x) = tensor([3.8981], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3570], requires_grad=True), f(x) = tensor([3.8979], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3566], requires_grad=True), f(x) = tensor([3.8977], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3561], requires_grad=True), f(x) = tensor([3.8975], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3557], requires_grad=True), f(x) = tensor([3.8974], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3553], requires_grad=True), f(x) = tensor([3.8972], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3549], requires_grad=True), f(x) = tensor([3.8970], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3545], requires_grad=True), f(x) = tensor([3.8968], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3540], requires_grad=True), f(x) = tensor([3.8966], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3536], requires_grad=True), f(x) = tensor([3.8965], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3532], requires_grad=True), f(x) = tensor([3.8963], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3528], requires_grad=True), f(x) = tensor([3.8961], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3524], requires_grad=True), f(x) = tensor([3.8960], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3520], requires_grad=True), f(x) = tensor([3.8958], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3516], requires_grad=True), f(x) = tensor([3.8956], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3512], requires_grad=True), f(x) = tensor([3.8955], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3508], requires_grad=True), f(x) = tensor([3.8953], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3504], requires_grad=True), f(x) = tensor([3.8951], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3500], requires_grad=True), f(x) = tensor([3.8950], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3496], requires_grad=True), f(x) = tensor([3.8948], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3492], requires_grad=True), f(x) = tensor([3.8947], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3488], requires_grad=True), f(x) = tensor([3.8945], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3484], requires_grad=True), f(x) = tensor([3.8944], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3480], requires_grad=True), f(x) = tensor([3.8942], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3476], requires_grad=True), f(x) = tensor([3.8940], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3472], requires_grad=True), f(x) = tensor([3.8939], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3468], requires_grad=True), f(x) = tensor([3.8937], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3464], requires_grad=True), f(x) = tensor([3.8936], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3460], requires_grad=True), f(x) = tensor([3.8934], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3456], requires_grad=True), f(x) = tensor([3.8933], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3453], requires_grad=True), f(x) = tensor([3.8931], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3449], requires_grad=True), f(x) = tensor([3.8930], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3445], requires_grad=True), f(x) = tensor([3.8929], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3441], requires_grad=True), f(x) = tensor([3.8927], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3437], requires_grad=True), f(x) = tensor([3.8926], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3434], requires_grad=True), f(x) = tensor([3.8924], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3430], requires_grad=True), f(x) = tensor([3.8923], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3426], requires_grad=True), f(x) = tensor([3.8922], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3423], requires_grad=True), f(x) = tensor([3.8920], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3419], requires_grad=True), f(x) = tensor([3.8919], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3415], requires_grad=True), f(x) = tensor([3.8918], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3412], requires_grad=True), f(x) = tensor([3.8916], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3408], requires_grad=True), f(x) = tensor([3.8915], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3404], requires_grad=True), f(x) = tensor([3.8914], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3401], requires_grad=True), f(x) = tensor([3.8912], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3397], requires_grad=True), f(x) = tensor([3.8911], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3393], requires_grad=True), f(x) = tensor([3.8910], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3390], requires_grad=True), f(x) = tensor([3.8908], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3386], requires_grad=True), f(x) = tensor([3.8907], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3383], requires_grad=True), f(x) = tensor([3.8906], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3379], requires_grad=True), f(x) = tensor([3.8905], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3376], requires_grad=True), f(x) = tensor([3.8903], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3372], requires_grad=True), f(x) = tensor([3.8902], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3369], requires_grad=True), f(x) = tensor([3.8901], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3365], requires_grad=True), f(x) = tensor([3.8900], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3362], requires_grad=True), f(x) = tensor([3.8899], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3358], requires_grad=True), f(x) = tensor([3.8897], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3355], requires_grad=True), f(x) = tensor([3.8896], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3351], requires_grad=True), f(x) = tensor([3.8895], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3348], requires_grad=True), f(x) = tensor([3.8894], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3345], requires_grad=True), f(x) = tensor([3.8893], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3341], requires_grad=True), f(x) = tensor([3.8892], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3338], requires_grad=True), f(x) = tensor([3.8890], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3335], requires_grad=True), f(x) = tensor([3.8889], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3331], requires_grad=True), f(x) = tensor([3.8888], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3328], requires_grad=True), f(x) = tensor([3.8887], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3325], requires_grad=True), f(x) = tensor([3.8886], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3321], requires_grad=True), f(x) = tensor([3.8885], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3318], requires_grad=True), f(x) = tensor([3.8884], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3315], requires_grad=True), f(x) = tensor([3.8883], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3312], requires_grad=True), f(x) = tensor([3.8882], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3308], requires_grad=True), f(x) = tensor([3.8881], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3305], requires_grad=True), f(x) = tensor([3.8880], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3302], requires_grad=True), f(x) = tensor([3.8879], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3299], requires_grad=True), f(x) = tensor([3.8878], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3295], requires_grad=True), f(x) = tensor([3.8877], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3292], requires_grad=True), f(x) = tensor([3.8876], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3289], requires_grad=True), f(x) = tensor([3.8875], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3286], requires_grad=True), f(x) = tensor([3.8874], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3283], requires_grad=True), f(x) = tensor([3.8873], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3280], requires_grad=True), f(x) = tensor([3.8872], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3277], requires_grad=True), f(x) = tensor([3.8871], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3273], requires_grad=True), f(x) = tensor([3.8870], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3270], requires_grad=True), f(x) = tensor([3.8869], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3267], requires_grad=True), f(x) = tensor([3.8868], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3264], requires_grad=True), f(x) = tensor([3.8867], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3261], requires_grad=True), f(x) = tensor([3.8866], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3258], requires_grad=True), f(x) = tensor([3.8865], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3255], requires_grad=True), f(x) = tensor([3.8864], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3252], requires_grad=True), f(x) = tensor([3.8863], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3249], requires_grad=True), f(x) = tensor([3.8862], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3246], requires_grad=True), f(x) = tensor([3.8861], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3243], requires_grad=True), f(x) = tensor([3.8860], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3240], requires_grad=True), f(x) = tensor([3.8860], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3237], requires_grad=True), f(x) = tensor([3.8859], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3234], requires_grad=True), f(x) = tensor([3.8858], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3231], requires_grad=True), f(x) = tensor([3.8857], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3228], requires_grad=True), f(x) = tensor([3.8856], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3225], requires_grad=True), f(x) = tensor([3.8855], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3222], requires_grad=True), f(x) = tensor([3.8854], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3220], requires_grad=True), f(x) = tensor([3.8854], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3217], requires_grad=True), f(x) = tensor([3.8853], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3214], requires_grad=True), f(x) = tensor([3.8852], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3211], requires_grad=True), f(x) = tensor([3.8851], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3208], requires_grad=True), f(x) = tensor([3.8850], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3205], requires_grad=True), f(x) = tensor([3.8849], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3202], requires_grad=True), f(x) = tensor([3.8849], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3200], requires_grad=True), f(x) = tensor([3.8848], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3197], requires_grad=True), f(x) = tensor([3.8847], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3194], requires_grad=True), f(x) = tensor([3.8846], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3191], requires_grad=True), f(x) = tensor([3.8846], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3189], requires_grad=True), f(x) = tensor([3.8845], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3186], requires_grad=True), f(x) = tensor([3.8844], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3183], requires_grad=True), f(x) = tensor([3.8843], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3180], requires_grad=True), f(x) = tensor([3.8843], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3178], requires_grad=True), f(x) = tensor([3.8842], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3175], requires_grad=True), f(x) = tensor([3.8841], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3172], requires_grad=True), f(x) = tensor([3.8840], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3169], requires_grad=True), f(x) = tensor([3.8840], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3167], requires_grad=True), f(x) = tensor([3.8839], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3164], requires_grad=True), f(x) = tensor([3.8838], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3161], requires_grad=True), f(x) = tensor([3.8838], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3159], requires_grad=True), f(x) = tensor([3.8837], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3156], requires_grad=True), f(x) = tensor([3.8836], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3154], requires_grad=True), f(x) = tensor([3.8835], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3151], requires_grad=True), f(x) = tensor([3.8835], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3148], requires_grad=True), f(x) = tensor([3.8834], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3146], requires_grad=True), f(x) = tensor([3.8833], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3143], requires_grad=True), f(x) = tensor([3.8833], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3141], requires_grad=True), f(x) = tensor([3.8832], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3138], requires_grad=True), f(x) = tensor([3.8831], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3135], requires_grad=True), f(x) = tensor([3.8831], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3133], requires_grad=True), f(x) = tensor([3.8830], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3130], requires_grad=True), f(x) = tensor([3.8829], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3128], requires_grad=True), f(x) = tensor([3.8829], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3125], requires_grad=True), f(x) = tensor([3.8828], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3123], requires_grad=True), f(x) = tensor([3.8828], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3120], requires_grad=True), f(x) = tensor([3.8827], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3118], requires_grad=True), f(x) = tensor([3.8826], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3115], requires_grad=True), f(x) = tensor([3.8826], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3113], requires_grad=True), f(x) = tensor([3.8825], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3111], requires_grad=True), f(x) = tensor([3.8825], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3108], requires_grad=True), f(x) = tensor([3.8824], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3106], requires_grad=True), f(x) = tensor([3.8823], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3103], requires_grad=True), f(x) = tensor([3.8823], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3101], requires_grad=True), f(x) = tensor([3.8822], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3098], requires_grad=True), f(x) = tensor([3.8822], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3096], requires_grad=True), f(x) = tensor([3.8821], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3094], requires_grad=True), f(x) = tensor([3.8820], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3091], requires_grad=True), f(x) = tensor([3.8820], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3089], requires_grad=True), f(x) = tensor([3.8819], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3087], requires_grad=True), f(x) = tensor([3.8819], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3084], requires_grad=True), f(x) = tensor([3.8818], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3082], requires_grad=True), f(x) = tensor([3.8818], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3080], requires_grad=True), f(x) = tensor([3.8817], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3077], requires_grad=True), f(x) = tensor([3.8817], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3075], requires_grad=True), f(x) = tensor([3.8816], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3073], requires_grad=True), f(x) = tensor([3.8816], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3070], requires_grad=True), f(x) = tensor([3.8815], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3068], requires_grad=True), f(x) = tensor([3.8815], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3066], requires_grad=True), f(x) = tensor([3.8814], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3063], requires_grad=True), f(x) = tensor([3.8814], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3061], requires_grad=True), f(x) = tensor([3.8813], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3059], requires_grad=True), f(x) = tensor([3.8812], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3057], requires_grad=True), f(x) = tensor([3.8812], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3055], requires_grad=True), f(x) = tensor([3.8812], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3052], requires_grad=True), f(x) = tensor([3.8811], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3050], requires_grad=True), f(x) = tensor([3.8811], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3048], requires_grad=True), f(x) = tensor([3.8810], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3046], requires_grad=True), f(x) = tensor([3.8810], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3044], requires_grad=True), f(x) = tensor([3.8809], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3041], requires_grad=True), f(x) = tensor([3.8809], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3039], requires_grad=True), f(x) = tensor([3.8808], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3037], requires_grad=True), f(x) = tensor([3.8808], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3035], requires_grad=True), f(x) = tensor([3.8807], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3033], requires_grad=True), f(x) = tensor([3.8807], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3031], requires_grad=True), f(x) = tensor([3.8806], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3028], requires_grad=True), f(x) = tensor([3.8806], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3026], requires_grad=True), f(x) = tensor([3.8805], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3024], requires_grad=True), f(x) = tensor([3.8805], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3022], requires_grad=True), f(x) = tensor([3.8805], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3020], requires_grad=True), f(x) = tensor([3.8804], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3018], requires_grad=True), f(x) = tensor([3.8804], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3016], requires_grad=True), f(x) = tensor([3.8803], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3014], requires_grad=True), f(x) = tensor([3.8803], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3012], requires_grad=True), f(x) = tensor([3.8802], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3010], requires_grad=True), f(x) = tensor([3.8802], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3008], requires_grad=True), f(x) = tensor([3.8802], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3006], requires_grad=True), f(x) = tensor([3.8801], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3004], requires_grad=True), f(x) = tensor([3.8801], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3002], requires_grad=True), f(x) = tensor([3.8800], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f04a4f6f390>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTAAwWN2DWEH"
      },
      "source": [
        "# c) Implementando un MLP en PyTorch para predicción de precios de inmuebles \n",
        "\n",
        "Contamos con una base de datos de 506 precios de inmuebles de la ciudad de Boston [1]. Cada inmueble está descripto por diversas características como el indice de crimen per capita en la zona, o el grado de accesibilidad a autopistas, etc. Se cuenta con el precio de cada uno, y se pretende desarrollar un módulo que permita predecir dicho precio a partir de las características.\n",
        "\n",
        "[1]: *Hedonic prices and the demand for clean air*, J. Environ. Economics & Management, vol.5, 81-102, 1978.\n",
        "\n",
        "Primero, vamos a generar un histograma de los precios con todos los datos disponibles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D-sOjKKSdmp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "ba0e2bcb-5874-4ebe-8dec-6db68bdf979f"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Importamos el dataset\n",
        "\n",
        "dataset_boston = load_boston()\n",
        "\n",
        "print(\"El dataset contiene \" + str(dataset_boston.keys()) + \" \\n\\n\")\n",
        "\n",
        "# Extraigo los datos (features) y los precios (etiquetas a predecir)\n",
        "data = dataset_boston['data']\n",
        "data = data.astype(np.float32)\n",
        "precios = np.expand_dims(dataset_boston['target'], axis=1).astype(np.float32)\n",
        "\n",
        "print(\"Fila de ejemplo:\")\n",
        "print(dataset_boston['feature_names'])\n",
        "print(data[0,:])\n",
        "\n",
        "# Dibujo un histograma de los precios de los inmuebles\n",
        "_ = plt.hist(precios, 50, density=True, facecolor='g', alpha=0.75)\n",
        "_ = plt.title(\"Precios\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El dataset contiene dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename']) \n",
            "\n",
            "\n",
            "Fila de ejemplo:\n",
            "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
            " 'B' 'LSTAT']\n",
            "[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
            " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgc2qJKLmhsB"
      },
      "source": [
        "Particionamos los datos en entrenamiento y prueba usando la función `sklearn.model_selection.train_test_split`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WWTSCnnVyRK"
      },
      "source": [
        " from sklearn.model_selection import train_test_split\n",
        " \n",
        " # Particiono los datos en entrenamiento y prueba usando el método de scikitlearn\n",
        " X_train, X_test, y_train, y_test = train_test_split(data, precios, test_size=0.33, random_state=42)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_D3sJMCoB_K"
      },
      "source": [
        "Ahora implementaremos un Perceptrón multicapa que usaremos para como regresor del precio utilizando PyTorch (ejemplo basado en el curso de [RPI](https://rpi.analyticsdojo.com/)).\n",
        "\n",
        "El perceptrón deberá contar con 3 capas:\n",
        "- Las dos primeras con 100 neuronas, y deberán usar la función de activación ReLU.\n",
        "- La última con una única neurona cuya salida sea un valor escalar que corresponda al precio estimado del inmueble, que no deberá utilizar ninguna función de activación.\n",
        "\n",
        "Algunas clases de PyTorch que resultarán útiles para implementar el modelo, son:\n",
        "- `torch.nn.Linear`: Implementa una capa totalmente conectada. Es necesario especificarle el número de parámetros de entrada y de salida.\n",
        "- `torch.nn.functional.relu`: Implementa la función de activación ReLU.\n",
        "\n",
        "Además, utilizaremos el optimizador `torch.optim.Adam` y la función de pérdida `torch.nn.MSELoss` (error cuadrático medio).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91rAzYsjkAUa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "abcbd97f-5362-444d-b270-f720916f7eb3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Tamaño del batch de entrenamiento\n",
        "batch_size = 50\n",
        "\n",
        "# Tasa de aprendizaje inicial para el gradiente descendente\n",
        "learning_rate = 0.00001\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_features, size_hidden, n_output):\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden1 = nn.Linear(input_features, size_hidden) # First layer\n",
        "        self.hidden2 = nn.Linear(size_hidden, size_hidden)    # Second layer\n",
        "        self.out = nn.Linear(size_hidden, n_output)           # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.hidden1(x))      # activation function for hidden layer linear output\n",
        "        x = F.relu(self.hidden2(x))\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "# Definimos el modelo del perceptrón\n",
        "# Establezco el tamaño de entrada a la red (cantidad de features entrantes)\n",
        "input_size = len(dataset_boston['feature_names'])\n",
        "net = Net(input_size, 100 , 1)\n",
        "\n",
        "# Construimos el optimizador, y le indicamos que los parámetros a optimizar \n",
        "# son los del modelo definido: net.parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "# Definimos también la función de pérdida a utilizar\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "# Creamos el objeto dataset que empaqueta los array de numpy para que puedan \n",
        "# ser leidos por PyTorch\n",
        "dataset = TensorDataset(torch.from_numpy(X_train).clone(), torch.from_numpy(y_train).clone())\n",
        "\n",
        "# Creamos un loader iterable indicandole que debe leer los datos a partir de\n",
        "# del dataset creado en el paso anterior. Este objeto puede ser iterado\n",
        "# y nos devuelve de a un batch (x, y).\n",
        "loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Número de épocas\n",
        "num_epochs = 5000\n",
        "\n",
        "# Lista en la que iremos guardando el valor de la función de pérdida en cada \n",
        "# etapa de entrenamiento\n",
        "loss_list = []\n",
        "\n",
        "# Bucle de entrenamiento\n",
        "for i in range(num_epochs):\n",
        "\n",
        "    # Itero sobre todos los batches del dataset\n",
        "    for x, y in loader:\n",
        "        # Seteo en cero los gradientes de los parámetros a optimizar\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Realizo la pasada forward computando la loss entre la salida de la red `net(x)` y las etiquetas `y`\n",
        "        # Forward pass\n",
        "        prediction = net(x)\n",
        "        # Funcion de perdida\n",
        "        loss = criterion(prediction, y)\n",
        "        \n",
        "        # Realizo la pasada backward por la red        \n",
        "        loss.backward()\n",
        "        \n",
        "        # Actualizo los pesos de la red con el optimizador\n",
        "        optimizer.step()\n",
        "\n",
        "        # Me guardo el valor actual de la función de pérdida para luego graficarlo\n",
        "        loss_list.append(loss.data.item())\n",
        "\n",
        "    # Muestro el valor de la función de pérdida cada 100 iteraciones        \n",
        "    if i > 0 and i % 100 == 0:\n",
        "        print('Epoch %d, loss = %g' % (i, loss))\n",
        "\n",
        "# Muestro la lista que contiene los valores de la función de pérdida\n",
        "# y una versión suavizada (rojo) para observar la tendencia\n",
        "plt.figure()\n",
        "loss_np_array = np.array(loss_list)\n",
        "plt.plot(loss_np_array, alpha = 0.3)\n",
        "N = 60\n",
        "running_avg_loss = np.convolve(loss_np_array, np.ones((N,))/N, mode='valid')\n",
        "plt.plot(running_avg_loss, color='red')\n",
        "plt.title(\"Función de pérdida durante el entrenamiento\")\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 100, loss = 92.2178\n",
            "Epoch 200, loss = 44.7619\n",
            "Epoch 300, loss = 20.4063\n",
            "Epoch 400, loss = 125.854\n",
            "Epoch 500, loss = 41.3257\n",
            "Epoch 600, loss = 75.5364\n",
            "Epoch 700, loss = 23.6191\n",
            "Epoch 800, loss = 76.8396\n",
            "Epoch 900, loss = 53.338\n",
            "Epoch 1000, loss = 80.0005\n",
            "Epoch 1100, loss = 41.6748\n",
            "Epoch 1200, loss = 26.2745\n",
            "Epoch 1300, loss = 9.83135\n",
            "Epoch 1400, loss = 21.7622\n",
            "Epoch 1500, loss = 54.7592\n",
            "Epoch 1600, loss = 17.4061\n",
            "Epoch 1700, loss = 32.6498\n",
            "Epoch 1800, loss = 35.3781\n",
            "Epoch 1900, loss = 55.3582\n",
            "Epoch 2000, loss = 39.9886\n",
            "Epoch 2100, loss = 32.0634\n",
            "Epoch 2200, loss = 24.3466\n",
            "Epoch 2300, loss = 64.678\n",
            "Epoch 2400, loss = 50.7721\n",
            "Epoch 2500, loss = 23.0024\n",
            "Epoch 2600, loss = 53.8849\n",
            "Epoch 2700, loss = 21.1223\n",
            "Epoch 2800, loss = 35.9887\n",
            "Epoch 2900, loss = 15.5511\n",
            "Epoch 3000, loss = 23.9271\n",
            "Epoch 3100, loss = 27.3704\n",
            "Epoch 3200, loss = 9.2124\n",
            "Epoch 3300, loss = 31.3653\n",
            "Epoch 3400, loss = 21.8242\n",
            "Epoch 3500, loss = 17.0637\n",
            "Epoch 3600, loss = 18.8446\n",
            "Epoch 3700, loss = 13.0251\n",
            "Epoch 3800, loss = 17.1178\n",
            "Epoch 3900, loss = 17.3152\n",
            "Epoch 4000, loss = 28.5133\n",
            "Epoch 4100, loss = 25.1608\n",
            "Epoch 4200, loss = 34.5206\n",
            "Epoch 4300, loss = 12.4753\n",
            "Epoch 4400, loss = 11.1433\n",
            "Epoch 4500, loss = 27.7697\n",
            "Epoch 4600, loss = 19.4505\n",
            "Epoch 4700, loss = 34.0348\n",
            "Epoch 4800, loss = 12.6498\n",
            "Epoch 4900, loss = 9.98645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Función de pérdida durante el entrenamiento')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhL0-b9J6eoD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "e8f97892-2923-49cc-8389-a92205833758"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Definimos un método para mostrar las predicciones como un scatter plot \n",
        "# y graficamos la recta de regresión para esos datos.\n",
        "\n",
        "def plotScatter(x_data, y_data, title, fit_line=True):\n",
        "  plt.figure()\n",
        "  \n",
        "  plt.plot(x_data, y_data, '+')\n",
        "  plt.xlabel('Valor real')\n",
        "  plt.ylabel('Predicción')\n",
        "  plt.ylim((0,50))\n",
        "  plt.xlim((0,50))\n",
        "  plt.title(title)\n",
        "\n",
        "  if fit_line:\n",
        "    X, Y = x_data.reshape(-1,1), y_data.reshape(-1,1)\n",
        "    plt.plot( X, LinearRegression().fit(X, Y).predict(X) )\n",
        "\n",
        "# Dibujamos el ground truth vs las predicciones en los datos de entrenamiento\n",
        "py = net(torch.FloatTensor(X_train))\n",
        "y_pred_train = py.cpu().detach().numpy()\n",
        "plotScatter(y_train, y_pred_train, \"Training data\")\n",
        "\n",
        "# Dibujamos el ground truth vs las predicciones en los datos de test\n",
        "py = net(torch.FloatTensor(X_test))\n",
        "y_pred_test = py.cpu().detach().numpy()\n",
        "plotScatter(y_test, y_pred_test, \"Test data\")\n",
        "\n",
        "print (\"MSE medio en training: \" + str(((y_train - y_pred_train)**2).mean()))\n",
        "print (\"MSE medio en test: \" + str(((y_test - y_pred_test)**2).mean()))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE medio en training: 19.10686\n",
            "MSE medio en test: 20.79309\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de7xUddn38c8liCiiiAfEA4KFmpmWqOidmUqUpaWVaWmmdxZq3T1IWWInuSvv0NItlaV0e6ASD6Vlj1SKiCfEA6ampoDKNkMSPIBoD5h2PX/MWrPXrL3WzJrZs+aw5/t+vfZrZq1Zh98e2L9r/c7m7oiIiABs0OwEiIhI61BQEBGRIgUFEREpUlAQEZEiBQURESlSUBARkSIFBekoZvZHMzux3sf2lZm5mb21EfcSKcc0TkFanZm9GtncBFgPvBlsn+LuVzY+VfVlZg6MdfcnKxw3GlgGbOjubzQgadJhBjY7ASKVuPum4Xsz6wY+5+63xI8zs4HKKEX6RtVH0rbM7GAz+7uZnWlm/wAuN7MtzOxGM1tlZi8H73eInHObmX0ueH+Smd1lZj8Mjl1mZh+s8dgxZnaHma01s1vM7CIz+1WZtH/VzFaY2XNm9tnYZ4eb2YNm9oqZPWtm0yIf3xG8rjazV83sADN7i5ndamYvmtkLZnalmQ3ry3crnUtBQdrdtsBwYCdgEoX/05cH26OA/wf8pMz544HFwFbAecClZmY1HDsbuA/YEpgGnJB2QzM7DDgDmAiMBd4XO+Q14DPAMOBw4DQzOyr47KDgdZi7b+ruCwEDvg9sB7wN2DFIg0jVFBSk3f0bONvd17v7/3P3F939Onf/p7uvBc4B3lvm/Gfc/efu/iYwCxgJjKjmWDMbBewLfNvdX3f3u4Dfl7nnMcDl7v6ou79GLAN399vc/RF3/7e7/wW4qtzv4O5Puvvc4DtYBVxQ4XcWSaWgIO1ulbuvCzfMbBMzu8TMnjGzVyhUtwwzswEp5/8jfOPu/wzeblrlsdsBL0X2ATxbJs3bxT5/JvqhmY03s/lBFdga4FQKpZNEZjbCzK42s+XB7/yrcseLlKOgIO0u3n3uK8CuwHh334ye6pa0KqF6WAEMN7NNIvt2rHB89PNRsc9nUyhp7OjumwMX05P+pO6C/xPsf0fwO3+afH9f6ccUFKS/GUqhHWG1mQ0Hzs77hu7+DLAImGZmg8zsAODDZU65FjjJzHYPAkk8jUMplDzWmdl+wHGRz1ZRqDLbOXb8q8AaM9se+GrffiPpZAoK0t9cCGwMvADcA/ypQfc9HjgAeBH4HnANhfEUvbj7Hymk81bgyeA16gvAd8xsLfBtCkEkPPefFNpJFpjZajPbH/hvYG9gDTAHuL5+v5Z0Gg1eE8mBmV0DPOHuuZdUROpJJQWROjCzfYPxAhsEXU6PBH7X7HSJVCvXEc3B6NO1FKYkeMPd9wnqea8BRgPdwDHu/nKe6RBpgG0pVNtsCfwdOM3dH2xukkSql2v1URAU9nH3FyL7zqPQiDbdzKYCW7j7mbklQkREMmtG9dGRFAb+ELweVeZYERFpoLxLCsuAlyn0ob7E3Wea2Wp3HxZ8bsDL4Xbs3EkUpi1gyJAh43bbbbfc0iki0h898MADL7j71tWck/csqQe6+3Iz2waYa2ZPRD90dw+mDO7F3WcCMwH22WcfX7RoUc5JFRHpX8zsmcpHlcq1+sjdlwevK4HfAvsBz5vZSIDgdWWeaRARkexyCwpmNsTMhobvgfcDj1IYvh+uZnUicENeaRARkerkWX00AvhtMLPwQGC2u//JzO4HrjWzkylMBHZMjmkQEZEq5BYU3P1pYK+E/S8CE/K6r4iI1E4jmkVEpEhBQUREihQURESkSEFBRESKFBRERKRIQUFERIoUFEREpEhBQUREihQURESkSEFBRESKFBRERKRIQUFERIoUFEREpEhBQUSknxowdKvtqj1HQUFEpJ8aMGTYyGrPUVAQEZGiPFdeExGRBuuau4QZ85bWfL65ex2Tk4999tnHFy1a1OxkiIi0lY1GjmX9iqVWzTkqKYiI9CMqKYiISKJaSgpqaBYRkSJVH4mI9CcvPwMz9gw2tqn6dAUFEZH+oCQY1E5BQUSknb3cDTP2Ktn15ddP5fp/HwScXvXlFBRERNrRS8vgR+8s3ffRmbDXsVwAXABsNEtBQUSkfysTDOpBQUFEpB289DT86F2l+yoEA3/zjdervY2CgohIK0sKBh/7Oex5TMVTbcDAQdXeTkFBRKQVJQaD/4U9P5HrbRUURERayYtPwY/3Lt338UvhHUdnOv3YSxZy77KXar69prkQEWkFfQwGSTQhnohIu0kKBkdfBnt8vKbLaUI8EZF2VOdgkEQlBRGRVvfCk/CTcaX76hwM+kJBQUSkERKDweWwx8eak54UuQcFMxsALAKWu/sRZjYGuBrYEngAOMHdqx5gISLSFl5YCj/Zp3TfJ66At3+0KcmppBElhcnA48Bmwfa5QJe7X21mFwMnAz9rQDpERBonMRjMgrcflettW7qh2cx2AGYB5wBfBj4MrAK2dfc3zOwAYJq7f6DcddTQLCJtY9USuGjf0n0NCAZJWrGh+ULga8DQYHtLYLW7vxFs/x3YPulEM5sETAIYNWpUzskUEemjpGBwzC9g9yObk54a5bYcp5kdAax09wdqOd/dZ7r7Pu6+z9Zbb13n1IlIK+mau6TZSahaMc2rFsO0zUsDwjG/hGlrmhIQuuYuYfTUOYyeOqem8/MsKbwb+IiZfQgYTKFNYQYwzMwGBqWFHYDlOaZBRNrAjHlLmTJxl2Ynoyo33nobUxbESgbH/gre9uHmJCgwZeIuxe+ypdZTcPezgLMAzOxg4Ax3P97Mfg0cTaEH0onADXmlQUSk7lYthov2Y95GkX0tEAzqJbfqozLOBL5sZk9SaGO4tAlpEOlX8qp+ybNaJ17NEb5v2aqklU8E1UT7FXed8voURq+bTdffd21iwkr1tfpI01yI9AOjp86he/rhbXPdZt2nJiufgJ+OL933ydmMvoLWTXOgFXsfiYi0p5WPw0/3L933ydmwWxgIansSb3UKCiJtKj5IKawumDxhbJ8abfO6bjmTJ4zN5bo1SQwGV8FuHyrZ1VJpriNVH4n0A+1efdQSnv8r/OyA0n2fuhp2/WBdb9M1d0nDelqp+khEpFoNCgahvLvf9nWaCwUFEUnVjCqShj1JJwaDa2DXw/K/d45adpyCiLS/Zgwoy30g2/OPwc/+o3RfzsGgGe00tVJQEGlT7ZTRtIR/PAoXv7tk1+/e1sVRx34291tHn94b2U7z5murV1R7joKCSJtqVkaTh1wD3D8egYsPLN133K9hl/dz+tQ5HHVs5bS1a5B9c+0Lz1V7joKCiNRFOBK5lgx0ysRdmDFvKd3TD69fgEsKBsf/BsZOrOoy9a7OavWurAoKIv1AXzOaejwNh0/6TX+qrhAMml3t1vTvpwIFBZF+oB4Zet7dJJOun5ZB1xSkVvwFLnlP6b7jr4Ox7+t1aFgSSSuVNDtwNJOCgnSMdq4bblVJfeKTMtC0oFOXdpEqgkG5tNQ9XW1KQUE6RjvO2Z+nejwNxzNPqDxJXN2C84qH4ZKDSvd9+jp4a3IwSNLq9fvNoKAg0qHyfhpOCzrhveMyZ9CJweB6eOuEqtOS5Z7VBo52L5Fq7iPp19KG/HdC3XAl0cyrHkGhXO+j6PVrvtdzD8HM95buqxAMkuRdHdRK1U1m9oC771PNOSopSL/WyXXDlUSr0+pRjVIpyEZLClVVVSUFgxN+C285tKZ0SnkKCiJSl1JTuWqTaOafOTg/9yDMPLh03wm/g7cc0qd05tGO0J96KykoSMdQo2K+mVe5hvyqrp1TMMiSllrbA/paIm2ldohmrNEs0hSt8kdXTt7rE0+ZuAvd0w8vZlrh++h307Q1kpf/ubAGcjQgfOYGmLambgGhkr5MOd2I+zbi30YlBZEW0grdZqNpqPQEm7XkUfa43dfCz2PtA5+5AXY+uI+/SXPkWSJtxP8PBQWRDpUl86qUCWWtNgmPmTJxl57jlj8AP98XFkQO/MzvYef3Jl4jL/WuUst6Tqu2Q6hLqkiTtUK32XKrdWWtH69Ulx5+fuRZM7hho2+XftiEYJCkWT3Uyt23L/8/1CVVpA01sttsWnVQ0sjkUL2eYPeyJ2Ha5tywUWTniTfCmPekntNMrdL425f/HwOGbrVdtfdTQ7NIB8naoJnUGF2LrrlLOOqsGUEw6Ckd/HqPiwsNyC0WEKJVao1sdM6rHWLAkGEjqz1HJQWRFtIK3WbT0pDUvlC2XvxtrzBlwQSmREsGJ82B0Qfyibqnuj6aVTLIet9G/P9Qm4J0vFapJojLkq6sx9RaJ13NVBjFz5+9Hy6NTUp30hxGX7ym5UeUt0L7Tl9Ff4cVs05n/YqlVs35CgrS8Vp1+oss6ao27dUeX00m+bGzurh+o2mlBwYlg/Bafc1YGxnAW/X/RTU2Gjm26qCgNgWRKtRr8FCeg5Ci1066TzX3zjLYjWfvg2mblwaEk/5QaDMYfWDJtfqqWYPL4po2wK8BFBSkI3XNXcLoqXOKdeDh+665S8r+wdcrU0q7Trl0ZT0meu34fSZPGFvc1+eM7W/3FkYgXxpZ8/g//xgEg3f37dotoFz9fasEp0refG31imrPUUOzdKRy3fxGT52T6ak2j6qMLN0Pw2PCqp1qqjimTNylmKGVG5h27CUL2X/nLUs+L2aSf7sXLnt/6Qn/+SfY6YDM6ahGswZ5tUsbQjlvrn3huWrPUVAQSRFm+tUuFlPuevXK3OLXKpfG+LiD6L60wHbvspe4d9lLJZ9N2fWlQskgKsdgULxvi0x/3qojkOtNDc3S8ZKqXULxP/i6LBaT8dxyJZFyGVK5NGZtOC5ZWvNv98BlHyg94bM3waj9y6Y/D63S+Nsq6ahEI5pFahCdlwd6/uDTqpFqXiymxnRFxTP1GfOWMmPe0or3DwNMWH0U/n7RjO3YSxZy77KXitv72BMw7bjSC2UIBnn2EGqFcRztpJYRzQoKIhFhqSHaiAs9mX74mnWR+jS1ZG5Zn/Kj1w7fZ1ll7ZpTgmqgZ+6Gyz9Y+uFnb4ZR4zOlM8+ZPFulmqZdglNLjWg2s8HAHcBGwX1+4+5nm9kY4GpgS+AB4AR3fz2vdIhUI5rpJDXi1itTqsdCLpAclEraAVLmOYKEjK3GYNCqg//y1J9/3zxLCuuBQ939VTPbELjLzP4IfBnocverzexi4GTgZzmmQ6Qq0ZIApDc4Q3MbG8NMPSzdVDUFRSTA0L0ArvhQybnf3PICttztPUwZVfl3ivZmSruXNE65GW8zcffcf4BNgD8D44EXgIHB/gOAmyqdP27cOBdppJ3OvNEvuHlx8X38s/CnGuH1aj0n7X2WtCR+vuwu97M3K/35231VpzHp+6lGLd+LZDNo27e6V5lfZxq8ZmYbmdlxZvZ1M/t2+JPhvAFm9hCwEpgLPAWsdvc3gkP+Dmyfcu4kM1tkZotWrVqVJZkifRIfFDZj3tKSrpt9VcvTW9pAtD49fXffVehaGi0dnHwLTFtD1xObp58XkWWQXVbtMhCsU2StProBWEOhDWB91ou7+5vAO81sGPBbYLcqzp0JzIRCl9Ss54lUcuwlC3saVSPKrSkQ9vKJa1Z9erVVWZMnjC0EgytibRCfmwc79PRYzNpI3CpjB6S8PEc07+Duh1V78ZC7rzaz+RSqi4aZ2cCgtLADsLzW64rUItrtspxoX//4dtZMsJYBT1kGok2eMDY1fb0su5MpC44oXfYyFgzqKetocLVB5C/PEc13m9k73P2RrBc2s62BfwUBYWNgInAuMB84mkIPpBMplEJEmi761D95wtiymXNW8SfqLJle2lN41U/ky+6AWR8u3fe5W2GHcSW7+ppBZ+2eGf1+VdJoXVmDwoHASWa2jEL1kQHu7nuWOWckMMvMBlCYeO9ad7/RzP4KXG1m3wMeBC6tPfki2cQHZoUZ3/gxw4tVSdGqk/hC80BxwFdf+qjX0oe/UhVVr/RkDAahvmbQWX+fPMcvSP1kDQofrHxIKXf/C/CuhP1PA/tVez2Rvoi2IWSdYiKtzj5+XDWrZlXbqBqeEw6cS1K8/9O3wy8+Uvrh52+F7ZODQatol4FgnSLz3EdmthcQLqh6p7s/nFuqYjT3kdRTNChkGSWcVm0UZtjlAkw18yplSW+ixGAwH7bfu+K140Etul2vRXHafSWzdlbL3EepQcHM9gye9jGzycDngeuDjz8KzHT3H/chvZkpKEg9pfU+yhIsoFCNFGaYmZeoTNlOkykzffo2+MWRpQckBINKE+ulpafedf1qO2i8WoJCuXEK25rZD4P3JwPj3f3b7v5tYH8KQUKk7SQFhLhwxbFQtLdPmGFX00c/a//96Ojk1BXPnppfGGcQDQiTbissbpNQOtA4AKlGapuCu99sZv8KNg14M/Lxm8E+kX6jXN12tFdSuZlGoXyX0qR7RJ/kyzbGPjUffnlU6b5Jt8F2vZruyirX2yhMQ9Jnfa3uUdtBe8jUpmBmX6bQffS3wa6jgCvc/cIc01ak6iOpt3JVKrXOHVNubYOs1TRJx13361/y8cf+q/SkSbeVDQbVrJvQqOojaby6tikkXHxvCl1TodDQ/GCV6auZgoL0RVIAyJrhpQ0Mi2b2aU/R1S5oE3f+3i/y8b9+qXTnpNthu3dWTHdSWqv5rJalPqX11H2RHTPbzN1fMbPhQHfwE3423N2zDQ0VaaJolUwe01KkVfnExzqUW9AmqvtzG8GvPg5/jeysIRhkkValEy7cI52n0jiF2cARFOY8ihYpLNjeOad0idRd+PQbPr1nqS9PyxjD/ZXGHqRd995lL/WuMnrylkIw+FXkwFPugJF7lf29KsnSVlLtZ9J/lQ0K7n5E8DqmMckRqU7ak3+WKSqyVI2UyxirWZZz/Jjhqdd57wYP91728pQ7YWS5CQOyy5q5az4igewNzR8FbnX3NcH2MOBgd/9dzukD1KYg6bK0DZQbfJY2cCuPNMRLFe/d4GFmDTq35LgPrv8+j/tOielrJDUy9w/1HqcQdXYYEKAw6ylwdjU3EmmUrGMCZsxbWnJs3v35i2MPTt6I7sHHlQaEU++CaWt43HcqHZMg0mBZg0LScXku5SmSqtICL/HMPRx4Fn3yjW+Xu1claXX28XSe9PXvFQadXfnxnoOCYMC276h4nyzC9PZ1USA1MneurNVHlwGrgYuCXV8Ehrv7SfklrYeqjyRN0jiAeNVHua6haaWDsOqmLtUoS26G2Z8o2fXLd87mhKN6RkjXq3dUlnER0jnq3iU14kvAt4BrKPQ6mkshMIg0XFrGWa7hN21Ng6RSRd2meF5yE8w+pnTfqQtg2z04IbIracpukWbJPHitmVRSkKjwKbjSpHWhMIiEx9cyhTUkN/wmBqiEYPCB9dNZ7KMSr5VUsqmmAbzSCGz1HupcuTU0m9ncoMdRuL2Fmd1UbQJF6ilt0ri4MMOcMW8p48cMZ8rEXcp2EY1LavhNbL9Y/KdCm0E0IJx2N0xbw03fP61XGtPaReIZfKUAlvQ9pKVbpJKs1UdbBT2OAHD3l81sm5zSJB0u6cm4mj70aZPOQc/6zOFMqbUssQmxUcyL/whXfbL0gNMWwojdU88vt9qZZjWVZsoaFP5tZqPc/W8AZrYTpSOcpYXlMbVDnpLq9CstGRkNBNGqoug1Q1mX1AyrqOLCfSd//Tt0Dz4frop8WCYYlCudVBpsl3UgWXSktUgtsgaFbwB3mdntFKa4eA8wKbdUSV2189q4WQJa0qI58SASlzS3T7ytIXpeNNC8b4MHCsEg6gv3wDZvK5vOtLmOkhrDq1nIJ0oN1tJXmYKCu/8pmCV1/2DX6e7+Qn7Jkk5T7km5UvVQWmZbqQE2rZtq+Fk8I56y41KmDC6djmLi+vOY+/1TUu+RRVoG3s7BXNpX2YZmM9steN0bGAU8F/yMCvZJi6o0wKvVZG00Do+t5ppJVSlJ+6KT5YW65i6BJ+YUGpCv7gkI71t/Hkxbw/DR5ecnquXfIakEI9IoZbukmtnP3f3zZjY/4WN390PzS1oPdUntm3YYyJRlYZtofXra1NPjxwwvViVVu1hOr26hOyyBa44vOWbWu67hxCMPq6lqJ8uxWuhe6qnug9fc/fPB6yF9SZhIJUkDzMLMMakPf7QNoZagFwaPaCZcXB95h8VMWfDp0hO+eB9svStnT53DiUfmV2dfqUFdJG+VFtn5WLnP3f36+iZH8tCO1Q/Rp+WsaxjHG6XDDDa+RvKMeUuLpYzo8U/Mn80lg7pKrjlh/Q94yreH859k8gQr3qeWKabb8d9BOk+lhuYPB6/bAP8B3BpsHwLcDSgotIF2qnZIqmsPM+C03yPs6pmlYTZpRPOU7Z+Aa0+AQZEDv3g/bL0LT0VKLdFBcNATYKrtGZSVgog0Q9YJ8W4GTnT3FcH2SOAKd/9AzukD1KbQKaptAyi3CH2Wax22wX1cPOjCkn3FkgGlmX5aG4KqeKSV5bmewo5hQAg8T6E3kkhFWXs8xXsgZb12Uu8e6JkyO+6wDe6je/BxJQHh0PU/ZPS62cWAEIp2UU36ffQ0L/1N1qAwz8xuMrOTzOwkYA5wS37JknYWn9O/1mkbopl6UnfVsM0g2u00+ll0EZ3u6YfTfcK/EoMB09bw4UPf2+v+aekuqXpqo6o5kSwyBQV3/y/gYmCv4Gemu38pz4RJ+4rXvWcVfQIfP2Y4M+YtTSwBhMemrZoW3z9520cL4wx+fWJx/yHrz6fr3ffztG8HFDL36FN/GGjiaQi18pgPkb6oZvW0PwNr3f0WM9vEzIa6+9q8EibtLcww4xlquR460Yw93uU02oU0emzaxHkz5i3l8A3u4aJBPyosDxU4ZP35fOTQg1gWaTiOpi3aCJ1lqoykNIi0s0xBwcw+T2Guo+HAW4DtKZQcJuSXNGkn5Sagq0ZaRpvUhRR6Z9YlwSDi4PXn0+0ji9tpq7SVE65oFj1fpL/J2vvoIWA/4F53f1ew7xF3r8/CshWo91F7iS8JWS7jzTKCN+viOEdssJCfDPpxyb54MIjfI967qNY0irSiPJfjXO/ur5tZeKOBaOrsjlePKbnTlsVMmoco3B4/Znix5DB5wlienj+LHw/6Scmxl+19Pd+5e13qfeMZerkxDuWW8xTpb7L2PrrdzL4ObGxmE4FfA/83v2RJO0h6eu6au6TXnP6Vum2GPYiiwl5E0V5HYW+ksL3hwxvczZQF+5YGhC/9mdHrZvPZj0woOSeehmgDsoj0yBoUzgRWAY8ApwB/AL6ZV6KkfSUtQl+uWiaqUvAIu5j+YfaP6R58XGIwYMu3MHnC2GJwuufpF0vSEO26mtS7KMsMploZTfqzikHBzAYAj7v7z939E+5+dPC+bPWRme1oZvPN7K9m9piZTQ72Dw/WfF4avG5Rp99FGqBeU3In1dOH3ULDjDveRfUjGyxgyoJ9+dCSyPPI/3mQrnffXwwG4XXC4BSdSTVe7ZM0XXelqiFVG0l/V7FNwd3fNLPF0eU4M3oD+Iq7/9nMhgIPmNlc4CRgnrtPN7OpwFQKJRFpA0mzeI6eOqekDaBS99O0htt4G0VxltT5l3PNoJ+WzE100Pou/uYj4LzHAbjn6Rd7rb6WlPYwXWnS2hZqnQRPpN1kbWjeAnjMzO4DXgt3uvtH0k4IpsVYEbxfa2aPU+jKeiRwcHDYLOA2FBTaXtYunuUy13iGfOQGdzFlwXElweA967t41kcUt8Pz4tNgR68dvoZjHaK9mrIECkgPhgoI0t9kDQrf6stNzGw08C7gXmBEZB6lfwAjUs6ZRLAO9KhRmmap1aQNTqt0Tlp9fHRain/e1sU3NpzNjDLBIOyBlFQ6geRG8P133rKYjmgVU3TAW/xayvSl01RaeW0wcCrwVgqNzJe6+xtV3cBsU+B24Bx3v97MVrv7sMjnL7t72XYFjVNobfHZSaupMoo6a+CVnDKwNLhcOu53fHfBP3sdO3nCWH7zwLMsmDqh10L30TQllVyyfJamXmMW6tGdV6SSPGZJnQXsQyEgfBA4v8oEbQhcB1wZWZDn+WDq7XAK7pXVXFPy15c5fcr19U+rojlr4JV0Dz6uNCCcciej181ODAjh9ZavLh2HEM2s0+6VVMKppqE8bS3pajN49WCSVlWp+mj3cNSymV0K3Jf1wlYY6XYphZ5LF0Q++j1wIjA9eL2hqhRL7rIsVhOVdfro+Epo3YfcDQtLB51xyh0wcq/guoNT5x6KV1uFr+ETeLytIOsTfrhgj0inqlRS+Ff4ptpqI+DdwAnAoWb2UPDzIQrBYKKZLQXeF2xLG0ualK6c63a+ke7Bx5UGhFPuLHQtDQJC9LrVrF8QjjsorrccGSsRf8JPSnu0C2sl1a6lUK/uvCK5cvfUH+BN4JXgZy2Fbqbh+1fKnVvPn3Hjxrnk64KbF/tOZ97Y6+eCmxeXPSfJTmfemHzCH6e6n71Zyc9hUy8qe4/wemH6otdPe01yzMV390pf0n3LXaOeGnUf6WzAIq8yvy1bUnD3Ae6+WfAz1N0HRt5vlm+4kkaqpa48S71419wl8MephfUM7vlpzwenLoBpa3jcdyr2ACoeHz8/cq+0aqPw9dhLFiamIz6ILfydw3voCV6koJr1FERSJY0ROHvgLKYMvKn0wFMX0PXIIGZcuBR4pnhsKN5NNK7cuslZ13iOB7qkMQh50zKe0qqyzn0kHaRchlVuTeSwdDFt4BV0Dz6O/4wGhNPuLsxNtO0emer3w/dpxyQ9xccz+2MvWZiY1rTSRCOpO6q0KpUUpJdKc/+ET+q9nqr/8FW4byYnRf5XfWD9dBb7KOjqBnoPMovuyzoQLr462vgxw3stlRkeF059kbUEoCd46XQqKUjfzTmj0GZw38yefacthGlrWOyjUtsq0koDUdGupqF40Aoz/r6OHUi6tkinUUlBKoo39kIhs/7uwMtg2nGlB3/hHkZf8DR0LQOWFY+tRnQepbBUEG9ojqYjC40/EMlGJQWp2Msm3nj7vYGX0j34OE4YeEtx367mvWsAAA+aSURBVC/edTWj180uBISY8Mk9OpAsXtcf7g/XQgil9YpKklSqCCXNoKreRSK9qaTQoaJz72QZwTxl4i5MWfdTeODykv3vW38eT/oOTN7kLXRP771kZfRJPjqQrFxvn3D20Sz1+1lnZ01S7chtkU6gkkKHCp/+056Wo0/zXxt4daHNIBIQ3rf+PEavm83hhx6SWH8fXj/+5F/unnHxa0aDRHQxHhGpn7KzpLYKzZJaf2l18SVzAS26DG6cUnrAF+9j9PlPps4yCj1jFtKe4pNmVe3r7KNZZx2t1yynIu2glllSVX3UQcoN7iqZevr+S2HOl4ufveYbMWTKIkZPf4TurXcFngR6d99MW+SmnGiVUmpX1wyyZujVDlTTFNfSaVRS6EBpmfXxA27hnA0v69mx4RBm7nUt/3PXml7Hhk/W8YnnKt0j7TrhObUGhVok3SceBBqVFpE85LGegvRjYWZ36e4P0z34uJ6AMGgofPkJ+MZzTDriwLKjj6PdRZOuH+81lNSLKK0nUt6S2iS0zoF0OlUfdYikqqNvfuN0ugdfDkEv0rW+MRPW/5CV67Zg8r1rmTJxZM33q3aNBWj8U3latVC5daRVlST9nYJCBwkz3G99YzLf3fCKng822px91/wPq9iiJFOOVqVUWpM5nnFGM894gKhXr6F61ffHg0C051S0wVykE6hNoUOMnjqH7o89B384o7hv3YBNec9r57KK3ktkxzPEpPWPw/eQPEVFtZIy+XIZfx4li3I9pUTajdoU2liudej3XlJY6SwICOsGbgZfWczgby3n/umfLsn00uYNakRde1Lm3+w6fo2FkE6j6qMWkcfo2vmzvsshy35Y3F7tQ5i4/gccN2FfpgzdNvGcLN1KwwFpzah3z/u+8SCgNgTpNKo+ahF1raa452fwp6k92xtvwb4vn8P9049PPaVr7hJ+88Cz7LDFJhXXKY53I4X6VB9F05JlgJmqdkTK0+C1NlP3p95ewWA4fOEeGDqCVRnGDSxfvY7lq9cljkSOj0JOamiu19O6BpiJNI+CQgOkZVp164658Kdw01k92xsPZ5+Xz2FRZFrrSnXj1dTdx9PdrK6a4e+kie1E6kdBoQFyy7QWXgQ3fb1ne5Ot4AsLYdNteCH2JJ92/2MvWdiruigsBUTXICgXVPLMkJt1X5FOpaDQIqrq5ZISDLoWrmbG9+4v7q5UHdU1d0li+0HS8WkZcC29c6qp7tEAM5HGUkNzTnKZjfPun8DN3+jZHrINnHY3bLp1r0Oz1sU3Y8bQejcQq8FZJJkamltIXadvuPvHcPM3i5uvbTicIZPvSwwGceWeyqPzF0WF23riFuk8GrzWyhb8qLC4TRgQNh0BX32Kt6/9SWpACAfBRRthywmXuwxVWvS+1kF2SUtwjp46py6D9jTATKR+FBQaoOpMa8GMQjCY+63C9qbbwlefgjOWwJCtyp5a7im/XAactDZCuetXK22t5Xp1YRWR+lD1UQOUy7RKqncWzIC53+75cNNt4bQFMGSroP7/3uJH8YbV+OR1aaOSo2mJnpM0rUXSPEQi0r+pobnJRk+dQ/cRS+GWs3t2Dt0OTr0LhmyZfk5sNtOkJ/jxY4YXexclLV6TdTnNejdIa7CZSGPU0tCsoNBMd3XBLdN6thOCQVIGmjRgLGnVsnKrnyVNC10uuFxzygG5TGkhIvlRUGgTd13+dQ585qLi9nLfkiPWn8NnJoxLDABJS0bOmLe0mLHHlWtkjpYe4uckzWekhe5F2pe6pLa6O8+Hed/hwGDzlUEjOOiV7/DQ9E/yYBWXCRe5B3rNTXTsJQtTM/EZ85ZyzSkHFPdlnc8ovE8zp7QQkcZQUGiEO34It363Z3vzUXDK7ez5nYWJh6c1FMef8sP1kaM9h+KZfihLY3e0pBDeK0xHuaooEek/FBTydMcP4Nbv9WwHwYBNys8pVGngWzSDzlK/H94jHhjKTSiXVKJQA7FI/5fbOAUzu8zMVprZo5F9w81srpktDV57rwPZH9zxg8I4gzAgbD4KvrYMpjxC14IXSgZxzZi3tKpBXPHjoteKHhPdl3aPauczUkAQ6f9ya2g2s4OAV4FfuPsewb7zgJfcfbqZTQW2cPczK12rbRqab/8BzI+UDIbtBJNuKykZRFWzVkCtDb6Vup1muV6tJQSVLESaq5aGZtw9tx9gNPBoZHsxMDJ4PxJYnOU648aN85Z227nuZ2/W89P1DvfXXqx42k5n3ljT7cLzspxfr2Nqkdd1RSQbYJFXmW83epqLEe6+Inj/D2BE2oFmNsnMFpnZolWrVjUmdVXomrsEbju3UE00/5zCzi3GwJndcPpfUksHUWE1Ta0jhaPVPGnX0LxAIlKNXMcpmNlo4EbvqT5a7e7DIp+/7O4V2xVarvro1VXww7f2bG8xBibNh43L/ypJ1Snx0cNZpA1oq3VQWT2reTSuQaR1tMM4hefNbKS7rzCzkcDKBt+/Pl56uvCaMRiEknr51DLBXL0z13per65ThotIwzU6KPweOBGYHrze0OD790npU/BsWAH89901PQXXY+UwrT4mIvWWW1Aws6uAg4GtzOzvwNkUgsG1ZnYy8AxwTF73z0MtT8HlZiyNqyUzj49obiVqzxBpP7kFBXf/VMpHE/K6ZyvKOhCtlgw9qTqqlbRy2kQkmRbZqVE1T8GVehfV44laT+UiUg8KCjWq5ik4rD5KyrirrTJKW9YyT1pcR6RzaO6jBkrK/KutYmlG755Wr6YSkfpRSSEntS5Ur6fy1qJ/D+k0KinkpNYn+mqeyvNqRwgzQnV3VSlJOo+CQhvLK7MKR1hrEJpI51FQaAEahNZa9O8hnUxrNNdR2hxC1TxpN+upvNycRdC5Yw5USpJ21g5zH/Vr0frndnva1JxFIgIKCrmJZ7JZg4EGobUW/XtIp1GX1D7K2vW03Gyo0WNboRShjLBHK/x7iDSSSgp9lKXaZfKEsWWDQqt1e2yltIhIYyko5Kjd2hVERNT7qI7KrWAWL0VohTIRyZt6HzVZNZl5K/X2iQezei7PKSLtRQ3NDdLKjbfxEkstS4SKSP+goNAg5Z68WzlgiEhnUZtCh0pr04hTG4dI+6qlTUFBQXq1aTS7jUNE6qOWoKDqIxERKVJQkF5tGmrjEOlcqj4SEemnVH0kIiJ9oqAgIiJFCgoiIlKkoCAiIkUKCiIiUqSgICIiRQoKIiJSpKAgIiJFCgoiIlKkoCAiIkUKCiIiUqSgICIiRQoKIiJS1JSgYGaHmdliM3vSzKY2Iw0iItJbw4OCmQ0ALgI+COwOfMrMdm90OkREpLdmlBT2A55096fd/XXgauDIJqRDRERiBjbhntsDz0a2/w6Mjx9kZpOAScHmejN7tAFpawdbAS80OxEtQt9FD30XPfRd9Ni12hOaERQycfeZwEwAM1tU7epB/ZW+ix76Lnrou+ih76KHmVW9ZGUzqo+WAztGtncI9omISJM1IyjcD4w1szFmNgj4JPD7JqRDRERiGl595O5vmNl/ATcBA4DL3P2xCqfNzD9lbUPfRQ99Fz30XfTQd9Gj6u/C3D2PhIiISBvSiGYRESlSUBARkaKWDgqdPh2GmV1mZiujYzTMbLiZzTWzpcHrFs1MYyOY2Y5mNt/M/mpmj5nZ5GB/J34Xg83sPjN7OPgu/jvYP8bM7g3+Vq4JOnF0BDMbYGYPmtmNwXZHfhdm1m1mj5jZQ2FX1Fr+Rlo2KGg6DACuAA6L7ZsKzHP3scC8YLu/ewP4irvvDuwPfDH4v9CJ38V64FB33wt4J3CYme0PnAt0uftbgZeBk5uYxkabDDwe2e7k7+IQd39nZJxG1X8jLRsU0HQYuPsdwEux3UcCs4L3s4CjGpqoJnD3Fe7+5+D9WgoZwPZ05nfh7v5qsLlh8OPAocBvgv0d8V0AmNkOwOHA/wbbRod+Fymq/htp5aCQNB3G9k1KSysZ4e4rgvf/AEY0MzGNZmajgXcB99Kh30VQXfIQsBKYCzwFrHb3N4JDOulv5ULga8C/g+0t6dzvwoGbzeyBYJogqOFvpGWnuZDK3N3NrGP6FJvZpsB1wOnu/krhobCgk74Ld38TeKeZDQN+C+zW5CQ1hZkdAax09wfM7OBmp6cFHOjuy81sG2CumT0R/TDr30grlxQ0HUay581sJEDwurLJ6WkIM9uQQkC40t2vD3Z35HcRcvfVwHzgAGCYmYUPeZ3yt/Ju4CNm1k2hevlQYAad+V3g7suD15UUHhb2o4a/kVYOCpoOI9nvgROD9ycCNzQxLQ0R1BNfCjzu7hdEPurE72LroISAmW0MTKTQxjIfODo4rCO+C3c/y913cPfRFPKHW939eDrwuzCzIWY2NHwPvB94lBr+Rlp6RLOZfYhCnWE4HcY5TU5SQ5nZVcDBFKYCfh44G/gdcC0wCngGOMbd443R/YqZHQjcCTxCT93x1ym0K3Tad7EnhQbDARQe6q519++Y2c4UnpaHAw8Cn3b39c1LaWMF1UdnuPsRnfhdBL/zb4PNgcBsdz/HzLakyr+Rlg4KIiLSWK1cfSQiIg2moCAiIkUKCiIiUqSgICIiRQoKIiJSpKAg/V4ww+oHYvtON7OflTnnNjNr2uLvZjbNzM5o1v2lcykoSCe4isLgpqhPBvvrIpjVN+0zTScjbUNBQTrBb4DDw3n1g0n1tgPuNLOfmdmi6NoEcWb2qWCe+kfN7NzI/lfN7Hwze5jCVBPRc24zswuDee0nm9k4M7s9mKzspsjUA583s/uD9RGuM7NNcvkGRDJSUJB+LxjBeR+FtTmgUEq41gsjN78RzD2/J/DeYMRwkZltR2F+/kMprF+wr5mF0w8PAe51973c/a6EWw8Krv0j4MfA0e4+DrgMCEfnX+/u+wbrIzxOZ839Ly1IQUE6RbQKKVp1dIyZ/ZnCdAhvp7CgU9S+wG3uviqYjvlK4KDgszcpTNKX5prgdVdgDwozVz4EfJPCRG0Ae5jZnWb2CHB8kAaRplFdp3SKG4AuM9sb2CSYbnkMcAawr7u/bGZXAIOruOa6YBrrNK8FrwY85u4HJBxzBXCUuz9sZidRmOtKpGlUUpCOEKxWNp9C1U1YStiMQsa9xsxG0FO9FHUfhWqlrYLG5E8Bt1d5+8XA1mZ2ABSmATezsEQwFFgRTA1+fJXXFak7lRSkk1xFYSbJTwIET+cPAk9QWOVvQfwEd19hZlMpBBQD5rh7VVMxu/vrZnY08CMz25zC392FwGPAtyjM9roqeB1a4+8mUheaJVVERIpUfSQiIkUKCiIiUqSgICIiRQoKIiJSpKAgIiJFCgoiIlKkoCAiIkX/H2pyehvy/lTmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc8fFGSm-D99"
      },
      "source": [
        "# Entregable \n",
        "1. Encontrar el mínimo de la función *f* definida en el apartado b). Para ello, deberán encontrar primero la derivada *f'(x)* de forma analítica, y utilizarla para computar el mínimo de la función. Posteriormente, deberán corrobarar que el valor coincida con el que obtuvieron optimizando la función con gradiente descendiente. \n",
        "\n",
        "2. Compara el rendimiento de 3 perceptrones multicapa que varíen en la cantidad de neuronas en sus capas intermedia. Probar colocando 2, 10 y 200 neuronas en dichas capas, al entrenar los perceptrones durante 5000 épocas. Mostrar los resultados utilizando:\n",
        "\n",
        "* los gráficos de dispersión con la recta de regresión\n",
        "* el error medio en los datos de entrenamiento y test\n",
        "\n",
        "  Analizar la relación entre dichos resultados y la cantidad de neuronas que posee el perceptrón.\n",
        " "
      ]
    }
  ]
}