{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica 2 - Estudiantes",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmilianoLS/DiTellaML/blob/main/Deep%20Learning/Practica_2_Estudiantes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZSWq2lIEnbB"
      },
      "source": [
        "# Práctica 2: Introducción a PyTorch\n",
        "\n",
        "## a) Calculando el gradiente mediante Autograd\n",
        "\n",
        "En primer lugar, vamos a calcular del gradiente para el perceptrón simple con función de activación sigmoidea que vimos en la teoría. Pero esta vez, en lugar de realizar manualmente el proceso de backpropagation, vamos a usar el módulo `autograd` de PyTorch.\n",
        "\n",
        "La función $f(x, w)$ a la cual queremos encontrarle el gradiente es:\n",
        "\n",
        "> $f(\\mathbf{x}, \\mathbf{w}) = \\frac{1}{1 + e^{-(w_0 x_0 + w_1 x_1 + w_2)}}$\n",
        "\n",
        "Definimos entonces la función utilizando `torch.tensor` (recordar usar el parámetro `requires_grad = True` para que PyTorch guarde los gradientes) y realizamos la pasada \"forward\" para los siguientes valores de x y w:\n",
        "\n",
        "> $\\mathbf{x} = (-1, -2)$\n",
        "\n",
        "> $\\mathbf{w} = (2, -3, -3)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UczyYh5Nj2u5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11a17279-89c1-4dcb-90f9-771f0311b6a3"
      },
      "source": [
        "import torch\n",
        "\n",
        "x = ....\n",
        "w = ....\n",
        "\n",
        "f = .....\n",
        "\n",
        "print(f)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7311, grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkrbxHMukzHQ"
      },
      "source": [
        "Ahora, utilizando la función `f.backward()` computamos los gradientes $\\frac{\\partial f}{ \\partial \\mathbf{x}}$ y $\\frac{\\partial f}{ \\partial \\mathbf{w}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q477bpjr77xp"
      },
      "source": [
        "f....."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXewlL_8YHMU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "090a7089-545d-4f96-fcf6-edf5c7a18908"
      },
      "source": [
        "print(\"Gradiente df/dx = \" + str( .... ))\n",
        "print(\"Gradiente df/dw = \" + str( .... ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradiente df/dx = tensor([ 0.3932, -0.5898])\n",
            "Gradiente df/dw = tensor([-0.1966, -0.3932,  0.1966])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsXTQ9wJnK2j"
      },
      "source": [
        "## b) Minimizando una función con Gradient Descent\n",
        "\n",
        "Ahora, vamos a implementar usar el algorítmo de gradiente descendiente (utilizando Autograd para computar el gradiente) para minimizar la función cuadrática $$f(x) = 2x^2 + x + 4$$\n",
        "\n",
        "Utilizaremos la implementación `torch.optim.SGD` de gradiente descendiente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKc75VsMYS4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cd7fd0f1-5f1f-4ccc-810d-c92a059df4dd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Definir la variable que será el parámetro a optimizar\n",
        "x = torch.tensor(.....)\n",
        "\n",
        "# Definir el optimizador, indicando el parámetro a optimizar y el learning rate\n",
        "optimizer = torch.optim.SGD( .... )\n",
        "\n",
        "# Acumuladores que usaremos para guardar los valores sucesivos de x, y\n",
        "f_values = []\n",
        "x_values = []\n",
        "\n",
        "# Loop de optimización\n",
        "for i in range(1000):\n",
        "\n",
        "    # Setemos en 0 los gradientes de todos los elementos\n",
        "    optimizer.....\n",
        "       \n",
        "    # Pasada forward: ejecutar la función a minimizar\n",
        "    f = .....\n",
        "\n",
        "    print(\"X = \" + str(x) + \", f(x) = \" + str(f))\n",
        "\n",
        "    # Pasada backward: computar los gradientes\n",
        "    f......\n",
        "\n",
        "    # Actualizar los pesos dando un paso de gradiente descendiente\n",
        "    optimizer....\n",
        "\n",
        "    # Guardar los valores para luego plotearlos\n",
        "    f_values.append(f.data.item())\n",
        "    x_values.append(x.data.item())\n",
        "\n",
        "# Ploteo los valores\n",
        "plt.title(\"Optimizando la función f = 2 * x**2 + x + 4\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.plot(x_values,f_values)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X = tensor(0., requires_grad=True), f(x) = tensor(4., grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0010, requires_grad=True), f(x) = tensor(3.9990, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0020, requires_grad=True), f(x) = tensor(3.9980, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0030, requires_grad=True), f(x) = tensor(3.9970, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0040, requires_grad=True), f(x) = tensor(3.9961, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0050, requires_grad=True), f(x) = tensor(3.9951, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0059, requires_grad=True), f(x) = tensor(3.9941, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0069, requires_grad=True), f(x) = tensor(3.9932, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0079, requires_grad=True), f(x) = tensor(3.9922, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0089, requires_grad=True), f(x) = tensor(3.9913, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0098, requires_grad=True), f(x) = tensor(3.9904, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0108, requires_grad=True), f(x) = tensor(3.9894, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0117, requires_grad=True), f(x) = tensor(3.9885, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0127, requires_grad=True), f(x) = tensor(3.9876, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0136, requires_grad=True), f(x) = tensor(3.9867, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0146, requires_grad=True), f(x) = tensor(3.9858, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0155, requires_grad=True), f(x) = tensor(3.9850, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0165, requires_grad=True), f(x) = tensor(3.9841, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0174, requires_grad=True), f(x) = tensor(3.9832, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0183, requires_grad=True), f(x) = tensor(3.9823, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0193, requires_grad=True), f(x) = tensor(3.9815, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0202, requires_grad=True), f(x) = tensor(3.9806, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0211, requires_grad=True), f(x) = tensor(3.9798, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0220, requires_grad=True), f(x) = tensor(3.9790, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0229, requires_grad=True), f(x) = tensor(3.9781, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0238, requires_grad=True), f(x) = tensor(3.9773, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0247, requires_grad=True), f(x) = tensor(3.9765, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0256, requires_grad=True), f(x) = tensor(3.9757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0265, requires_grad=True), f(x) = tensor(3.9749, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0274, requires_grad=True), f(x) = tensor(3.9741, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0283, requires_grad=True), f(x) = tensor(3.9733, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0292, requires_grad=True), f(x) = tensor(3.9725, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0301, requires_grad=True), f(x) = tensor(3.9717, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0310, requires_grad=True), f(x) = tensor(3.9709, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0318, requires_grad=True), f(x) = tensor(3.9702, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0327, requires_grad=True), f(x) = tensor(3.9694, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0336, requires_grad=True), f(x) = tensor(3.9687, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0345, requires_grad=True), f(x) = tensor(3.9679, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0353, requires_grad=True), f(x) = tensor(3.9672, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0362, requires_grad=True), f(x) = tensor(3.9664, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0370, requires_grad=True), f(x) = tensor(3.9657, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0379, requires_grad=True), f(x) = tensor(3.9650, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0387, requires_grad=True), f(x) = tensor(3.9643, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0396, requires_grad=True), f(x) = tensor(3.9636, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0404, requires_grad=True), f(x) = tensor(3.9628, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0413, requires_grad=True), f(x) = tensor(3.9621, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0421, requires_grad=True), f(x) = tensor(3.9615, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0429, requires_grad=True), f(x) = tensor(3.9608, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0438, requires_grad=True), f(x) = tensor(3.9601, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0446, requires_grad=True), f(x) = tensor(3.9594, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0454, requires_grad=True), f(x) = tensor(3.9587, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0462, requires_grad=True), f(x) = tensor(3.9581, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0470, requires_grad=True), f(x) = tensor(3.9574, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0478, requires_grad=True), f(x) = tensor(3.9567, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0487, requires_grad=True), f(x) = tensor(3.9561, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0495, requires_grad=True), f(x) = tensor(3.9554, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0503, requires_grad=True), f(x) = tensor(3.9548, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0511, requires_grad=True), f(x) = tensor(3.9542, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0519, requires_grad=True), f(x) = tensor(3.9535, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0526, requires_grad=True), f(x) = tensor(3.9529, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0534, requires_grad=True), f(x) = tensor(3.9523, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0542, requires_grad=True), f(x) = tensor(3.9517, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0550, requires_grad=True), f(x) = tensor(3.9510, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0558, requires_grad=True), f(x) = tensor(3.9504, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0566, requires_grad=True), f(x) = tensor(3.9498, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0573, requires_grad=True), f(x) = tensor(3.9492, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0581, requires_grad=True), f(x) = tensor(3.9486, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0589, requires_grad=True), f(x) = tensor(3.9481, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0596, requires_grad=True), f(x) = tensor(3.9475, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0604, requires_grad=True), f(x) = tensor(3.9469, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0612, requires_grad=True), f(x) = tensor(3.9463, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0619, requires_grad=True), f(x) = tensor(3.9458, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0627, requires_grad=True), f(x) = tensor(3.9452, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0634, requires_grad=True), f(x) = tensor(3.9446, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0642, requires_grad=True), f(x) = tensor(3.9441, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0649, requires_grad=True), f(x) = tensor(3.9435, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0656, requires_grad=True), f(x) = tensor(3.9430, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0664, requires_grad=True), f(x) = tensor(3.9424, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0671, requires_grad=True), f(x) = tensor(3.9419, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0679, requires_grad=True), f(x) = tensor(3.9414, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0686, requires_grad=True), f(x) = tensor(3.9408, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0693, requires_grad=True), f(x) = tensor(3.9403, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0700, requires_grad=True), f(x) = tensor(3.9398, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0707, requires_grad=True), f(x) = tensor(3.9393, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0715, requires_grad=True), f(x) = tensor(3.9387, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0722, requires_grad=True), f(x) = tensor(3.9382, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0729, requires_grad=True), f(x) = tensor(3.9377, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0736, requires_grad=True), f(x) = tensor(3.9372, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0743, requires_grad=True), f(x) = tensor(3.9367, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0750, requires_grad=True), f(x) = tensor(3.9362, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0757, requires_grad=True), f(x) = tensor(3.9358, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0764, requires_grad=True), f(x) = tensor(3.9353, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0771, requires_grad=True), f(x) = tensor(3.9348, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0778, requires_grad=True), f(x) = tensor(3.9343, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0785, requires_grad=True), f(x) = tensor(3.9338, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0792, requires_grad=True), f(x) = tensor(3.9334, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0798, requires_grad=True), f(x) = tensor(3.9329, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0805, requires_grad=True), f(x) = tensor(3.9324, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0812, requires_grad=True), f(x) = tensor(3.9320, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0819, requires_grad=True), f(x) = tensor(3.9315, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0826, requires_grad=True), f(x) = tensor(3.9311, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0832, requires_grad=True), f(x) = tensor(3.9306, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0839, requires_grad=True), f(x) = tensor(3.9302, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0846, requires_grad=True), f(x) = tensor(3.9297, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0852, requires_grad=True), f(x) = tensor(3.9293, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0859, requires_grad=True), f(x) = tensor(3.9289, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0865, requires_grad=True), f(x) = tensor(3.9284, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0872, requires_grad=True), f(x) = tensor(3.9280, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0878, requires_grad=True), f(x) = tensor(3.9276, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0885, requires_grad=True), f(x) = tensor(3.9272, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0891, requires_grad=True), f(x) = tensor(3.9268, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0898, requires_grad=True), f(x) = tensor(3.9263, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0904, requires_grad=True), f(x) = tensor(3.9259, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0911, requires_grad=True), f(x) = tensor(3.9255, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0917, requires_grad=True), f(x) = tensor(3.9251, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0923, requires_grad=True), f(x) = tensor(3.9247, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0930, requires_grad=True), f(x) = tensor(3.9243, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0936, requires_grad=True), f(x) = tensor(3.9239, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0942, requires_grad=True), f(x) = tensor(3.9235, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0948, requires_grad=True), f(x) = tensor(3.9232, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0955, requires_grad=True), f(x) = tensor(3.9228, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0961, requires_grad=True), f(x) = tensor(3.9224, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0967, requires_grad=True), f(x) = tensor(3.9220, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0973, requires_grad=True), f(x) = tensor(3.9216, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0979, requires_grad=True), f(x) = tensor(3.9213, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0985, requires_grad=True), f(x) = tensor(3.9209, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0991, requires_grad=True), f(x) = tensor(3.9205, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.0997, requires_grad=True), f(x) = tensor(3.9202, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1003, requires_grad=True), f(x) = tensor(3.9198, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1009, requires_grad=True), f(x) = tensor(3.9194, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1015, requires_grad=True), f(x) = tensor(3.9191, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1021, requires_grad=True), f(x) = tensor(3.9187, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1027, requires_grad=True), f(x) = tensor(3.9184, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1033, requires_grad=True), f(x) = tensor(3.9180, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1039, requires_grad=True), f(x) = tensor(3.9177, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1045, requires_grad=True), f(x) = tensor(3.9174, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1051, requires_grad=True), f(x) = tensor(3.9170, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1056, requires_grad=True), f(x) = tensor(3.9167, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1062, requires_grad=True), f(x) = tensor(3.9164, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1068, requires_grad=True), f(x) = tensor(3.9160, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1074, requires_grad=True), f(x) = tensor(3.9157, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1079, requires_grad=True), f(x) = tensor(3.9154, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1085, requires_grad=True), f(x) = tensor(3.9150, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1091, requires_grad=True), f(x) = tensor(3.9147, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1096, requires_grad=True), f(x) = tensor(3.9144, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1102, requires_grad=True), f(x) = tensor(3.9141, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1107, requires_grad=True), f(x) = tensor(3.9138, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1113, requires_grad=True), f(x) = tensor(3.9135, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1119, requires_grad=True), f(x) = tensor(3.9132, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1124, requires_grad=True), f(x) = tensor(3.9129, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1130, requires_grad=True), f(x) = tensor(3.9126, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1135, requires_grad=True), f(x) = tensor(3.9123, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1141, requires_grad=True), f(x) = tensor(3.9120, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1146, requires_grad=True), f(x) = tensor(3.9117, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1151, requires_grad=True), f(x) = tensor(3.9114, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1157, requires_grad=True), f(x) = tensor(3.9111, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1162, requires_grad=True), f(x) = tensor(3.9108, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1168, requires_grad=True), f(x) = tensor(3.9105, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1173, requires_grad=True), f(x) = tensor(3.9102, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1178, requires_grad=True), f(x) = tensor(3.9099, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1183, requires_grad=True), f(x) = tensor(3.9097, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1189, requires_grad=True), f(x) = tensor(3.9094, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1194, requires_grad=True), f(x) = tensor(3.9091, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1199, requires_grad=True), f(x) = tensor(3.9088, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1204, requires_grad=True), f(x) = tensor(3.9086, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1210, requires_grad=True), f(x) = tensor(3.9083, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1215, requires_grad=True), f(x) = tensor(3.9080, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1220, requires_grad=True), f(x) = tensor(3.9078, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1225, requires_grad=True), f(x) = tensor(3.9075, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1230, requires_grad=True), f(x) = tensor(3.9073, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1235, requires_grad=True), f(x) = tensor(3.9070, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1240, requires_grad=True), f(x) = tensor(3.9067, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1245, requires_grad=True), f(x) = tensor(3.9065, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1250, requires_grad=True), f(x) = tensor(3.9062, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1255, requires_grad=True), f(x) = tensor(3.9060, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1260, requires_grad=True), f(x) = tensor(3.9057, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1265, requires_grad=True), f(x) = tensor(3.9055, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1270, requires_grad=True), f(x) = tensor(3.9052, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1275, requires_grad=True), f(x) = tensor(3.9050, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1280, requires_grad=True), f(x) = tensor(3.9048, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1285, requires_grad=True), f(x) = tensor(3.9045, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1290, requires_grad=True), f(x) = tensor(3.9043, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1295, requires_grad=True), f(x) = tensor(3.9041, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1299, requires_grad=True), f(x) = tensor(3.9038, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1304, requires_grad=True), f(x) = tensor(3.9036, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1309, requires_grad=True), f(x) = tensor(3.9034, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1314, requires_grad=True), f(x) = tensor(3.9031, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1318, requires_grad=True), f(x) = tensor(3.9029, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1323, requires_grad=True), f(x) = tensor(3.9027, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1328, requires_grad=True), f(x) = tensor(3.9025, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1333, requires_grad=True), f(x) = tensor(3.9023, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1337, requires_grad=True), f(x) = tensor(3.9020, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1342, requires_grad=True), f(x) = tensor(3.9018, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1347, requires_grad=True), f(x) = tensor(3.9016, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1351, requires_grad=True), f(x) = tensor(3.9014, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1356, requires_grad=True), f(x) = tensor(3.9012, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1360, requires_grad=True), f(x) = tensor(3.9010, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1365, requires_grad=True), f(x) = tensor(3.9008, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1369, requires_grad=True), f(x) = tensor(3.9006, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1374, requires_grad=True), f(x) = tensor(3.9004, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1378, requires_grad=True), f(x) = tensor(3.9002, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1383, requires_grad=True), f(x) = tensor(3.9000, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1387, requires_grad=True), f(x) = tensor(3.8998, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1392, requires_grad=True), f(x) = tensor(3.8996, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1396, requires_grad=True), f(x) = tensor(3.8994, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1401, requires_grad=True), f(x) = tensor(3.8992, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1405, requires_grad=True), f(x) = tensor(3.8990, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1410, requires_grad=True), f(x) = tensor(3.8988, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1414, requires_grad=True), f(x) = tensor(3.8986, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1418, requires_grad=True), f(x) = tensor(3.8984, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1423, requires_grad=True), f(x) = tensor(3.8982, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1427, requires_grad=True), f(x) = tensor(3.8980, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1431, requires_grad=True), f(x) = tensor(3.8978, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1435, requires_grad=True), f(x) = tensor(3.8977, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1440, requires_grad=True), f(x) = tensor(3.8975, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1444, requires_grad=True), f(x) = tensor(3.8973, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1448, requires_grad=True), f(x) = tensor(3.8971, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1452, requires_grad=True), f(x) = tensor(3.8970, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1457, requires_grad=True), f(x) = tensor(3.8968, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1461, requires_grad=True), f(x) = tensor(3.8966, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1465, requires_grad=True), f(x) = tensor(3.8964, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1469, requires_grad=True), f(x) = tensor(3.8963, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1473, requires_grad=True), f(x) = tensor(3.8961, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1477, requires_grad=True), f(x) = tensor(3.8959, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1481, requires_grad=True), f(x) = tensor(3.8958, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1485, requires_grad=True), f(x) = tensor(3.8956, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1489, requires_grad=True), f(x) = tensor(3.8954, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1494, requires_grad=True), f(x) = tensor(3.8953, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1498, requires_grad=True), f(x) = tensor(3.8951, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1502, requires_grad=True), f(x) = tensor(3.8949, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1506, requires_grad=True), f(x) = tensor(3.8948, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1510, requires_grad=True), f(x) = tensor(3.8946, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1513, requires_grad=True), f(x) = tensor(3.8945, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1517, requires_grad=True), f(x) = tensor(3.8943, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1521, requires_grad=True), f(x) = tensor(3.8942, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1525, requires_grad=True), f(x) = tensor(3.8940, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1529, requires_grad=True), f(x) = tensor(3.8939, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1533, requires_grad=True), f(x) = tensor(3.8937, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1537, requires_grad=True), f(x) = tensor(3.8936, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1541, requires_grad=True), f(x) = tensor(3.8934, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1545, requires_grad=True), f(x) = tensor(3.8933, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1548, requires_grad=True), f(x) = tensor(3.8931, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1552, requires_grad=True), f(x) = tensor(3.8930, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1556, requires_grad=True), f(x) = tensor(3.8928, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1560, requires_grad=True), f(x) = tensor(3.8927, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1564, requires_grad=True), f(x) = tensor(3.8925, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1567, requires_grad=True), f(x) = tensor(3.8924, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1571, requires_grad=True), f(x) = tensor(3.8923, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1575, requires_grad=True), f(x) = tensor(3.8921, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1578, requires_grad=True), f(x) = tensor(3.8920, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1582, requires_grad=True), f(x) = tensor(3.8918, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1586, requires_grad=True), f(x) = tensor(3.8917, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1589, requires_grad=True), f(x) = tensor(3.8916, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1593, requires_grad=True), f(x) = tensor(3.8914, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1597, requires_grad=True), f(x) = tensor(3.8913, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1600, requires_grad=True), f(x) = tensor(3.8912, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1604, requires_grad=True), f(x) = tensor(3.8911, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1608, requires_grad=True), f(x) = tensor(3.8909, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1611, requires_grad=True), f(x) = tensor(3.8908, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1615, requires_grad=True), f(x) = tensor(3.8907, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1618, requires_grad=True), f(x) = tensor(3.8906, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1622, requires_grad=True), f(x) = tensor(3.8904, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1625, requires_grad=True), f(x) = tensor(3.8903, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1629, requires_grad=True), f(x) = tensor(3.8902, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1632, requires_grad=True), f(x) = tensor(3.8901, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1636, requires_grad=True), f(x) = tensor(3.8899, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1639, requires_grad=True), f(x) = tensor(3.8898, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1643, requires_grad=True), f(x) = tensor(3.8897, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1646, requires_grad=True), f(x) = tensor(3.8896, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1649, requires_grad=True), f(x) = tensor(3.8895, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1653, requires_grad=True), f(x) = tensor(3.8894, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1656, requires_grad=True), f(x) = tensor(3.8892, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1660, requires_grad=True), f(x) = tensor(3.8891, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1663, requires_grad=True), f(x) = tensor(3.8890, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1666, requires_grad=True), f(x) = tensor(3.8889, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1670, requires_grad=True), f(x) = tensor(3.8888, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1673, requires_grad=True), f(x) = tensor(3.8887, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1676, requires_grad=True), f(x) = tensor(3.8886, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1680, requires_grad=True), f(x) = tensor(3.8885, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1683, requires_grad=True), f(x) = tensor(3.8884, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1686, requires_grad=True), f(x) = tensor(3.8882, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1689, requires_grad=True), f(x) = tensor(3.8881, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1693, requires_grad=True), f(x) = tensor(3.8880, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1696, requires_grad=True), f(x) = tensor(3.8879, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1699, requires_grad=True), f(x) = tensor(3.8878, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1702, requires_grad=True), f(x) = tensor(3.8877, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1705, requires_grad=True), f(x) = tensor(3.8876, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1709, requires_grad=True), f(x) = tensor(3.8875, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1712, requires_grad=True), f(x) = tensor(3.8874, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1715, requires_grad=True), f(x) = tensor(3.8873, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1718, requires_grad=True), f(x) = tensor(3.8872, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1721, requires_grad=True), f(x) = tensor(3.8871, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1724, requires_grad=True), f(x) = tensor(3.8870, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1727, requires_grad=True), f(x) = tensor(3.8869, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1731, requires_grad=True), f(x) = tensor(3.8868, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1734, requires_grad=True), f(x) = tensor(3.8867, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1737, requires_grad=True), f(x) = tensor(3.8867, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1740, requires_grad=True), f(x) = tensor(3.8866, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1743, requires_grad=True), f(x) = tensor(3.8865, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1746, requires_grad=True), f(x) = tensor(3.8864, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1749, requires_grad=True), f(x) = tensor(3.8863, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1752, requires_grad=True), f(x) = tensor(3.8862, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1755, requires_grad=True), f(x) = tensor(3.8861, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1758, requires_grad=True), f(x) = tensor(3.8860, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1761, requires_grad=True), f(x) = tensor(3.8859, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1764, requires_grad=True), f(x) = tensor(3.8858, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1767, requires_grad=True), f(x) = tensor(3.8858, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1770, requires_grad=True), f(x) = tensor(3.8857, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1773, requires_grad=True), f(x) = tensor(3.8856, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1775, requires_grad=True), f(x) = tensor(3.8855, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1778, requires_grad=True), f(x) = tensor(3.8854, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1781, requires_grad=True), f(x) = tensor(3.8853, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1784, requires_grad=True), f(x) = tensor(3.8853, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1787, requires_grad=True), f(x) = tensor(3.8852, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1790, requires_grad=True), f(x) = tensor(3.8851, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1793, requires_grad=True), f(x) = tensor(3.8850, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1795, requires_grad=True), f(x) = tensor(3.8849, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1798, requires_grad=True), f(x) = tensor(3.8848, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1801, requires_grad=True), f(x) = tensor(3.8848, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1804, requires_grad=True), f(x) = tensor(3.8847, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1807, requires_grad=True), f(x) = tensor(3.8846, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1809, requires_grad=True), f(x) = tensor(3.8845, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1812, requires_grad=True), f(x) = tensor(3.8845, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1815, requires_grad=True), f(x) = tensor(3.8844, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1818, requires_grad=True), f(x) = tensor(3.8843, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1820, requires_grad=True), f(x) = tensor(3.8842, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1823, requires_grad=True), f(x) = tensor(3.8842, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1826, requires_grad=True), f(x) = tensor(3.8841, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1829, requires_grad=True), f(x) = tensor(3.8840, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1831, requires_grad=True), f(x) = tensor(3.8839, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1834, requires_grad=True), f(x) = tensor(3.8839, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1837, requires_grad=True), f(x) = tensor(3.8838, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1839, requires_grad=True), f(x) = tensor(3.8837, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1842, requires_grad=True), f(x) = tensor(3.8837, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1845, requires_grad=True), f(x) = tensor(3.8836, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1847, requires_grad=True), f(x) = tensor(3.8835, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1850, requires_grad=True), f(x) = tensor(3.8835, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1852, requires_grad=True), f(x) = tensor(3.8834, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1855, requires_grad=True), f(x) = tensor(3.8833, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1858, requires_grad=True), f(x) = tensor(3.8833, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1860, requires_grad=True), f(x) = tensor(3.8832, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1863, requires_grad=True), f(x) = tensor(3.8831, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1865, requires_grad=True), f(x) = tensor(3.8831, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1868, requires_grad=True), f(x) = tensor(3.8830, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1870, requires_grad=True), f(x) = tensor(3.8829, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1873, requires_grad=True), f(x) = tensor(3.8829, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1875, requires_grad=True), f(x) = tensor(3.8828, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1878, requires_grad=True), f(x) = tensor(3.8827, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1880, requires_grad=True), f(x) = tensor(3.8827, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1883, requires_grad=True), f(x) = tensor(3.8826, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1885, requires_grad=True), f(x) = tensor(3.8826, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1888, requires_grad=True), f(x) = tensor(3.8825, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1890, requires_grad=True), f(x) = tensor(3.8824, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1893, requires_grad=True), f(x) = tensor(3.8824, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1895, requires_grad=True), f(x) = tensor(3.8823, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1897, requires_grad=True), f(x) = tensor(3.8823, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1900, requires_grad=True), f(x) = tensor(3.8822, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1902, requires_grad=True), f(x) = tensor(3.8821, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1905, requires_grad=True), f(x) = tensor(3.8821, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1907, requires_grad=True), f(x) = tensor(3.8820, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1909, requires_grad=True), f(x) = tensor(3.8820, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1912, requires_grad=True), f(x) = tensor(3.8819, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1914, requires_grad=True), f(x) = tensor(3.8819, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1916, requires_grad=True), f(x) = tensor(3.8818, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1919, requires_grad=True), f(x) = tensor(3.8818, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1921, requires_grad=True), f(x) = tensor(3.8817, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1923, requires_grad=True), f(x) = tensor(3.8816, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1926, requires_grad=True), f(x) = tensor(3.8816, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1928, requires_grad=True), f(x) = tensor(3.8815, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1930, requires_grad=True), f(x) = tensor(3.8815, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1933, requires_grad=True), f(x) = tensor(3.8814, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1935, requires_grad=True), f(x) = tensor(3.8814, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1937, requires_grad=True), f(x) = tensor(3.8813, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1939, requires_grad=True), f(x) = tensor(3.8813, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1942, requires_grad=True), f(x) = tensor(3.8812, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1944, requires_grad=True), f(x) = tensor(3.8812, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1946, requires_grad=True), f(x) = tensor(3.8811, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1948, requires_grad=True), f(x) = tensor(3.8811, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1950, requires_grad=True), f(x) = tensor(3.8810, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1953, requires_grad=True), f(x) = tensor(3.8810, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1955, requires_grad=True), f(x) = tensor(3.8809, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1957, requires_grad=True), f(x) = tensor(3.8809, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1959, requires_grad=True), f(x) = tensor(3.8808, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1961, requires_grad=True), f(x) = tensor(3.8808, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1964, requires_grad=True), f(x) = tensor(3.8808, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1966, requires_grad=True), f(x) = tensor(3.8807, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1968, requires_grad=True), f(x) = tensor(3.8807, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1970, requires_grad=True), f(x) = tensor(3.8806, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1972, requires_grad=True), f(x) = tensor(3.8806, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1974, requires_grad=True), f(x) = tensor(3.8805, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1976, requires_grad=True), f(x) = tensor(3.8805, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1978, requires_grad=True), f(x) = tensor(3.8804, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1980, requires_grad=True), f(x) = tensor(3.8804, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1983, requires_grad=True), f(x) = tensor(3.8804, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1985, requires_grad=True), f(x) = tensor(3.8803, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1987, requires_grad=True), f(x) = tensor(3.8803, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1989, requires_grad=True), f(x) = tensor(3.8802, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1991, requires_grad=True), f(x) = tensor(3.8802, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1993, requires_grad=True), f(x) = tensor(3.8801, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1995, requires_grad=True), f(x) = tensor(3.8801, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1997, requires_grad=True), f(x) = tensor(3.8801, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.1999, requires_grad=True), f(x) = tensor(3.8800, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2001, requires_grad=True), f(x) = tensor(3.8800, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2003, requires_grad=True), f(x) = tensor(3.8799, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2005, requires_grad=True), f(x) = tensor(3.8799, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2007, requires_grad=True), f(x) = tensor(3.8799, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2009, requires_grad=True), f(x) = tensor(3.8798, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2011, requires_grad=True), f(x) = tensor(3.8798, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2013, requires_grad=True), f(x) = tensor(3.8797, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2015, requires_grad=True), f(x) = tensor(3.8797, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2017, requires_grad=True), f(x) = tensor(3.8797, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2019, requires_grad=True), f(x) = tensor(3.8796, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2021, requires_grad=True), f(x) = tensor(3.8796, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2022, requires_grad=True), f(x) = tensor(3.8796, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2024, requires_grad=True), f(x) = tensor(3.8795, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2026, requires_grad=True), f(x) = tensor(3.8795, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2028, requires_grad=True), f(x) = tensor(3.8795, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2030, requires_grad=True), f(x) = tensor(3.8794, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2032, requires_grad=True), f(x) = tensor(3.8794, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2034, requires_grad=True), f(x) = tensor(3.8793, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2036, requires_grad=True), f(x) = tensor(3.8793, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2037, requires_grad=True), f(x) = tensor(3.8793, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2039, requires_grad=True), f(x) = tensor(3.8792, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2041, requires_grad=True), f(x) = tensor(3.8792, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2043, requires_grad=True), f(x) = tensor(3.8792, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2045, requires_grad=True), f(x) = tensor(3.8791, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2047, requires_grad=True), f(x) = tensor(3.8791, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2048, requires_grad=True), f(x) = tensor(3.8791, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2050, requires_grad=True), f(x) = tensor(3.8790, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2052, requires_grad=True), f(x) = tensor(3.8790, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2054, requires_grad=True), f(x) = tensor(3.8790, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2056, requires_grad=True), f(x) = tensor(3.8789, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2057, requires_grad=True), f(x) = tensor(3.8789, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2059, requires_grad=True), f(x) = tensor(3.8789, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2061, requires_grad=True), f(x) = tensor(3.8789, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2063, requires_grad=True), f(x) = tensor(3.8788, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2064, requires_grad=True), f(x) = tensor(3.8788, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2066, requires_grad=True), f(x) = tensor(3.8788, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2068, requires_grad=True), f(x) = tensor(3.8787, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2070, requires_grad=True), f(x) = tensor(3.8787, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2071, requires_grad=True), f(x) = tensor(3.8787, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2073, requires_grad=True), f(x) = tensor(3.8786, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2075, requires_grad=True), f(x) = tensor(3.8786, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2077, requires_grad=True), f(x) = tensor(3.8786, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2078, requires_grad=True), f(x) = tensor(3.8786, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2080, requires_grad=True), f(x) = tensor(3.8785, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2082, requires_grad=True), f(x) = tensor(3.8785, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2083, requires_grad=True), f(x) = tensor(3.8785, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2085, requires_grad=True), f(x) = tensor(3.8784, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2087, requires_grad=True), f(x) = tensor(3.8784, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2088, requires_grad=True), f(x) = tensor(3.8784, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2090, requires_grad=True), f(x) = tensor(3.8784, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2092, requires_grad=True), f(x) = tensor(3.8783, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2093, requires_grad=True), f(x) = tensor(3.8783, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2095, requires_grad=True), f(x) = tensor(3.8783, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2096, requires_grad=True), f(x) = tensor(3.8783, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2098, requires_grad=True), f(x) = tensor(3.8782, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2100, requires_grad=True), f(x) = tensor(3.8782, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2101, requires_grad=True), f(x) = tensor(3.8782, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2103, requires_grad=True), f(x) = tensor(3.8782, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2104, requires_grad=True), f(x) = tensor(3.8781, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2106, requires_grad=True), f(x) = tensor(3.8781, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2108, requires_grad=True), f(x) = tensor(3.8781, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2109, requires_grad=True), f(x) = tensor(3.8781, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2111, requires_grad=True), f(x) = tensor(3.8780, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2112, requires_grad=True), f(x) = tensor(3.8780, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2114, requires_grad=True), f(x) = tensor(3.8780, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2115, requires_grad=True), f(x) = tensor(3.8780, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2117, requires_grad=True), f(x) = tensor(3.8779, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2118, requires_grad=True), f(x) = tensor(3.8779, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2120, requires_grad=True), f(x) = tensor(3.8779, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2121, requires_grad=True), f(x) = tensor(3.8779, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2123, requires_grad=True), f(x) = tensor(3.8778, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2125, requires_grad=True), f(x) = tensor(3.8778, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2126, requires_grad=True), f(x) = tensor(3.8778, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2128, requires_grad=True), f(x) = tensor(3.8778, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2129, requires_grad=True), f(x) = tensor(3.8778, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2130, requires_grad=True), f(x) = tensor(3.8777, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2132, requires_grad=True), f(x) = tensor(3.8777, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2133, requires_grad=True), f(x) = tensor(3.8777, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2135, requires_grad=True), f(x) = tensor(3.8777, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2136, requires_grad=True), f(x) = tensor(3.8776, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2138, requires_grad=True), f(x) = tensor(3.8776, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2139, requires_grad=True), f(x) = tensor(3.8776, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2141, requires_grad=True), f(x) = tensor(3.8776, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2142, requires_grad=True), f(x) = tensor(3.8776, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2144, requires_grad=True), f(x) = tensor(3.8775, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2145, requires_grad=True), f(x) = tensor(3.8775, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2146, requires_grad=True), f(x) = tensor(3.8775, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2148, requires_grad=True), f(x) = tensor(3.8775, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2149, requires_grad=True), f(x) = tensor(3.8775, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2151, requires_grad=True), f(x) = tensor(3.8774, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2152, requires_grad=True), f(x) = tensor(3.8774, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2153, requires_grad=True), f(x) = tensor(3.8774, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2155, requires_grad=True), f(x) = tensor(3.8774, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2156, requires_grad=True), f(x) = tensor(3.8774, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2158, requires_grad=True), f(x) = tensor(3.8773, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2159, requires_grad=True), f(x) = tensor(3.8773, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2160, requires_grad=True), f(x) = tensor(3.8773, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2162, requires_grad=True), f(x) = tensor(3.8773, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2163, requires_grad=True), f(x) = tensor(3.8773, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2164, requires_grad=True), f(x) = tensor(3.8773, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2166, requires_grad=True), f(x) = tensor(3.8772, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2167, requires_grad=True), f(x) = tensor(3.8772, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2168, requires_grad=True), f(x) = tensor(3.8772, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2170, requires_grad=True), f(x) = tensor(3.8772, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2171, requires_grad=True), f(x) = tensor(3.8772, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2172, requires_grad=True), f(x) = tensor(3.8771, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2174, requires_grad=True), f(x) = tensor(3.8771, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2175, requires_grad=True), f(x) = tensor(3.8771, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2176, requires_grad=True), f(x) = tensor(3.8771, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2178, requires_grad=True), f(x) = tensor(3.8771, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2179, requires_grad=True), f(x) = tensor(3.8771, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2180, requires_grad=True), f(x) = tensor(3.8770, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2181, requires_grad=True), f(x) = tensor(3.8770, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2183, requires_grad=True), f(x) = tensor(3.8770, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2184, requires_grad=True), f(x) = tensor(3.8770, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2185, requires_grad=True), f(x) = tensor(3.8770, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2186, requires_grad=True), f(x) = tensor(3.8770, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2188, requires_grad=True), f(x) = tensor(3.8770, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2189, requires_grad=True), f(x) = tensor(3.8769, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2190, requires_grad=True), f(x) = tensor(3.8769, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2191, requires_grad=True), f(x) = tensor(3.8769, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2193, requires_grad=True), f(x) = tensor(3.8769, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2194, requires_grad=True), f(x) = tensor(3.8769, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2195, requires_grad=True), f(x) = tensor(3.8769, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2196, requires_grad=True), f(x) = tensor(3.8768, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2198, requires_grad=True), f(x) = tensor(3.8768, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2199, requires_grad=True), f(x) = tensor(3.8768, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2200, requires_grad=True), f(x) = tensor(3.8768, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2201, requires_grad=True), f(x) = tensor(3.8768, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2202, requires_grad=True), f(x) = tensor(3.8768, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2204, requires_grad=True), f(x) = tensor(3.8768, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2205, requires_grad=True), f(x) = tensor(3.8767, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2206, requires_grad=True), f(x) = tensor(3.8767, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2207, requires_grad=True), f(x) = tensor(3.8767, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2208, requires_grad=True), f(x) = tensor(3.8767, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2209, requires_grad=True), f(x) = tensor(3.8767, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2211, requires_grad=True), f(x) = tensor(3.8767, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2212, requires_grad=True), f(x) = tensor(3.8767, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2213, requires_grad=True), f(x) = tensor(3.8766, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2214, requires_grad=True), f(x) = tensor(3.8766, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2215, requires_grad=True), f(x) = tensor(3.8766, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2216, requires_grad=True), f(x) = tensor(3.8766, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2217, requires_grad=True), f(x) = tensor(3.8766, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2219, requires_grad=True), f(x) = tensor(3.8766, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2220, requires_grad=True), f(x) = tensor(3.8766, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2221, requires_grad=True), f(x) = tensor(3.8766, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2222, requires_grad=True), f(x) = tensor(3.8765, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2223, requires_grad=True), f(x) = tensor(3.8765, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2224, requires_grad=True), f(x) = tensor(3.8765, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2225, requires_grad=True), f(x) = tensor(3.8765, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2226, requires_grad=True), f(x) = tensor(3.8765, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2228, requires_grad=True), f(x) = tensor(3.8765, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2229, requires_grad=True), f(x) = tensor(3.8765, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2230, requires_grad=True), f(x) = tensor(3.8765, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2231, requires_grad=True), f(x) = tensor(3.8764, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2232, requires_grad=True), f(x) = tensor(3.8764, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2233, requires_grad=True), f(x) = tensor(3.8764, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2234, requires_grad=True), f(x) = tensor(3.8764, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2235, requires_grad=True), f(x) = tensor(3.8764, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2236, requires_grad=True), f(x) = tensor(3.8764, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2237, requires_grad=True), f(x) = tensor(3.8764, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2238, requires_grad=True), f(x) = tensor(3.8764, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2239, requires_grad=True), f(x) = tensor(3.8764, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2240, requires_grad=True), f(x) = tensor(3.8763, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2241, requires_grad=True), f(x) = tensor(3.8763, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2242, requires_grad=True), f(x) = tensor(3.8763, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2243, requires_grad=True), f(x) = tensor(3.8763, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2244, requires_grad=True), f(x) = tensor(3.8763, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2245, requires_grad=True), f(x) = tensor(3.8763, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2246, requires_grad=True), f(x) = tensor(3.8763, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2247, requires_grad=True), f(x) = tensor(3.8763, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2248, requires_grad=True), f(x) = tensor(3.8763, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2250, requires_grad=True), f(x) = tensor(3.8763, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2251, requires_grad=True), f(x) = tensor(3.8762, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2252, requires_grad=True), f(x) = tensor(3.8762, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2252, requires_grad=True), f(x) = tensor(3.8762, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2253, requires_grad=True), f(x) = tensor(3.8762, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2254, requires_grad=True), f(x) = tensor(3.8762, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2255, requires_grad=True), f(x) = tensor(3.8762, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2256, requires_grad=True), f(x) = tensor(3.8762, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2257, requires_grad=True), f(x) = tensor(3.8762, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2258, requires_grad=True), f(x) = tensor(3.8762, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2259, requires_grad=True), f(x) = tensor(3.8762, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2260, requires_grad=True), f(x) = tensor(3.8761, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2261, requires_grad=True), f(x) = tensor(3.8761, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2262, requires_grad=True), f(x) = tensor(3.8761, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2263, requires_grad=True), f(x) = tensor(3.8761, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2264, requires_grad=True), f(x) = tensor(3.8761, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2265, requires_grad=True), f(x) = tensor(3.8761, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2266, requires_grad=True), f(x) = tensor(3.8761, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2267, requires_grad=True), f(x) = tensor(3.8761, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2268, requires_grad=True), f(x) = tensor(3.8761, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2269, requires_grad=True), f(x) = tensor(3.8761, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2270, requires_grad=True), f(x) = tensor(3.8761, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2271, requires_grad=True), f(x) = tensor(3.8761, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2272, requires_grad=True), f(x) = tensor(3.8760, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2272, requires_grad=True), f(x) = tensor(3.8760, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2273, requires_grad=True), f(x) = tensor(3.8760, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2274, requires_grad=True), f(x) = tensor(3.8760, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2275, requires_grad=True), f(x) = tensor(3.8760, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2276, requires_grad=True), f(x) = tensor(3.8760, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2277, requires_grad=True), f(x) = tensor(3.8760, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2278, requires_grad=True), f(x) = tensor(3.8760, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2279, requires_grad=True), f(x) = tensor(3.8760, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2280, requires_grad=True), f(x) = tensor(3.8760, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2281, requires_grad=True), f(x) = tensor(3.8760, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2281, requires_grad=True), f(x) = tensor(3.8760, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2282, requires_grad=True), f(x) = tensor(3.8759, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2283, requires_grad=True), f(x) = tensor(3.8759, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2284, requires_grad=True), f(x) = tensor(3.8759, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2285, requires_grad=True), f(x) = tensor(3.8759, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2286, requires_grad=True), f(x) = tensor(3.8759, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2287, requires_grad=True), f(x) = tensor(3.8759, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2287, requires_grad=True), f(x) = tensor(3.8759, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2288, requires_grad=True), f(x) = tensor(3.8759, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2289, requires_grad=True), f(x) = tensor(3.8759, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2290, requires_grad=True), f(x) = tensor(3.8759, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2291, requires_grad=True), f(x) = tensor(3.8759, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2292, requires_grad=True), f(x) = tensor(3.8759, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2293, requires_grad=True), f(x) = tensor(3.8759, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2293, requires_grad=True), f(x) = tensor(3.8759, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2294, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2295, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2296, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2297, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2297, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2298, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2299, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2300, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2301, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2301, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2302, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2303, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2304, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2305, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2305, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2306, requires_grad=True), f(x) = tensor(3.8758, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2307, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2308, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2308, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2309, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2310, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2311, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2312, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2312, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2313, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2314, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2315, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2315, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2316, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2317, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2317, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2318, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2319, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2320, requires_grad=True), f(x) = tensor(3.8757, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2320, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2321, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2322, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2323, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2323, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2324, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2325, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2325, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2326, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2327, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2327, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2328, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2329, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2330, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2330, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2331, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2332, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2332, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2333, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2334, requires_grad=True), f(x) = tensor(3.8756, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2334, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2335, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2336, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2336, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2337, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2338, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2338, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2339, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2339, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2340, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2341, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2341, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2342, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2343, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2343, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2344, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2345, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2345, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2346, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2346, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2347, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2348, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2348, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2349, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2349, requires_grad=True), f(x) = tensor(3.8755, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2350, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2351, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2351, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2352, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2352, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2353, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2354, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2354, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2355, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2355, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2356, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2357, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2357, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2358, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2358, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2359, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2359, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2360, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2360, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2361, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2362, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2362, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2363, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2363, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2364, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2364, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2365, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2365, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2366, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2366, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2367, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2368, requires_grad=True), f(x) = tensor(3.8754, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2368, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2369, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2369, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2370, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2370, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2371, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2371, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2372, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2372, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2373, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2373, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2374, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2374, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2375, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2375, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2376, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2376, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2377, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2377, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2378, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2378, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2379, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2379, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2380, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2380, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2381, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2381, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2382, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2382, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2383, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2383, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2383, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2384, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2384, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2385, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2385, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2386, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2386, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2387, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2387, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2388, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2388, requires_grad=True), f(x) = tensor(3.8753, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2389, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2389, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2389, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2390, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2390, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2391, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2391, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2392, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2392, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2392, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2393, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2393, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2394, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2394, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2395, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2395, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2395, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2396, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2396, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2397, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2397, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2398, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2398, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2398, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2399, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2399, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2400, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2400, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2400, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2401, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2401, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2402, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2402, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2402, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2403, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2403, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2404, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2404, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2404, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2405, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2405, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2405, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2406, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2406, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2407, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2407, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2407, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2408, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2408, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2408, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2409, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2409, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2409, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2410, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2410, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2411, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2411, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2411, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2412, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2412, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2412, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2413, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2413, requires_grad=True), f(x) = tensor(3.8752, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2413, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2414, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2414, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2414, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2415, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2415, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2415, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2416, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2416, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2416, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2417, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2417, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2417, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2418, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2418, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2418, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2419, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2419, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2419, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2420, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2420, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2420, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2421, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2421, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2421, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2422, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2422, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2422, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2423, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2423, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2423, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2424, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2424, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2424, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2424, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2425, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2425, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2425, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2426, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2426, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2426, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2427, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2427, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2427, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2427, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2428, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2428, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2428, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2429, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2429, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2429, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2429, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2430, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2430, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2430, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2431, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2431, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2431, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2431, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2432, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2432, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2432, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2432, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2433, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2433, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2433, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2434, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2434, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2434, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2434, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2435, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2435, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2435, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2435, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2436, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2436, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2436, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2436, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2437, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2437, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2437, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2437, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2438, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2438, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2438, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2438, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2439, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2439, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2439, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2439, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2440, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2440, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2440, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2440, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2441, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2441, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2441, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2441, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2442, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2442, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2442, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2442, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2442, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2443, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2443, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2443, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2443, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2444, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2444, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2444, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2444, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2444, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2445, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2445, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2445, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2445, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2446, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2446, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2446, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2446, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2446, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2447, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2447, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2447, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2447, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2448, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2448, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2448, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2448, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2448, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2449, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2449, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2449, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2449, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2449, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2450, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2450, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2450, requires_grad=True), f(x) = tensor(3.8751, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2450, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2450, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2451, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2451, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2451, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2451, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2451, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2452, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2452, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2452, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2452, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2452, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2453, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2453, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2453, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2453, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2453, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2453, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2454, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2454, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2454, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2454, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n",
            "X = tensor(-0.2454, requires_grad=True), f(x) = tensor(3.8750, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f62892d18d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEWCAYAAABollyxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTAAwWN2DWEH"
      },
      "source": [
        "# c) Implementando un MLP en PyTorch para predicción de precios de inmuebles \n",
        "\n",
        "Contamos con una base de datos de 506 precios de inmuebles de la ciudad de Boston [1]. Cada inmueble está descripto por diversas características como el indice de crimen per capita en la zona, o el grado de accesibilidad a autopistas, etc. Se cuenta con el precio de cada uno, y se pretende desarrollar un módulo que permita predecir dicho precio a partir de las características.\n",
        "\n",
        "[1]: *Hedonic prices and the demand for clean air*, J. Environ. Economics & Management, vol.5, 81-102, 1978.\n",
        "\n",
        "Primero, vamos a generar un histograma de los precios con todos los datos disponibles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D-sOjKKSdmp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "a5b1162c-61ad-4615-9a20-269864cad676"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Importamos el dataset\n",
        "\n",
        "dataset_boston = load_boston()\n",
        "\n",
        "print(\"El dataset contiene \" + str(dataset_boston.keys()) + \" \\n\\n\")\n",
        "\n",
        "# Extraigo los datos (features) y los precios (etiquetas a predecir)\n",
        "data = dataset_boston['data']\n",
        "data = data.astype(np.float32)\n",
        "precios = np.expand_dims(dataset_boston['target'], axis=1).astype(np.float32)\n",
        "\n",
        "print(\"Fila de ejemplo:\")\n",
        "print(dataset_boston['feature_names'])\n",
        "print(data[0,:])\n",
        "\n",
        "# Dibujo un histograma de los precios de los inmuebles\n",
        "_ = plt.hist( ..... , 50, density=True, facecolor='g', alpha=0.75)\n",
        "_ = plt.title(\"Precios\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El dataset contiene dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename']) \n",
            "\n",
            "\n",
            "Fila de ejemplo:\n",
            "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
            " 'B' 'LSTAT']\n",
            "[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
            " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgc2qJKLmhsB"
      },
      "source": [
        "Particionamos los datos en entrenamiento y prueba usando la función `sklearn.model_selection.train_test_split`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WWTSCnnVyRK"
      },
      "source": [
        " from sklearn.model_selection import train_test_split\n",
        " \n",
        " # Particiono los datos en entrenamiento y prueba usando el método de scikitlearn\n",
        " X_train, X_test, y_train, y_test = train_test_split( .... , ..... , test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_D3sJMCoB_K"
      },
      "source": [
        "Ahora implementaremos un Perceptrón multicapa que usaremos para como regresor del precio utilizando PyTorch (ejemplo basado en el curso de [RPI](https://rpi.analyticsdojo.com/)).\n",
        "\n",
        "El perceptrón deberá contar con 3 capas:\n",
        "- Las dos primeras con 100 neuronas, y deberán usar la función de activación ReLU.\n",
        "- La última con una única neurona cuya salida sea un valor escalar que corresponda al precio estimado del inmueble, que no deberá utilizar ninguna función de activación.\n",
        "\n",
        "Algunas clases de PyTorch que resultarán útiles para implementar el modelo, son:\n",
        "- `torch.nn.Linear`: Implementa una capa totalmente conectada. Es necesario especificarle el número de parámetros de entrada y de salida.\n",
        "- `torch.nn.functional.relu`: Implementa la función de activación ReLU.\n",
        "\n",
        "Además, utilizaremos el optimizador `torch.optim.Adam` y la función de pérdida `torch.nn.MSELoss` (error cuadrático medio).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91rAzYsjkAUa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77cbb350-c08f-4386-bb9c-c6f725742cb9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Tamaño del batch de entrenamiento\n",
        "batch_size = 50\n",
        "\n",
        "# Tasa de aprendizaje inicial para el gradiente descendente\n",
        "learning_rate = 0.00001\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_features, size_hidden, n_output):\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden1 = .....\n",
        "        self.hidden2 = .....\n",
        "        self.out = .....\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = .....      # activation function for hidden layer\n",
        "                     # linear output\n",
        "        return x\n",
        "\n",
        "# Definimos el modelo del perceptrón\n",
        "net = Net( .... , ..... , ......)\n",
        "\n",
        "# Construimos el optimizador, y le indicamos que los parámetros a optimizar \n",
        "# son los del modelo definido: net.parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam( ...... , lr=learning_rate)\n",
        "\n",
        "# Definimos también la función de pérdida a utilizar\n",
        "criterion = .....\n",
        "\n",
        "# Creamos el objeto dataset que empaqueta los array de numpy para que puedan \n",
        "# ser leidos por PyTorch\n",
        "dataset = TensorDataset(torch.from_numpy(X_train).clone(), torch.from_numpy(y_train).clone())\n",
        "\n",
        "# Creamos un loader iterable indicandole que debe leer los datos a partir de\n",
        "# del dataset creado en el paso anterior. Este objeto puede ser iterado\n",
        "# y nos devuelve de a un batch (x, y).\n",
        "loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Número de épocas\n",
        "num_epochs = 5000\n",
        "\n",
        "# Lista en la que iremos guardando el valor de la función de pérdida en cada \n",
        "# etapa de entrenamiento\n",
        "loss_list = []\n",
        "\n",
        "# Bucle de entrenamiento\n",
        "for i in range(num_epochs):\n",
        "\n",
        "    # Itero sobre todos los batches del dataset\n",
        "    for x, y in loader:\n",
        "        # Seteo en cero los gradientes de los parámetros a optimizar\n",
        "        optimizer.......\n",
        "\n",
        "        # Realizo la pasada forward computando la loss entre la salida de la red `net(x)` y las etiquetas `y`\n",
        "        loss = .......\n",
        "        \n",
        "        # Realizo la pasada backward por la red        \n",
        "        loss......\n",
        "        \n",
        "        # Actualizo los pesos de la red con el optimizador\n",
        "        optimizer.......\n",
        "\n",
        "        # Me guardo el valor actual de la función de pérdida para luego graficarlo\n",
        "        loss_list.append(loss.data.item())\n",
        "\n",
        "    # Muestro el valor de la función de pérdida cada 100 iteraciones        \n",
        "    if i > 0 and i % 100 == 0:\n",
        "        print('Epoch %d, loss = %g' % (i, loss))\n",
        "\n",
        "# Muestro la lista que contiene los valores de la función de pérdida\n",
        "# y una versión suavizada (rojo) para observar la tendencia\n",
        "plt.figure()\n",
        "loss_np_array = np.array(loss_list)\n",
        "plt.plot(loss_np_array, alpha = 0.3)\n",
        "N = 60\n",
        "running_avg_loss = np.convolve(loss_np_array, np.ones((N,))/N, mode='valid')\n",
        "plt.plot(running_avg_loss, color='red')\n",
        "plt.title(\"Función de pérdida durante el entrenamiento\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 100, loss = 923.106\n",
            "Epoch 200, loss = 715.819\n",
            "Epoch 300, loss = 879.679\n",
            "Epoch 400, loss = 725.334\n",
            "Epoch 500, loss = 673.847\n",
            "Epoch 600, loss = 347.52\n",
            "Epoch 700, loss = 527.921\n",
            "Epoch 800, loss = 283.484\n",
            "Epoch 900, loss = 529.495\n",
            "Epoch 1000, loss = 401.049\n",
            "Epoch 1100, loss = 201.275\n",
            "Epoch 1200, loss = 118.789\n",
            "Epoch 1300, loss = 226.777\n",
            "Epoch 1400, loss = 87.881\n",
            "Epoch 1500, loss = 63.0272\n",
            "Epoch 1600, loss = 100.337\n",
            "Epoch 1700, loss = 65.8468\n",
            "Epoch 1800, loss = 129.8\n",
            "Epoch 1900, loss = 46.6197\n",
            "Epoch 2000, loss = 95.6299\n",
            "Epoch 2100, loss = 60.0859\n",
            "Epoch 2200, loss = 37.3812\n",
            "Epoch 2300, loss = 63.4171\n",
            "Epoch 2400, loss = 123.206\n",
            "Epoch 2500, loss = 67.7605\n",
            "Epoch 2600, loss = 74.0231\n",
            "Epoch 2700, loss = 82.4982\n",
            "Epoch 2800, loss = 40.9975\n",
            "Epoch 2900, loss = 49.8836\n",
            "Epoch 3000, loss = 85.0704\n",
            "Epoch 3100, loss = 54.7555\n",
            "Epoch 3200, loss = 67.2247\n",
            "Epoch 3300, loss = 58.443\n",
            "Epoch 3400, loss = 121.901\n",
            "Epoch 3500, loss = 123.038\n",
            "Epoch 3600, loss = 59.1262\n",
            "Epoch 3700, loss = 100.994\n",
            "Epoch 3800, loss = 52.6257\n",
            "Epoch 3900, loss = 62.6242\n",
            "Epoch 4000, loss = 96.3365\n",
            "Epoch 4100, loss = 99.842\n",
            "Epoch 4200, loss = 37.2965\n",
            "Epoch 4300, loss = 64.553\n",
            "Epoch 4400, loss = 70.2143\n",
            "Epoch 4500, loss = 83.4168\n",
            "Epoch 4600, loss = 64.7246\n",
            "Epoch 4700, loss = 30.0104\n",
            "Epoch 4800, loss = 62.4811\n",
            "Epoch 4900, loss = 63.7695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Función de pérdida durante el entrenamiento')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhL0-b9J6eoD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "d17361f5-6986-4868-bf28-5f1ebb143172"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Definimos un método para mostrar las predicciones como un scatter plot \n",
        "# y graficamos la recta de regresión para esos datos.\n",
        "\n",
        "def plotScatter(x_data, y_data, title, fit_line=True):\n",
        "  plt.figure()\n",
        "  \n",
        "  plt.plot(x_data, y_data, '+')\n",
        "  plt.xlabel('Valor real')\n",
        "  plt.ylabel('Predicción')\n",
        "  plt.ylim((0,50))\n",
        "  plt.xlim((0,50))\n",
        "  plt.title(title)\n",
        "\n",
        "  if fit_line:\n",
        "    X, Y = x_data.reshape(-1,1), y_data.reshape(-1,1)\n",
        "    plt.plot( X, LinearRegression().fit(X, Y).predict(X) )\n",
        "\n",
        "# Dibujamos el ground truth vs las predicciones en los datos de entrenamiento\n",
        "py = net(torch.FloatTensor(X_train))\n",
        "y_pred_train = py.cpu().detach().numpy()\n",
        "plotScatter(y_train, y_pred_train, \"Training data\")\n",
        "\n",
        "# Dibujamos el ground truth vs las predicciones en los datos de test\n",
        "py = net(torch.FloatTensor(X_test))\n",
        "y_pred_test = py.cpu().detach().numpy()\n",
        "plotScatter(y_test, y_pred_test, \"Test data\")\n",
        "\n",
        "print (\"MSE medio en training: \" + str(((y_train - y_pred_train)**2).mean()))\n",
        "print (\"MSE medio en test: \" + str(((y_test - y_pred_test)**2).mean()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE medio en training: 75.63646254973968\n",
            "MSE medio en test: 64.81397696010436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc8fFGSm-D99"
      },
      "source": [
        "# Entregable \n",
        "1. Encontrar el mínimo de la función *f* definida en el apartado b). Para ello, deberán encontrar primero la derivada *f'(x)* de forma analítica, y utilizarla para computar el mínimo de la función. Posteriormente, deberán corrobarar que el valor coincida con el que obtuvieron optimizando la función con gradiente descendiente. \n",
        "\n",
        "2. Compara el rendimiento de 3 perceptrones multicapa que varíen en la cantidad de neuronas en sus capas intermedia. Probar colocando 2, 10 y 200 neuronas en dichas capas, al entrenar los perceptrones durante 5000 épocas. Mostrar los resultados utilizando:\n",
        "\n",
        "* los gráficos de dispersión con la recta de regresión\n",
        "* el error medio en los datos de entrenamiento y test\n",
        "\n",
        "  Analizar la relación entre dichos resultados y la cantidad de neuronas que posee el perceptrón.\n",
        " "
      ]
    }
  ]
}