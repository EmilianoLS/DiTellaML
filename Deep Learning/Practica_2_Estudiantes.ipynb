{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica 2 - Estudiantes",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmilianoLS/DiTellaML/blob/main/Deep%20Learning/Practica_2_Estudiantes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZSWq2lIEnbB"
      },
      "source": [
        "# Práctica 2: Introducción a PyTorch\n",
        "\n",
        "## a) Calculando el gradiente mediante Autograd\n",
        "\n",
        "En primer lugar, vamos a calcular del gradiente para el perceptrón simple con función de activación sigmoidea que vimos en la teoría. Pero esta vez, en lugar de realizar manualmente el proceso de backpropagation, vamos a usar el módulo `autograd` de PyTorch.\n",
        "\n",
        "La función $f(x, w)$ a la cual queremos encontrarle el gradiente es:\n",
        "\n",
        "> $f(\\mathbf{x}, \\mathbf{w}) = \\frac{1}{1 + e^{-(w_0 x_0 + w_1 x_1 + w_2)}}$\n",
        "\n",
        "Definimos entonces la función utilizando `torch.tensor` (recordar usar el parámetro `requires_grad = True` para que PyTorch guarde los gradientes) y realizamos la pasada \"forward\" para los siguientes valores de x y w:\n",
        "\n",
        "> $\\mathbf{x} = (-1, -2)$\n",
        "\n",
        "> $\\mathbf{w} = (2, -3, -3)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UczyYh5Nj2u5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a814e922-0d82-4064-ea22-1ec1c3007730"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Declaro los tensores x y w\n",
        "x = torch.tensor([-1.,-2.], requires_grad = True)\n",
        "w = torch.tensor([2.,-3., -3.], requires_grad = True)\n",
        "\n",
        "# Defino la funcion sinusoide\n",
        "f = 1/(1 + torch.exp(-(w[0]*x[0] + w[1]*x[1] + w[2])))\n",
        "\n",
        "print(f)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7311, grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkrbxHMukzHQ"
      },
      "source": [
        "Ahora, utilizando la función `f.backward()` computamos los gradientes $\\frac{\\partial f}{ \\partial \\mathbf{x}}$ y $\\frac{\\partial f}{ \\partial \\mathbf{w}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q477bpjr77xp"
      },
      "source": [
        "f.backward()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXewlL_8YHMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "543d5aa1-7748-41c0-ccf2-0b06ce07a81a"
      },
      "source": [
        "print(\"Gradiente df/dx = \" + str(x.grad))\n",
        "print(\"Gradiente df/dw = \" + str(w.grad))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradiente df/dx = tensor([ 0.3932, -0.5898])\n",
            "Gradiente df/dw = tensor([-0.1966, -0.3932,  0.1966])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsXTQ9wJnK2j"
      },
      "source": [
        "## b) Minimizando una función con Gradient Descent\n",
        "\n",
        "Ahora, vamos a implementar usar el algorítmo de gradiente descendiente (utilizando Autograd para computar el gradiente) para minimizar la función cuadrática $$f(x) = 2x^2 + x + 4$$\n",
        "\n",
        "Utilizaremos la implementación `torch.optim.SGD` de gradiente descendiente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKc75VsMYS4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa652e42-1e04-413b-e072-e473ec0efba9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Definir la variable que será el parámetro a optimizar\n",
        "x = torch.tensor([-3.], requires_grad = True)\n",
        "\n",
        "# Definir el optimizador, indicando el parámetro a optimizar y el learning rate\n",
        "optimizer = torch.optim.SGD([x], lr = 0.001)\n",
        "\n",
        "# Acumuladores que usaremos para guardar los valores sucesivos de x, y\n",
        "f_values = []\n",
        "x_values = []\n",
        "\n",
        "# Loop de optimización\n",
        "for i in range(1000):\n",
        "\n",
        "    # Setemos en 0 los gradientes de todos los elementos\n",
        "    optimizer.zero_grad()\n",
        "       \n",
        "    # Pasada forward: ejecutar la función a minimizar\n",
        "    f = 2 * x ** 2 + x + 4\n",
        "\n",
        "    print(\"X = \" + str(x) + \", f(x) = \" + str(f))\n",
        "\n",
        "    # Pasada backward: computar los gradientes\n",
        "    f.backward()\n",
        "\n",
        "    # Actualizar los pesos dando un paso de gradiente descendiente\n",
        "    optimizer.step()\n",
        "\n",
        "    # Guardar los valores para luego plotearlos\n",
        "    f_values.append(f.data.item())\n",
        "    x_values.append(x.data.item())\n",
        "\n",
        "# Ploteo los valores\n",
        "plt.title(\"Optimizando la función f = 2 * x**2 + x + 4\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.plot(x_values,f_values)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X = tensor([-3.], requires_grad=True), f(x) = tensor([19.], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9890], requires_grad=True), f(x) = tensor([18.8792], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9780], requires_grad=True), f(x) = tensor([18.7594], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9671], requires_grad=True), f(x) = tensor([18.6406], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9563], requires_grad=True), f(x) = tensor([18.5227], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9454], requires_grad=True), f(x) = tensor([18.4058], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9347], requires_grad=True), f(x) = tensor([18.2898], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9239], requires_grad=True), f(x) = tensor([18.1747], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9132], requires_grad=True), f(x) = tensor([18.0605], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.9026], requires_grad=True), f(x) = tensor([17.9472], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8920], requires_grad=True), f(x) = tensor([17.8349], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8814], requires_grad=True), f(x) = tensor([17.7234], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8709], requires_grad=True), f(x) = tensor([17.6129], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8604], requires_grad=True), f(x) = tensor([17.5032], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8499], requires_grad=True), f(x) = tensor([17.3944], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8395], requires_grad=True), f(x) = tensor([17.2864], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8292], requires_grad=True), f(x) = tensor([17.1794], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8189], requires_grad=True), f(x) = tensor([17.0731], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.8086], requires_grad=True), f(x) = tensor([16.9678], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7984], requires_grad=True), f(x) = tensor([16.8632], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7882], requires_grad=True), f(x) = tensor([16.7595], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7780], requires_grad=True), f(x) = tensor([16.6567], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7679], requires_grad=True), f(x) = tensor([16.5546], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7578], requires_grad=True), f(x) = tensor([16.4534], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7478], requires_grad=True), f(x) = tensor([16.3530], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7378], requires_grad=True), f(x) = tensor([16.2533], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7279], requires_grad=True), f(x) = tensor([16.1545], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7179], requires_grad=True), f(x) = tensor([16.0565], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.7081], requires_grad=True), f(x) = tensor([15.9592], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6982], requires_grad=True), f(x) = tensor([15.8627], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6884], requires_grad=True), f(x) = tensor([15.7670], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6787], requires_grad=True), f(x) = tensor([15.6721], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6690], requires_grad=True), f(x) = tensor([15.5779], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6593], requires_grad=True), f(x) = tensor([15.4845], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6497], requires_grad=True), f(x) = tensor([15.3918], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6401], requires_grad=True), f(x) = tensor([15.2998], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6305], requires_grad=True), f(x) = tensor([15.2086], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6210], requires_grad=True), f(x) = tensor([15.1181], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6115], requires_grad=True), f(x) = tensor([15.0283], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.6021], requires_grad=True), f(x) = tensor([14.9393], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5926], requires_grad=True), f(x) = tensor([14.8510], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5833], requires_grad=True), f(x) = tensor([14.7633], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5739], requires_grad=True), f(x) = tensor([14.6764], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5646], requires_grad=True), f(x) = tensor([14.5902], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5554], requires_grad=True), f(x) = tensor([14.5046], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5462], requires_grad=True), f(x) = tensor([14.4197], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5370], requires_grad=True), f(x) = tensor([14.3356], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5278], requires_grad=True), f(x) = tensor([14.2520], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5187], requires_grad=True), f(x) = tensor([14.1692], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5096], requires_grad=True), f(x) = tensor([14.0870], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.5006], requires_grad=True), f(x) = tensor([14.0055], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4916], requires_grad=True), f(x) = tensor([13.9246], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4826], requires_grad=True), f(x) = tensor([13.8443], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4737], requires_grad=True), f(x) = tensor([13.7648], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4648], requires_grad=True), f(x) = tensor([13.6858], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4560], requires_grad=True), f(x) = tensor([13.6075], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4471], requires_grad=True), f(x) = tensor([13.5298], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4383], requires_grad=True), f(x) = tensor([13.4527], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4296], requires_grad=True), f(x) = tensor([13.3762], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4209], requires_grad=True), f(x) = tensor([13.3003], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4122], requires_grad=True), f(x) = tensor([13.2251], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.4035], requires_grad=True), f(x) = tensor([13.1504], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3949], requires_grad=True), f(x) = tensor([13.0764], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3863], requires_grad=True), f(x) = tensor([13.0029], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3778], requires_grad=True), f(x) = tensor([12.9301], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3693], requires_grad=True), f(x) = tensor([12.8578], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3608], requires_grad=True), f(x) = tensor([12.7860], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3524], requires_grad=True), f(x) = tensor([12.7149], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3440], requires_grad=True), f(x) = tensor([12.6443], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3356], requires_grad=True), f(x) = tensor([12.5743], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3272], requires_grad=True), f(x) = tensor([12.5048], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3189], requires_grad=True), f(x) = tensor([12.4359], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3107], requires_grad=True), f(x) = tensor([12.3676], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.3024], requires_grad=True), f(x) = tensor([12.2998], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2942], requires_grad=True), f(x) = tensor([12.2325], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2860], requires_grad=True), f(x) = tensor([12.1658], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2779], requires_grad=True), f(x) = tensor([12.0996], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2698], requires_grad=True), f(x) = tensor([12.0339], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2617], requires_grad=True), f(x) = tensor([11.9688], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2536], requires_grad=True), f(x) = tensor([11.9042], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2456], requires_grad=True), f(x) = tensor([11.8401], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2376], requires_grad=True), f(x) = tensor([11.7765], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2297], requires_grad=True), f(x) = tensor([11.7134], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2218], requires_grad=True), f(x) = tensor([11.6508], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2139], requires_grad=True), f(x) = tensor([11.5887], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.2060], requires_grad=True), f(x) = tensor([11.5271], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1982], requires_grad=True), f(x) = tensor([11.4660], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1904], requires_grad=True), f(x) = tensor([11.4054], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1827], requires_grad=True), f(x) = tensor([11.3453], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1749], requires_grad=True), f(x) = tensor([11.2857], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1672], requires_grad=True), f(x) = tensor([11.2265], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1596], requires_grad=True), f(x) = tensor([11.1678], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1519], requires_grad=True), f(x) = tensor([11.1096], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1443], requires_grad=True), f(x) = tensor([11.0518], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1367], requires_grad=True), f(x) = tensor([10.9945], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1292], requires_grad=True), f(x) = tensor([10.9377], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1217], requires_grad=True), f(x) = tensor([10.8813], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1142], requires_grad=True), f(x) = tensor([10.8254], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.1067], requires_grad=True), f(x) = tensor([10.7699], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0993], requires_grad=True), f(x) = tensor([10.7148], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0919], requires_grad=True), f(x) = tensor([10.6602], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0845], requires_grad=True), f(x) = tensor([10.6060], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0772], requires_grad=True), f(x) = tensor([10.5523], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0699], requires_grad=True), f(x) = tensor([10.4990], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0626], requires_grad=True), f(x) = tensor([10.4461], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0554], requires_grad=True), f(x) = tensor([10.3936], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0481], requires_grad=True), f(x) = tensor([10.3416], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0409], requires_grad=True), f(x) = tensor([10.2900], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0338], requires_grad=True), f(x) = tensor([10.2387], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0266], requires_grad=True), f(x) = tensor([10.1879], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0195], requires_grad=True), f(x) = tensor([10.1375], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0125], requires_grad=True), f(x) = tensor([10.0875], grad_fn=<AddBackward0>)\n",
            "X = tensor([-2.0054], requires_grad=True), f(x) = tensor([10.0379], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9984], requires_grad=True), f(x) = tensor([9.9887], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9914], requires_grad=True), f(x) = tensor([9.9399], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9844], requires_grad=True), f(x) = tensor([9.8915], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9775], requires_grad=True), f(x) = tensor([9.8435], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9706], requires_grad=True), f(x) = tensor([9.7958], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9637], requires_grad=True), f(x) = tensor([9.7485], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9568], requires_grad=True), f(x) = tensor([9.7016], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9500], requires_grad=True), f(x) = tensor([9.6551], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9432], requires_grad=True), f(x) = tensor([9.6090], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9364], requires_grad=True), f(x) = tensor([9.5632], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9297], requires_grad=True), f(x) = tensor([9.5178], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9230], requires_grad=True), f(x) = tensor([9.4727], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9163], requires_grad=True), f(x) = tensor([9.4280], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9096], requires_grad=True), f(x) = tensor([9.3837], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.9030], requires_grad=True), f(x) = tensor([9.3397], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8964], requires_grad=True), f(x) = tensor([9.2961], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8898], requires_grad=True), f(x) = tensor([9.2528], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8832], requires_grad=True), f(x) = tensor([9.2099], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8767], requires_grad=True), f(x) = tensor([9.1673], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8702], requires_grad=True), f(x) = tensor([9.1250], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8637], requires_grad=True), f(x) = tensor([9.0831], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8573], requires_grad=True), f(x) = tensor([9.0415], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8508], requires_grad=True), f(x) = tensor([9.0003], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8444], requires_grad=True), f(x) = tensor([8.9593], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8380], requires_grad=True), f(x) = tensor([8.9188], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8317], requires_grad=True), f(x) = tensor([8.8785], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8254], requires_grad=True), f(x) = tensor([8.8385], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8191], requires_grad=True), f(x) = tensor([8.7989], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8128], requires_grad=True), f(x) = tensor([8.7596], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8065], requires_grad=True), f(x) = tensor([8.7206], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.8003], requires_grad=True), f(x) = tensor([8.6819], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7941], requires_grad=True), f(x) = tensor([8.6435], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7879], requires_grad=True), f(x) = tensor([8.6055], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7818], requires_grad=True), f(x) = tensor([8.5677], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7757], requires_grad=True), f(x) = tensor([8.5302], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7695], requires_grad=True), f(x) = tensor([8.4931], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7635], requires_grad=True), f(x) = tensor([8.4562], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7574], requires_grad=True), f(x) = tensor([8.4196], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7514], requires_grad=True), f(x) = tensor([8.3833], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7454], requires_grad=True), f(x) = tensor([8.3473], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7394], requires_grad=True), f(x) = tensor([8.3116], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7334], requires_grad=True), f(x) = tensor([8.2762], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7275], requires_grad=True), f(x) = tensor([8.2411], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7216], requires_grad=True), f(x) = tensor([8.2062], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7157], requires_grad=True), f(x) = tensor([8.1716], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7098], requires_grad=True), f(x) = tensor([8.1373], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.7040], requires_grad=True), f(x) = tensor([8.1033], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6982], requires_grad=True), f(x) = tensor([8.0695], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6924], requires_grad=True), f(x) = tensor([8.0360], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6866], requires_grad=True), f(x) = tensor([8.0028], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6809], requires_grad=True), f(x) = tensor([7.9699], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6752], requires_grad=True), f(x) = tensor([7.9372], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6695], requires_grad=True), f(x) = tensor([7.9047], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6638], requires_grad=True), f(x) = tensor([7.8726], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6581], requires_grad=True), f(x) = tensor([7.8406], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6525], requires_grad=True), f(x) = tensor([7.8090], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6469], requires_grad=True), f(x) = tensor([7.7776], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6413], requires_grad=True), f(x) = tensor([7.7464], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6357], requires_grad=True), f(x) = tensor([7.7155], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6302], requires_grad=True), f(x) = tensor([7.6848], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6247], requires_grad=True), f(x) = tensor([7.6544], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6192], requires_grad=True), f(x) = tensor([7.6243], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6137], requires_grad=True), f(x) = tensor([7.5943], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6082], requires_grad=True), f(x) = tensor([7.5646], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.6028], requires_grad=True), f(x) = tensor([7.5352], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5974], requires_grad=True), f(x) = tensor([7.5059], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5920], requires_grad=True), f(x) = tensor([7.4770], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5866], requires_grad=True), f(x) = tensor([7.4482], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5813], requires_grad=True), f(x) = tensor([7.4197], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5760], requires_grad=True), f(x) = tensor([7.3914], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5707], requires_grad=True), f(x) = tensor([7.3633], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5654], requires_grad=True), f(x) = tensor([7.3354], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5601], requires_grad=True), f(x) = tensor([7.3078], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5549], requires_grad=True), f(x) = tensor([7.2804], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5497], requires_grad=True), f(x) = tensor([7.2532], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5445], requires_grad=True), f(x) = tensor([7.2262], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5393], requires_grad=True), f(x) = tensor([7.1995], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5341], requires_grad=True), f(x) = tensor([7.1729], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5290], requires_grad=True), f(x) = tensor([7.1466], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5239], requires_grad=True), f(x) = tensor([7.1205], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5188], requires_grad=True), f(x) = tensor([7.0946], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5137], requires_grad=True), f(x) = tensor([7.0689], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5086], requires_grad=True), f(x) = tensor([7.0434], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.5036], requires_grad=True), f(x) = tensor([7.0181], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4986], requires_grad=True), f(x) = tensor([6.9930], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4936], requires_grad=True), f(x) = tensor([6.9681], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4886], requires_grad=True), f(x) = tensor([6.9434], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4837], requires_grad=True), f(x) = tensor([6.9189], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4787], requires_grad=True), f(x) = tensor([6.8946], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4738], requires_grad=True), f(x) = tensor([6.8705], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4689], requires_grad=True), f(x) = tensor([6.8466], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4641], requires_grad=True), f(x) = tensor([6.8228], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4592], requires_grad=True), f(x) = tensor([6.7993], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4544], requires_grad=True), f(x) = tensor([6.7760], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4495], requires_grad=True), f(x) = tensor([6.7528], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4447], requires_grad=True), f(x) = tensor([6.7298], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4400], requires_grad=True), f(x) = tensor([6.7070], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4352], requires_grad=True), f(x) = tensor([6.6844], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4305], requires_grad=True), f(x) = tensor([6.6620], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4257], requires_grad=True), f(x) = tensor([6.6397], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4210], requires_grad=True), f(x) = tensor([6.6177], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4164], requires_grad=True), f(x) = tensor([6.5958], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4117], requires_grad=True), f(x) = tensor([6.5740], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4070], requires_grad=True), f(x) = tensor([6.5525], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.4024], requires_grad=True), f(x) = tensor([6.5311], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3978], requires_grad=True), f(x) = tensor([6.5099], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3932], requires_grad=True), f(x) = tensor([6.4889], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3886], requires_grad=True), f(x) = tensor([6.4680], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3841], requires_grad=True), f(x) = tensor([6.4473], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3796], requires_grad=True), f(x) = tensor([6.4268], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3750], requires_grad=True), f(x) = tensor([6.4064], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3705], requires_grad=True), f(x) = tensor([6.3862], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3661], requires_grad=True), f(x) = tensor([6.3661], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3616], requires_grad=True), f(x) = tensor([6.3462], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3571], requires_grad=True), f(x) = tensor([6.3265], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3527], requires_grad=True), f(x) = tensor([6.3069], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3483], requires_grad=True), f(x) = tensor([6.2875], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3439], requires_grad=True), f(x) = tensor([6.2683], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3395], requires_grad=True), f(x) = tensor([6.2492], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3352], requires_grad=True), f(x) = tensor([6.2302], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3308], requires_grad=True), f(x) = tensor([6.2114], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3265], requires_grad=True), f(x) = tensor([6.1927], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3222], requires_grad=True), f(x) = tensor([6.1742], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3179], requires_grad=True), f(x) = tensor([6.1559], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3136], requires_grad=True), f(x) = tensor([6.1377], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3094], requires_grad=True), f(x) = tensor([6.1196], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3052], requires_grad=True), f(x) = tensor([6.1017], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.3009], requires_grad=True), f(x) = tensor([6.0839], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2967], requires_grad=True), f(x) = tensor([6.0663], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2925], requires_grad=True), f(x) = tensor([6.0488], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2884], requires_grad=True), f(x) = tensor([6.0314], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2842], requires_grad=True), f(x) = tensor([6.0142], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2801], requires_grad=True), f(x) = tensor([5.9971], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2760], requires_grad=True), f(x) = tensor([5.9802], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2719], requires_grad=True), f(x) = tensor([5.9634], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2678], requires_grad=True), f(x) = tensor([5.9467], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2637], requires_grad=True), f(x) = tensor([5.9302], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2596], requires_grad=True), f(x) = tensor([5.9138], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2556], requires_grad=True), f(x) = tensor([5.8975], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2516], requires_grad=True), f(x) = tensor([5.8813], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2476], requires_grad=True), f(x) = tensor([5.8653], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2436], requires_grad=True), f(x) = tensor([5.8494], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2396], requires_grad=True), f(x) = tensor([5.8337], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2357], requires_grad=True), f(x) = tensor([5.8180], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2317], requires_grad=True), f(x) = tensor([5.8025], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2278], requires_grad=True), f(x) = tensor([5.7871], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2239], requires_grad=True), f(x) = tensor([5.7718], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2200], requires_grad=True), f(x) = tensor([5.7567], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2161], requires_grad=True), f(x) = tensor([5.7417], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2122], requires_grad=True), f(x) = tensor([5.7268], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2084], requires_grad=True), f(x) = tensor([5.7120], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2045], requires_grad=True), f(x) = tensor([5.6973], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.2007], requires_grad=True), f(x) = tensor([5.6828], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1969], requires_grad=True), f(x) = tensor([5.6683], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1931], requires_grad=True), f(x) = tensor([5.6540], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1894], requires_grad=True), f(x) = tensor([5.6398], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1856], requires_grad=True), f(x) = tensor([5.6257], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1819], requires_grad=True), f(x) = tensor([5.6118], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1781], requires_grad=True), f(x) = tensor([5.5979], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1744], requires_grad=True), f(x) = tensor([5.5841], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1707], requires_grad=True), f(x) = tensor([5.5705], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1670], requires_grad=True), f(x) = tensor([5.5569], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1634], requires_grad=True), f(x) = tensor([5.5435], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1597], requires_grad=True), f(x) = tensor([5.5302], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1561], requires_grad=True), f(x) = tensor([5.5170], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1525], requires_grad=True), f(x) = tensor([5.5039], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1489], requires_grad=True), f(x) = tensor([5.4909], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1453], requires_grad=True), f(x) = tensor([5.4780], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1417], requires_grad=True), f(x) = tensor([5.4652], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1381], requires_grad=True), f(x) = tensor([5.4525], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1346], requires_grad=True), f(x) = tensor([5.4399], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1310], requires_grad=True), f(x) = tensor([5.4274], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1275], requires_grad=True), f(x) = tensor([5.4150], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1240], requires_grad=True), f(x) = tensor([5.4027], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1205], requires_grad=True), f(x) = tensor([5.3905], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1170], requires_grad=True), f(x) = tensor([5.3784], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1135], requires_grad=True), f(x) = tensor([5.3664], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1101], requires_grad=True), f(x) = tensor([5.3545], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1066], requires_grad=True), f(x) = tensor([5.3427], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.1032], requires_grad=True), f(x) = tensor([5.3310], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0998], requires_grad=True), f(x) = tensor([5.3193], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0964], requires_grad=True), f(x) = tensor([5.3078], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0930], requires_grad=True), f(x) = tensor([5.2964], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0896], requires_grad=True), f(x) = tensor([5.2850], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0863], requires_grad=True), f(x) = tensor([5.2738], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0829], requires_grad=True), f(x) = tensor([5.2626], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0796], requires_grad=True), f(x) = tensor([5.2515], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0763], requires_grad=True), f(x) = tensor([5.2405], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0730], requires_grad=True), f(x) = tensor([5.2296], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0697], requires_grad=True), f(x) = tensor([5.2188], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0664], requires_grad=True), f(x) = tensor([5.2081], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0632], requires_grad=True), f(x) = tensor([5.1974], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0599], requires_grad=True), f(x) = tensor([5.1869], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0567], requires_grad=True), f(x) = tensor([5.1764], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0534], requires_grad=True), f(x) = tensor([5.1660], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0502], requires_grad=True), f(x) = tensor([5.1557], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0470], requires_grad=True), f(x) = tensor([5.1455], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0438], requires_grad=True), f(x) = tensor([5.1353], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0407], requires_grad=True), f(x) = tensor([5.1253], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0375], requires_grad=True), f(x) = tensor([5.1153], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0343], requires_grad=True), f(x) = tensor([5.1054], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0312], requires_grad=True), f(x) = tensor([5.0956], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0281], requires_grad=True), f(x) = tensor([5.0858], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0250], requires_grad=True), f(x) = tensor([5.0761], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0219], requires_grad=True), f(x) = tensor([5.0666], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0188], requires_grad=True), f(x) = tensor([5.0570], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0157], requires_grad=True), f(x) = tensor([5.0476], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0126], requires_grad=True), f(x) = tensor([5.0382], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0096], requires_grad=True), f(x) = tensor([5.0290], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0066], requires_grad=True), f(x) = tensor([5.0197], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0035], requires_grad=True), f(x) = tensor([5.0106], grad_fn=<AddBackward0>)\n",
            "X = tensor([-1.0005], requires_grad=True), f(x) = tensor([5.0015], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9975], requires_grad=True), f(x) = tensor([4.9925], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9945], requires_grad=True), f(x) = tensor([4.9836], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9915], requires_grad=True), f(x) = tensor([4.9748], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9886], requires_grad=True), f(x) = tensor([4.9660], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9856], requires_grad=True), f(x) = tensor([4.9573], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9827], requires_grad=True), f(x) = tensor([4.9486], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9797], requires_grad=True), f(x) = tensor([4.9401], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9768], requires_grad=True), f(x) = tensor([4.9316], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9739], requires_grad=True), f(x) = tensor([4.9231], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9710], requires_grad=True), f(x) = tensor([4.9148], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9681], requires_grad=True), f(x) = tensor([4.9065], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9653], requires_grad=True), f(x) = tensor([4.8982], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9624], requires_grad=True), f(x) = tensor([4.8901], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9596], requires_grad=True), f(x) = tensor([4.8819], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9567], requires_grad=True), f(x) = tensor([4.8739], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9539], requires_grad=True), f(x) = tensor([4.8659], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9511], requires_grad=True), f(x) = tensor([4.8580], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9483], requires_grad=True), f(x) = tensor([4.8502], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9455], requires_grad=True), f(x) = tensor([4.8424], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9427], requires_grad=True), f(x) = tensor([4.8347], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9399], requires_grad=True), f(x) = tensor([4.8270], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9372], requires_grad=True), f(x) = tensor([4.8194], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9344], requires_grad=True), f(x) = tensor([4.8119], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9317], requires_grad=True), f(x) = tensor([4.8044], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9290], requires_grad=True), f(x) = tensor([4.7970], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9262], requires_grad=True), f(x) = tensor([4.7896], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9235], requires_grad=True), f(x) = tensor([4.7823], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9208], requires_grad=True), f(x) = tensor([4.7751], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9182], requires_grad=True), f(x) = tensor([4.7679], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9155], requires_grad=True), f(x) = tensor([4.7607], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9128], requires_grad=True), f(x) = tensor([4.7537], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9102], requires_grad=True), f(x) = tensor([4.7467], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9075], requires_grad=True), f(x) = tensor([4.7397], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9049], requires_grad=True), f(x) = tensor([4.7328], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.9023], requires_grad=True), f(x) = tensor([4.7259], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8997], requires_grad=True), f(x) = tensor([4.7191], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8971], requires_grad=True), f(x) = tensor([4.7124], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8945], requires_grad=True), f(x) = tensor([4.7057], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8919], requires_grad=True), f(x) = tensor([4.6991], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8893], requires_grad=True), f(x) = tensor([4.6925], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8868], requires_grad=True), f(x) = tensor([4.6860], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8842], requires_grad=True), f(x) = tensor([4.6795], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8817], requires_grad=True), f(x) = tensor([4.6731], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8792], requires_grad=True), f(x) = tensor([4.6667], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8767], requires_grad=True), f(x) = tensor([4.6604], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8741], requires_grad=True), f(x) = tensor([4.6541], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8717], requires_grad=True), f(x) = tensor([4.6479], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8692], requires_grad=True), f(x) = tensor([4.6417], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8667], requires_grad=True), f(x) = tensor([4.6356], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8642], requires_grad=True), f(x) = tensor([4.6295], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8618], requires_grad=True), f(x) = tensor([4.6235], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8593], requires_grad=True), f(x) = tensor([4.6175], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8569], requires_grad=True), f(x) = tensor([4.6116], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8545], requires_grad=True), f(x) = tensor([4.6057], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8520], requires_grad=True), f(x) = tensor([4.5999], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8496], requires_grad=True), f(x) = tensor([4.5941], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8472], requires_grad=True), f(x) = tensor([4.5884], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8448], requires_grad=True), f(x) = tensor([4.5827], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8425], requires_grad=True), f(x) = tensor([4.5770], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8401], requires_grad=True), f(x) = tensor([4.5714], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8377], requires_grad=True), f(x) = tensor([4.5659], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8354], requires_grad=True), f(x) = tensor([4.5603], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8330], requires_grad=True), f(x) = tensor([4.5549], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8307], requires_grad=True), f(x) = tensor([4.5494], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8284], requires_grad=True), f(x) = tensor([4.5441], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8261], requires_grad=True), f(x) = tensor([4.5387], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8238], requires_grad=True), f(x) = tensor([4.5334], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8215], requires_grad=True), f(x) = tensor([4.5282], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8192], requires_grad=True), f(x) = tensor([4.5229], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8169], requires_grad=True), f(x) = tensor([4.5178], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8146], requires_grad=True), f(x) = tensor([4.5126], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8124], requires_grad=True), f(x) = tensor([4.5075], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8101], requires_grad=True), f(x) = tensor([4.5025], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8079], requires_grad=True), f(x) = tensor([4.4975], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8057], requires_grad=True), f(x) = tensor([4.4925], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8034], requires_grad=True), f(x) = tensor([4.4876], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.8012], requires_grad=True), f(x) = tensor([4.4827], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7990], requires_grad=True), f(x) = tensor([4.4778], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7968], requires_grad=True), f(x) = tensor([4.4730], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7946], requires_grad=True), f(x) = tensor([4.4683], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7925], requires_grad=True), f(x) = tensor([4.4635], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7903], requires_grad=True), f(x) = tensor([4.4588], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7881], requires_grad=True), f(x) = tensor([4.4542], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7860], requires_grad=True), f(x) = tensor([4.4495], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7838], requires_grad=True), f(x) = tensor([4.4449], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7817], requires_grad=True), f(x) = tensor([4.4404], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7796], requires_grad=True), f(x) = tensor([4.4359], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7774], requires_grad=True), f(x) = tensor([4.4314], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7753], requires_grad=True), f(x) = tensor([4.4270], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7732], requires_grad=True), f(x) = tensor([4.4226], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7711], requires_grad=True), f(x) = tensor([4.4182], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7691], requires_grad=True), f(x) = tensor([4.4138], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7670], requires_grad=True), f(x) = tensor([4.4095], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7649], requires_grad=True), f(x) = tensor([4.4053], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7629], requires_grad=True), f(x) = tensor([4.4010], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7608], requires_grad=True), f(x) = tensor([4.3968], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7588], requires_grad=True), f(x) = tensor([4.3927], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7567], requires_grad=True), f(x) = tensor([4.3885], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7547], requires_grad=True), f(x) = tensor([4.3844], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7527], requires_grad=True), f(x) = tensor([4.3804], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7507], requires_grad=True), f(x) = tensor([4.3763], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7487], requires_grad=True), f(x) = tensor([4.3723], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7467], requires_grad=True), f(x) = tensor([4.3684], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7447], requires_grad=True), f(x) = tensor([4.3644], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7427], requires_grad=True), f(x) = tensor([4.3605], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7407], requires_grad=True), f(x) = tensor([4.3566], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7388], requires_grad=True), f(x) = tensor([4.3528], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7368], requires_grad=True), f(x) = tensor([4.3490], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7349], requires_grad=True), f(x) = tensor([4.3452], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7329], requires_grad=True), f(x) = tensor([4.3414], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7310], requires_grad=True), f(x) = tensor([4.3377], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7291], requires_grad=True), f(x) = tensor([4.3340], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7272], requires_grad=True), f(x) = tensor([4.3304], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7253], requires_grad=True), f(x) = tensor([4.3267], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7233], requires_grad=True), f(x) = tensor([4.3231], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7215], requires_grad=True), f(x) = tensor([4.3195], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7196], requires_grad=True), f(x) = tensor([4.3160], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7177], requires_grad=True), f(x) = tensor([4.3125], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7158], requires_grad=True), f(x) = tensor([4.3090], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7140], requires_grad=True), f(x) = tensor([4.3055], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7121], requires_grad=True), f(x) = tensor([4.3021], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7103], requires_grad=True), f(x) = tensor([4.2987], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7084], requires_grad=True), f(x) = tensor([4.2953], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7066], requires_grad=True), f(x) = tensor([4.2919], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7048], requires_grad=True), f(x) = tensor([4.2886], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7029], requires_grad=True), f(x) = tensor([4.2853], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.7011], requires_grad=True), f(x) = tensor([4.2820], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6993], requires_grad=True), f(x) = tensor([4.2788], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6975], requires_grad=True), f(x) = tensor([4.2755], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6957], requires_grad=True), f(x) = tensor([4.2724], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6939], requires_grad=True), f(x) = tensor([4.2692], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6922], requires_grad=True), f(x) = tensor([4.2660], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6904], requires_grad=True), f(x) = tensor([4.2629], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6886], requires_grad=True), f(x) = tensor([4.2598], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6869], requires_grad=True), f(x) = tensor([4.2567], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6851], requires_grad=True), f(x) = tensor([4.2537], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6834], requires_grad=True), f(x) = tensor([4.2507], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6817], requires_grad=True), f(x) = tensor([4.2477], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6799], requires_grad=True), f(x) = tensor([4.2447], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6782], requires_grad=True), f(x) = tensor([4.2417], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6765], requires_grad=True), f(x) = tensor([4.2388], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6748], requires_grad=True), f(x) = tensor([4.2359], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6731], requires_grad=True), f(x) = tensor([4.2330], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6714], requires_grad=True), f(x) = tensor([4.2302], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6697], requires_grad=True), f(x) = tensor([4.2273], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6680], requires_grad=True), f(x) = tensor([4.2245], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6664], requires_grad=True), f(x) = tensor([4.2217], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6647], requires_grad=True), f(x) = tensor([4.2190], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6630], requires_grad=True), f(x) = tensor([4.2162], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6614], requires_grad=True), f(x) = tensor([4.2135], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6597], requires_grad=True), f(x) = tensor([4.2108], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6581], requires_grad=True), f(x) = tensor([4.2081], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6565], requires_grad=True), f(x) = tensor([4.2054], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6549], requires_grad=True), f(x) = tensor([4.2028], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6532], requires_grad=True), f(x) = tensor([4.2002], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6516], requires_grad=True), f(x) = tensor([4.1976], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6500], requires_grad=True), f(x) = tensor([4.1950], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6484], requires_grad=True), f(x) = tensor([4.1925], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6468], requires_grad=True), f(x) = tensor([4.1899], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6452], requires_grad=True), f(x) = tensor([4.1874], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6437], requires_grad=True), f(x) = tensor([4.1849], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6421], requires_grad=True), f(x) = tensor([4.1824], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6405], requires_grad=True), f(x) = tensor([4.1800], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6389], requires_grad=True), f(x) = tensor([4.1776], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6374], requires_grad=True), f(x) = tensor([4.1751], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6358], requires_grad=True), f(x) = tensor([4.1727], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6343], requires_grad=True), f(x) = tensor([4.1704], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6328], requires_grad=True), f(x) = tensor([4.1680], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6312], requires_grad=True), f(x) = tensor([4.1657], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6297], requires_grad=True), f(x) = tensor([4.1634], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6282], requires_grad=True), f(x) = tensor([4.1610], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6267], requires_grad=True), f(x) = tensor([4.1588], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6252], requires_grad=True), f(x) = tensor([4.1565], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6237], requires_grad=True), f(x) = tensor([4.1543], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6222], requires_grad=True), f(x) = tensor([4.1520], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6207], requires_grad=True), f(x) = tensor([4.1498], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6192], requires_grad=True), f(x) = tensor([4.1476], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6177], requires_grad=True), f(x) = tensor([4.1454], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6163], requires_grad=True), f(x) = tensor([4.1433], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6148], requires_grad=True), f(x) = tensor([4.1411], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6133], requires_grad=True), f(x) = tensor([4.1390], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6119], requires_grad=True), f(x) = tensor([4.1369], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6104], requires_grad=True), f(x) = tensor([4.1348], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6090], requires_grad=True), f(x) = tensor([4.1327], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6075], requires_grad=True), f(x) = tensor([4.1307], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6061], requires_grad=True), f(x) = tensor([4.1286], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6047], requires_grad=True), f(x) = tensor([4.1266], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6033], requires_grad=True), f(x) = tensor([4.1246], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6019], requires_grad=True), f(x) = tensor([4.1226], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.6005], requires_grad=True), f(x) = tensor([4.1206], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5991], requires_grad=True), f(x) = tensor([4.1187], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5977], requires_grad=True), f(x) = tensor([4.1167], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5963], requires_grad=True), f(x) = tensor([4.1148], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5949], requires_grad=True), f(x) = tensor([4.1129], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5935], requires_grad=True), f(x) = tensor([4.1110], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5921], requires_grad=True), f(x) = tensor([4.1091], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5908], requires_grad=True), f(x) = tensor([4.1072], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5894], requires_grad=True), f(x) = tensor([4.1054], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5880], requires_grad=True), f(x) = tensor([4.1035], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5867], requires_grad=True), f(x) = tensor([4.1017], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5853], requires_grad=True), f(x) = tensor([4.0999], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5840], requires_grad=True), f(x) = tensor([4.0981], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5827], requires_grad=True), f(x) = tensor([4.0963], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5813], requires_grad=True), f(x) = tensor([4.0946], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5800], requires_grad=True), f(x) = tensor([4.0928], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5787], requires_grad=True), f(x) = tensor([4.0911], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5774], requires_grad=True), f(x) = tensor([4.0893], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5761], requires_grad=True), f(x) = tensor([4.0876], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5748], requires_grad=True), f(x) = tensor([4.0859], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5735], requires_grad=True), f(x) = tensor([4.0843], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5722], requires_grad=True), f(x) = tensor([4.0826], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5709], requires_grad=True), f(x) = tensor([4.0809], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5696], requires_grad=True), f(x) = tensor([4.0793], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5683], requires_grad=True), f(x) = tensor([4.0776], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5670], requires_grad=True), f(x) = tensor([4.0760], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5658], requires_grad=True), f(x) = tensor([4.0744], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5645], requires_grad=True), f(x) = tensor([4.0728], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5633], requires_grad=True), f(x) = tensor([4.0713], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5620], requires_grad=True), f(x) = tensor([4.0697], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5608], requires_grad=True), f(x) = tensor([4.0681], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5595], requires_grad=True), f(x) = tensor([4.0666], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5583], requires_grad=True), f(x) = tensor([4.0651], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5570], requires_grad=True), f(x) = tensor([4.0635], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5558], requires_grad=True), f(x) = tensor([4.0620], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5546], requires_grad=True), f(x) = tensor([4.0605], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5534], requires_grad=True), f(x) = tensor([4.0591], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5522], requires_grad=True), f(x) = tensor([4.0576], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5509], requires_grad=True), f(x) = tensor([4.0561], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5497], requires_grad=True), f(x) = tensor([4.0547], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5485], requires_grad=True), f(x) = tensor([4.0533], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5473], requires_grad=True), f(x) = tensor([4.0518], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5462], requires_grad=True), f(x) = tensor([4.0504], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5450], requires_grad=True), f(x) = tensor([4.0490], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5438], requires_grad=True), f(x) = tensor([4.0476], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5426], requires_grad=True), f(x) = tensor([4.0463], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5414], requires_grad=True), f(x) = tensor([4.0449], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5403], requires_grad=True), f(x) = tensor([4.0435], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5391], requires_grad=True), f(x) = tensor([4.0422], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5380], requires_grad=True), f(x) = tensor([4.0408], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5368], requires_grad=True), f(x) = tensor([4.0395], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5357], requires_grad=True), f(x) = tensor([4.0382], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5345], requires_grad=True), f(x) = tensor([4.0369], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5334], requires_grad=True), f(x) = tensor([4.0356], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5323], requires_grad=True), f(x) = tensor([4.0343], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5311], requires_grad=True), f(x) = tensor([4.0331], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5300], requires_grad=True), f(x) = tensor([4.0318], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5289], requires_grad=True), f(x) = tensor([4.0305], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5278], requires_grad=True), f(x) = tensor([4.0293], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5267], requires_grad=True), f(x) = tensor([4.0281], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5255], requires_grad=True), f(x) = tensor([4.0268], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5244], requires_grad=True), f(x) = tensor([4.0256], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5233], requires_grad=True), f(x) = tensor([4.0244], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5223], requires_grad=True), f(x) = tensor([4.0232], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5212], requires_grad=True), f(x) = tensor([4.0221], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5201], requires_grad=True), f(x) = tensor([4.0209], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5190], requires_grad=True), f(x) = tensor([4.0197], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5179], requires_grad=True), f(x) = tensor([4.0186], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5168], requires_grad=True), f(x) = tensor([4.0174], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5158], requires_grad=True), f(x) = tensor([4.0163], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5147], requires_grad=True), f(x) = tensor([4.0152], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5137], requires_grad=True), f(x) = tensor([4.0140], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5126], requires_grad=True), f(x) = tensor([4.0129], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5116], requires_grad=True), f(x) = tensor([4.0118], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5105], requires_grad=True), f(x) = tensor([4.0107], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5095], requires_grad=True), f(x) = tensor([4.0096], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5084], requires_grad=True), f(x) = tensor([4.0086], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5074], requires_grad=True), f(x) = tensor([4.0075], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5064], requires_grad=True), f(x) = tensor([4.0064], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5053], requires_grad=True), f(x) = tensor([4.0054], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5043], requires_grad=True), f(x) = tensor([4.0044], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5033], requires_grad=True), f(x) = tensor([4.0033], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5023], requires_grad=True), f(x) = tensor([4.0023], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5013], requires_grad=True), f(x) = tensor([4.0013], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.5003], requires_grad=True), f(x) = tensor([4.0003], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4993], requires_grad=True), f(x) = tensor([3.9993], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4983], requires_grad=True), f(x) = tensor([3.9983], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4973], requires_grad=True), f(x) = tensor([3.9973], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4963], requires_grad=True), f(x) = tensor([3.9963], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4953], requires_grad=True), f(x) = tensor([3.9954], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4943], requires_grad=True), f(x) = tensor([3.9944], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4934], requires_grad=True), f(x) = tensor([3.9934], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4924], requires_grad=True), f(x) = tensor([3.9925], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4914], requires_grad=True), f(x) = tensor([3.9916], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4904], requires_grad=True), f(x) = tensor([3.9906], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4895], requires_grad=True), f(x) = tensor([3.9897], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4885], requires_grad=True), f(x) = tensor([3.9888], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4876], requires_grad=True), f(x) = tensor([3.9879], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4866], requires_grad=True), f(x) = tensor([3.9870], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4857], requires_grad=True), f(x) = tensor([3.9861], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4847], requires_grad=True), f(x) = tensor([3.9852], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4838], requires_grad=True), f(x) = tensor([3.9843], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4829], requires_grad=True), f(x) = tensor([3.9834], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4819], requires_grad=True), f(x) = tensor([3.9826], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4810], requires_grad=True), f(x) = tensor([3.9817], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4801], requires_grad=True), f(x) = tensor([3.9809], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4792], requires_grad=True), f(x) = tensor([3.9800], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4782], requires_grad=True), f(x) = tensor([3.9792], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4773], requires_grad=True), f(x) = tensor([3.9783], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4764], requires_grad=True), f(x) = tensor([3.9775], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4755], requires_grad=True), f(x) = tensor([3.9767], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4746], requires_grad=True), f(x) = tensor([3.9759], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4737], requires_grad=True), f(x) = tensor([3.9751], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4728], requires_grad=True), f(x) = tensor([3.9743], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4719], requires_grad=True), f(x) = tensor([3.9735], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4710], requires_grad=True), f(x) = tensor([3.9727], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4701], requires_grad=True), f(x) = tensor([3.9719], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4693], requires_grad=True), f(x) = tensor([3.9712], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4684], requires_grad=True), f(x) = tensor([3.9704], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4675], requires_grad=True), f(x) = tensor([3.9696], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4666], requires_grad=True), f(x) = tensor([3.9689], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4658], requires_grad=True), f(x) = tensor([3.9681], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4649], requires_grad=True), f(x) = tensor([3.9674], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4641], requires_grad=True), f(x) = tensor([3.9666], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4632], requires_grad=True), f(x) = tensor([3.9659], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4623], requires_grad=True), f(x) = tensor([3.9652], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4615], requires_grad=True), f(x) = tensor([3.9645], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4607], requires_grad=True), f(x) = tensor([3.9637], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4598], requires_grad=True), f(x) = tensor([3.9630], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4590], requires_grad=True), f(x) = tensor([3.9623], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4581], requires_grad=True), f(x) = tensor([3.9616], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4573], requires_grad=True), f(x) = tensor([3.9609], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4565], requires_grad=True), f(x) = tensor([3.9603], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4556], requires_grad=True), f(x) = tensor([3.9596], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4548], requires_grad=True), f(x) = tensor([3.9589], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4540], requires_grad=True), f(x) = tensor([3.9582], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4532], requires_grad=True), f(x) = tensor([3.9576], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4524], requires_grad=True), f(x) = tensor([3.9569], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4516], requires_grad=True), f(x) = tensor([3.9563], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4508], requires_grad=True), f(x) = tensor([3.9556], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4500], requires_grad=True), f(x) = tensor([3.9550], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4492], requires_grad=True), f(x) = tensor([3.9543], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4484], requires_grad=True), f(x) = tensor([3.9537], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4476], requires_grad=True), f(x) = tensor([3.9531], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4468], requires_grad=True), f(x) = tensor([3.9524], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4460], requires_grad=True), f(x) = tensor([3.9518], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4452], requires_grad=True), f(x) = tensor([3.9512], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4444], requires_grad=True), f(x) = tensor([3.9506], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4436], requires_grad=True), f(x) = tensor([3.9500], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4429], requires_grad=True), f(x) = tensor([3.9494], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4421], requires_grad=True), f(x) = tensor([3.9488], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4413], requires_grad=True), f(x) = tensor([3.9482], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4406], requires_grad=True), f(x) = tensor([3.9476], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4398], requires_grad=True), f(x) = tensor([3.9471], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4390], requires_grad=True), f(x) = tensor([3.9465], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4383], requires_grad=True), f(x) = tensor([3.9459], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4375], requires_grad=True), f(x) = tensor([3.9453], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4368], requires_grad=True), f(x) = tensor([3.9448], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4360], requires_grad=True), f(x) = tensor([3.9442], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4353], requires_grad=True), f(x) = tensor([3.9437], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4346], requires_grad=True), f(x) = tensor([3.9431], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4338], requires_grad=True), f(x) = tensor([3.9426], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4331], requires_grad=True), f(x) = tensor([3.9420], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4323], requires_grad=True), f(x) = tensor([3.9415], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4316], requires_grad=True), f(x) = tensor([3.9410], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4309], requires_grad=True), f(x) = tensor([3.9404], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4302], requires_grad=True), f(x) = tensor([3.9399], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4294], requires_grad=True), f(x) = tensor([3.9394], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4287], requires_grad=True), f(x) = tensor([3.9389], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4280], requires_grad=True), f(x) = tensor([3.9384], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4273], requires_grad=True), f(x) = tensor([3.9379], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4266], requires_grad=True), f(x) = tensor([3.9374], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4259], requires_grad=True), f(x) = tensor([3.9369], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4252], requires_grad=True), f(x) = tensor([3.9364], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4245], requires_grad=True), f(x) = tensor([3.9359], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4238], requires_grad=True), f(x) = tensor([3.9354], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4231], requires_grad=True), f(x) = tensor([3.9349], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4224], requires_grad=True), f(x) = tensor([3.9344], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4217], requires_grad=True), f(x) = tensor([3.9340], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4210], requires_grad=True), f(x) = tensor([3.9335], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4203], requires_grad=True), f(x) = tensor([3.9330], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4197], requires_grad=True), f(x) = tensor([3.9326], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4190], requires_grad=True), f(x) = tensor([3.9321], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4183], requires_grad=True), f(x) = tensor([3.9317], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4176], requires_grad=True), f(x) = tensor([3.9312], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4170], requires_grad=True), f(x) = tensor([3.9308], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4163], requires_grad=True), f(x) = tensor([3.9303], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4156], requires_grad=True), f(x) = tensor([3.9299], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4150], requires_grad=True), f(x) = tensor([3.9294], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4143], requires_grad=True), f(x) = tensor([3.9290], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4136], requires_grad=True), f(x) = tensor([3.9286], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4130], requires_grad=True), f(x) = tensor([3.9281], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4123], requires_grad=True), f(x) = tensor([3.9277], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4117], requires_grad=True), f(x) = tensor([3.9273], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4110], requires_grad=True), f(x) = tensor([3.9269], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4104], requires_grad=True), f(x) = tensor([3.9265], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4098], requires_grad=True), f(x) = tensor([3.9260], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4091], requires_grad=True), f(x) = tensor([3.9256], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4085], requires_grad=True), f(x) = tensor([3.9252], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4078], requires_grad=True), f(x) = tensor([3.9248], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4072], requires_grad=True), f(x) = tensor([3.9244], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4066], requires_grad=True), f(x) = tensor([3.9240], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4060], requires_grad=True), f(x) = tensor([3.9236], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4053], requires_grad=True), f(x) = tensor([3.9233], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4047], requires_grad=True), f(x) = tensor([3.9229], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4041], requires_grad=True), f(x) = tensor([3.9225], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4035], requires_grad=True), f(x) = tensor([3.9221], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4029], requires_grad=True), f(x) = tensor([3.9217], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4023], requires_grad=True), f(x) = tensor([3.9214], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4016], requires_grad=True), f(x) = tensor([3.9210], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4010], requires_grad=True), f(x) = tensor([3.9206], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.4004], requires_grad=True), f(x) = tensor([3.9203], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3998], requires_grad=True), f(x) = tensor([3.9199], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3992], requires_grad=True), f(x) = tensor([3.9195], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3986], requires_grad=True), f(x) = tensor([3.9192], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3980], requires_grad=True), f(x) = tensor([3.9188], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3975], requires_grad=True), f(x) = tensor([3.9185], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3969], requires_grad=True), f(x) = tensor([3.9181], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3963], requires_grad=True), f(x) = tensor([3.9178], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3957], requires_grad=True), f(x) = tensor([3.9175], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3951], requires_grad=True), f(x) = tensor([3.9171], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3945], requires_grad=True), f(x) = tensor([3.9168], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3939], requires_grad=True), f(x) = tensor([3.9164], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3934], requires_grad=True), f(x) = tensor([3.9161], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3928], requires_grad=True), f(x) = tensor([3.9158], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3922], requires_grad=True), f(x) = tensor([3.9155], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3917], requires_grad=True), f(x) = tensor([3.9151], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3911], requires_grad=True), f(x) = tensor([3.9148], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3905], requires_grad=True), f(x) = tensor([3.9145], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3900], requires_grad=True), f(x) = tensor([3.9142], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3894], requires_grad=True), f(x) = tensor([3.9139], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3888], requires_grad=True), f(x) = tensor([3.9136], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3883], requires_grad=True), f(x) = tensor([3.9132], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3877], requires_grad=True), f(x) = tensor([3.9129], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3872], requires_grad=True), f(x) = tensor([3.9126], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3866], requires_grad=True), f(x) = tensor([3.9123], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3861], requires_grad=True), f(x) = tensor([3.9120], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3855], requires_grad=True), f(x) = tensor([3.9117], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3850], requires_grad=True), f(x) = tensor([3.9115], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3845], requires_grad=True), f(x) = tensor([3.9112], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3839], requires_grad=True), f(x) = tensor([3.9109], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3834], requires_grad=True), f(x) = tensor([3.9106], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3829], requires_grad=True), f(x) = tensor([3.9103], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3823], requires_grad=True), f(x) = tensor([3.9100], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3818], requires_grad=True), f(x) = tensor([3.9097], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3813], requires_grad=True), f(x) = tensor([3.9095], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3807], requires_grad=True), f(x) = tensor([3.9092], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3802], requires_grad=True), f(x) = tensor([3.9089], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3797], requires_grad=True), f(x) = tensor([3.9086], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3792], requires_grad=True), f(x) = tensor([3.9084], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3787], requires_grad=True), f(x) = tensor([3.9081], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3782], requires_grad=True), f(x) = tensor([3.9078], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3776], requires_grad=True), f(x) = tensor([3.9076], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3771], requires_grad=True), f(x) = tensor([3.9073], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3766], requires_grad=True), f(x) = tensor([3.9071], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3761], requires_grad=True), f(x) = tensor([3.9068], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3756], requires_grad=True), f(x) = tensor([3.9066], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3751], requires_grad=True), f(x) = tensor([3.9063], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3746], requires_grad=True), f(x) = tensor([3.9061], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3741], requires_grad=True), f(x) = tensor([3.9058], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3736], requires_grad=True), f(x) = tensor([3.9056], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3731], requires_grad=True), f(x) = tensor([3.9053], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3726], requires_grad=True), f(x) = tensor([3.9051], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3721], requires_grad=True), f(x) = tensor([3.9048], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3716], requires_grad=True), f(x) = tensor([3.9046], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3712], requires_grad=True), f(x) = tensor([3.9044], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3707], requires_grad=True), f(x) = tensor([3.9041], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3702], requires_grad=True), f(x) = tensor([3.9039], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3697], requires_grad=True), f(x) = tensor([3.9037], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3692], requires_grad=True), f(x) = tensor([3.9034], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3688], requires_grad=True), f(x) = tensor([3.9032], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3683], requires_grad=True), f(x) = tensor([3.9030], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3678], requires_grad=True), f(x) = tensor([3.9028], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3673], requires_grad=True), f(x) = tensor([3.9025], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3669], requires_grad=True), f(x) = tensor([3.9023], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3664], requires_grad=True), f(x) = tensor([3.9021], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3659], requires_grad=True), f(x) = tensor([3.9019], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3655], requires_grad=True), f(x) = tensor([3.9017], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3650], requires_grad=True), f(x) = tensor([3.9015], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3645], requires_grad=True), f(x) = tensor([3.9012], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3641], requires_grad=True), f(x) = tensor([3.9010], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3636], requires_grad=True), f(x) = tensor([3.9008], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3632], requires_grad=True), f(x) = tensor([3.9006], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3627], requires_grad=True), f(x) = tensor([3.9004], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3623], requires_grad=True), f(x) = tensor([3.9002], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3618], requires_grad=True), f(x) = tensor([3.9000], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3614], requires_grad=True), f(x) = tensor([3.8998], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3609], requires_grad=True), f(x) = tensor([3.8996], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3605], requires_grad=True), f(x) = tensor([3.8994], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3600], requires_grad=True), f(x) = tensor([3.8992], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3596], requires_grad=True), f(x) = tensor([3.8990], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3592], requires_grad=True), f(x) = tensor([3.8988], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3587], requires_grad=True), f(x) = tensor([3.8986], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3583], requires_grad=True), f(x) = tensor([3.8985], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3579], requires_grad=True), f(x) = tensor([3.8983], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3574], requires_grad=True), f(x) = tensor([3.8981], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3570], requires_grad=True), f(x) = tensor([3.8979], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3566], requires_grad=True), f(x) = tensor([3.8977], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3561], requires_grad=True), f(x) = tensor([3.8975], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3557], requires_grad=True), f(x) = tensor([3.8974], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3553], requires_grad=True), f(x) = tensor([3.8972], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3549], requires_grad=True), f(x) = tensor([3.8970], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3545], requires_grad=True), f(x) = tensor([3.8968], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3540], requires_grad=True), f(x) = tensor([3.8966], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3536], requires_grad=True), f(x) = tensor([3.8965], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3532], requires_grad=True), f(x) = tensor([3.8963], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3528], requires_grad=True), f(x) = tensor([3.8961], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3524], requires_grad=True), f(x) = tensor([3.8960], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3520], requires_grad=True), f(x) = tensor([3.8958], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3516], requires_grad=True), f(x) = tensor([3.8956], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3512], requires_grad=True), f(x) = tensor([3.8955], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3508], requires_grad=True), f(x) = tensor([3.8953], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3504], requires_grad=True), f(x) = tensor([3.8951], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3500], requires_grad=True), f(x) = tensor([3.8950], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3496], requires_grad=True), f(x) = tensor([3.8948], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3492], requires_grad=True), f(x) = tensor([3.8947], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3488], requires_grad=True), f(x) = tensor([3.8945], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3484], requires_grad=True), f(x) = tensor([3.8944], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3480], requires_grad=True), f(x) = tensor([3.8942], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3476], requires_grad=True), f(x) = tensor([3.8940], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3472], requires_grad=True), f(x) = tensor([3.8939], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3468], requires_grad=True), f(x) = tensor([3.8937], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3464], requires_grad=True), f(x) = tensor([3.8936], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3460], requires_grad=True), f(x) = tensor([3.8934], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3456], requires_grad=True), f(x) = tensor([3.8933], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3453], requires_grad=True), f(x) = tensor([3.8931], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3449], requires_grad=True), f(x) = tensor([3.8930], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3445], requires_grad=True), f(x) = tensor([3.8929], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3441], requires_grad=True), f(x) = tensor([3.8927], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3437], requires_grad=True), f(x) = tensor([3.8926], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3434], requires_grad=True), f(x) = tensor([3.8924], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3430], requires_grad=True), f(x) = tensor([3.8923], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3426], requires_grad=True), f(x) = tensor([3.8922], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3423], requires_grad=True), f(x) = tensor([3.8920], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3419], requires_grad=True), f(x) = tensor([3.8919], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3415], requires_grad=True), f(x) = tensor([3.8918], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3412], requires_grad=True), f(x) = tensor([3.8916], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3408], requires_grad=True), f(x) = tensor([3.8915], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3404], requires_grad=True), f(x) = tensor([3.8914], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3401], requires_grad=True), f(x) = tensor([3.8912], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3397], requires_grad=True), f(x) = tensor([3.8911], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3393], requires_grad=True), f(x) = tensor([3.8910], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3390], requires_grad=True), f(x) = tensor([3.8908], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3386], requires_grad=True), f(x) = tensor([3.8907], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3383], requires_grad=True), f(x) = tensor([3.8906], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3379], requires_grad=True), f(x) = tensor([3.8905], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3376], requires_grad=True), f(x) = tensor([3.8903], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3372], requires_grad=True), f(x) = tensor([3.8902], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3369], requires_grad=True), f(x) = tensor([3.8901], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3365], requires_grad=True), f(x) = tensor([3.8900], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3362], requires_grad=True), f(x) = tensor([3.8899], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3358], requires_grad=True), f(x) = tensor([3.8897], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3355], requires_grad=True), f(x) = tensor([3.8896], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3351], requires_grad=True), f(x) = tensor([3.8895], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3348], requires_grad=True), f(x) = tensor([3.8894], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3345], requires_grad=True), f(x) = tensor([3.8893], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3341], requires_grad=True), f(x) = tensor([3.8892], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3338], requires_grad=True), f(x) = tensor([3.8890], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3335], requires_grad=True), f(x) = tensor([3.8889], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3331], requires_grad=True), f(x) = tensor([3.8888], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3328], requires_grad=True), f(x) = tensor([3.8887], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3325], requires_grad=True), f(x) = tensor([3.8886], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3321], requires_grad=True), f(x) = tensor([3.8885], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3318], requires_grad=True), f(x) = tensor([3.8884], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3315], requires_grad=True), f(x) = tensor([3.8883], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3312], requires_grad=True), f(x) = tensor([3.8882], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3308], requires_grad=True), f(x) = tensor([3.8881], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3305], requires_grad=True), f(x) = tensor([3.8880], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3302], requires_grad=True), f(x) = tensor([3.8879], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3299], requires_grad=True), f(x) = tensor([3.8878], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3295], requires_grad=True), f(x) = tensor([3.8877], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3292], requires_grad=True), f(x) = tensor([3.8876], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3289], requires_grad=True), f(x) = tensor([3.8875], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3286], requires_grad=True), f(x) = tensor([3.8874], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3283], requires_grad=True), f(x) = tensor([3.8873], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3280], requires_grad=True), f(x) = tensor([3.8872], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3277], requires_grad=True), f(x) = tensor([3.8871], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3273], requires_grad=True), f(x) = tensor([3.8870], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3270], requires_grad=True), f(x) = tensor([3.8869], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3267], requires_grad=True), f(x) = tensor([3.8868], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3264], requires_grad=True), f(x) = tensor([3.8867], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3261], requires_grad=True), f(x) = tensor([3.8866], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3258], requires_grad=True), f(x) = tensor([3.8865], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3255], requires_grad=True), f(x) = tensor([3.8864], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3252], requires_grad=True), f(x) = tensor([3.8863], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3249], requires_grad=True), f(x) = tensor([3.8862], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3246], requires_grad=True), f(x) = tensor([3.8861], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3243], requires_grad=True), f(x) = tensor([3.8860], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3240], requires_grad=True), f(x) = tensor([3.8860], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3237], requires_grad=True), f(x) = tensor([3.8859], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3234], requires_grad=True), f(x) = tensor([3.8858], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3231], requires_grad=True), f(x) = tensor([3.8857], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3228], requires_grad=True), f(x) = tensor([3.8856], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3225], requires_grad=True), f(x) = tensor([3.8855], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3222], requires_grad=True), f(x) = tensor([3.8854], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3220], requires_grad=True), f(x) = tensor([3.8854], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3217], requires_grad=True), f(x) = tensor([3.8853], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3214], requires_grad=True), f(x) = tensor([3.8852], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3211], requires_grad=True), f(x) = tensor([3.8851], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3208], requires_grad=True), f(x) = tensor([3.8850], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3205], requires_grad=True), f(x) = tensor([3.8849], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3202], requires_grad=True), f(x) = tensor([3.8849], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3200], requires_grad=True), f(x) = tensor([3.8848], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3197], requires_grad=True), f(x) = tensor([3.8847], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3194], requires_grad=True), f(x) = tensor([3.8846], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3191], requires_grad=True), f(x) = tensor([3.8846], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3189], requires_grad=True), f(x) = tensor([3.8845], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3186], requires_grad=True), f(x) = tensor([3.8844], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3183], requires_grad=True), f(x) = tensor([3.8843], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3180], requires_grad=True), f(x) = tensor([3.8843], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3178], requires_grad=True), f(x) = tensor([3.8842], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3175], requires_grad=True), f(x) = tensor([3.8841], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3172], requires_grad=True), f(x) = tensor([3.8840], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3169], requires_grad=True), f(x) = tensor([3.8840], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3167], requires_grad=True), f(x) = tensor([3.8839], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3164], requires_grad=True), f(x) = tensor([3.8838], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3161], requires_grad=True), f(x) = tensor([3.8838], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3159], requires_grad=True), f(x) = tensor([3.8837], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3156], requires_grad=True), f(x) = tensor([3.8836], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3154], requires_grad=True), f(x) = tensor([3.8835], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3151], requires_grad=True), f(x) = tensor([3.8835], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3148], requires_grad=True), f(x) = tensor([3.8834], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3146], requires_grad=True), f(x) = tensor([3.8833], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3143], requires_grad=True), f(x) = tensor([3.8833], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3141], requires_grad=True), f(x) = tensor([3.8832], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3138], requires_grad=True), f(x) = tensor([3.8831], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3135], requires_grad=True), f(x) = tensor([3.8831], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3133], requires_grad=True), f(x) = tensor([3.8830], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3130], requires_grad=True), f(x) = tensor([3.8829], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3128], requires_grad=True), f(x) = tensor([3.8829], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3125], requires_grad=True), f(x) = tensor([3.8828], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3123], requires_grad=True), f(x) = tensor([3.8828], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3120], requires_grad=True), f(x) = tensor([3.8827], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3118], requires_grad=True), f(x) = tensor([3.8826], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3115], requires_grad=True), f(x) = tensor([3.8826], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3113], requires_grad=True), f(x) = tensor([3.8825], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3111], requires_grad=True), f(x) = tensor([3.8825], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3108], requires_grad=True), f(x) = tensor([3.8824], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3106], requires_grad=True), f(x) = tensor([3.8823], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3103], requires_grad=True), f(x) = tensor([3.8823], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3101], requires_grad=True), f(x) = tensor([3.8822], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3098], requires_grad=True), f(x) = tensor([3.8822], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3096], requires_grad=True), f(x) = tensor([3.8821], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3094], requires_grad=True), f(x) = tensor([3.8820], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3091], requires_grad=True), f(x) = tensor([3.8820], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3089], requires_grad=True), f(x) = tensor([3.8819], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3087], requires_grad=True), f(x) = tensor([3.8819], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3084], requires_grad=True), f(x) = tensor([3.8818], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3082], requires_grad=True), f(x) = tensor([3.8818], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3080], requires_grad=True), f(x) = tensor([3.8817], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3077], requires_grad=True), f(x) = tensor([3.8817], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3075], requires_grad=True), f(x) = tensor([3.8816], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3073], requires_grad=True), f(x) = tensor([3.8816], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3070], requires_grad=True), f(x) = tensor([3.8815], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3068], requires_grad=True), f(x) = tensor([3.8815], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3066], requires_grad=True), f(x) = tensor([3.8814], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3063], requires_grad=True), f(x) = tensor([3.8814], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3061], requires_grad=True), f(x) = tensor([3.8813], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3059], requires_grad=True), f(x) = tensor([3.8812], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3057], requires_grad=True), f(x) = tensor([3.8812], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3055], requires_grad=True), f(x) = tensor([3.8812], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3052], requires_grad=True), f(x) = tensor([3.8811], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3050], requires_grad=True), f(x) = tensor([3.8811], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3048], requires_grad=True), f(x) = tensor([3.8810], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3046], requires_grad=True), f(x) = tensor([3.8810], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3044], requires_grad=True), f(x) = tensor([3.8809], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3041], requires_grad=True), f(x) = tensor([3.8809], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3039], requires_grad=True), f(x) = tensor([3.8808], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3037], requires_grad=True), f(x) = tensor([3.8808], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3035], requires_grad=True), f(x) = tensor([3.8807], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3033], requires_grad=True), f(x) = tensor([3.8807], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3031], requires_grad=True), f(x) = tensor([3.8806], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3028], requires_grad=True), f(x) = tensor([3.8806], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3026], requires_grad=True), f(x) = tensor([3.8805], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3024], requires_grad=True), f(x) = tensor([3.8805], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3022], requires_grad=True), f(x) = tensor([3.8805], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3020], requires_grad=True), f(x) = tensor([3.8804], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3018], requires_grad=True), f(x) = tensor([3.8804], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3016], requires_grad=True), f(x) = tensor([3.8803], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3014], requires_grad=True), f(x) = tensor([3.8803], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3012], requires_grad=True), f(x) = tensor([3.8802], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3010], requires_grad=True), f(x) = tensor([3.8802], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3008], requires_grad=True), f(x) = tensor([3.8802], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3006], requires_grad=True), f(x) = tensor([3.8801], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3004], requires_grad=True), f(x) = tensor([3.8801], grad_fn=<AddBackward0>)\n",
            "X = tensor([-0.3002], requires_grad=True), f(x) = tensor([3.8800], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f04a4f6f390>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTAAwWN2DWEH"
      },
      "source": [
        "# c) Implementando un MLP en PyTorch para predicción de precios de inmuebles \n",
        "\n",
        "Contamos con una base de datos de 506 precios de inmuebles de la ciudad de Boston [1]. Cada inmueble está descripto por diversas características como el indice de crimen per capita en la zona, o el grado de accesibilidad a autopistas, etc. Se cuenta con el precio de cada uno, y se pretende desarrollar un módulo que permita predecir dicho precio a partir de las características.\n",
        "\n",
        "[1]: *Hedonic prices and the demand for clean air*, J. Environ. Economics & Management, vol.5, 81-102, 1978.\n",
        "\n",
        "Primero, vamos a generar un histograma de los precios con todos los datos disponibles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D-sOjKKSdmp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "ba0e2bcb-5874-4ebe-8dec-6db68bdf979f"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Importamos el dataset\n",
        "\n",
        "dataset_boston = load_boston()\n",
        "\n",
        "print(\"El dataset contiene \" + str(dataset_boston.keys()) + \" \\n\\n\")\n",
        "\n",
        "# Extraigo los datos (features) y los precios (etiquetas a predecir)\n",
        "data = dataset_boston['data']\n",
        "data = data.astype(np.float32)\n",
        "precios = np.expand_dims(dataset_boston['target'], axis=1).astype(np.float32)\n",
        "\n",
        "print(\"Fila de ejemplo:\")\n",
        "print(dataset_boston['feature_names'])\n",
        "print(data[0,:])\n",
        "\n",
        "# Dibujo un histograma de los precios de los inmuebles\n",
        "_ = plt.hist(precios, 50, density=True, facecolor='g', alpha=0.75)\n",
        "_ = plt.title(\"Precios\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El dataset contiene dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename']) \n",
            "\n",
            "\n",
            "Fila de ejemplo:\n",
            "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
            " 'B' 'LSTAT']\n",
            "[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
            " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgc2qJKLmhsB"
      },
      "source": [
        "Particionamos los datos en entrenamiento y prueba usando la función `sklearn.model_selection.train_test_split`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WWTSCnnVyRK"
      },
      "source": [
        " from sklearn.model_selection import train_test_split\n",
        " \n",
        " # Particiono los datos en entrenamiento y prueba usando el método de scikitlearn\n",
        " X_train, X_test, y_train, y_test = train_test_split(data, precios, test_size=0.33, random_state=42)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_D3sJMCoB_K"
      },
      "source": [
        "Ahora implementaremos un Perceptrón multicapa que usaremos para como regresor del precio utilizando PyTorch (ejemplo basado en el curso de [RPI](https://rpi.analyticsdojo.com/)).\n",
        "\n",
        "El perceptrón deberá contar con 3 capas:\n",
        "- Las dos primeras con 100 neuronas, y deberán usar la función de activación ReLU.\n",
        "- La última con una única neurona cuya salida sea un valor escalar que corresponda al precio estimado del inmueble, que no deberá utilizar ninguna función de activación.\n",
        "\n",
        "Algunas clases de PyTorch que resultarán útiles para implementar el modelo, son:\n",
        "- `torch.nn.Linear`: Implementa una capa totalmente conectada. Es necesario especificarle el número de parámetros de entrada y de salida.\n",
        "- `torch.nn.functional.relu`: Implementa la función de activación ReLU.\n",
        "\n",
        "Además, utilizaremos el optimizador `torch.optim.Adam` y la función de pérdida `torch.nn.MSELoss` (error cuadrático medio).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91rAzYsjkAUa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77cbb350-c08f-4386-bb9c-c6f725742cb9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Tamaño del batch de entrenamiento\n",
        "batch_size = 50\n",
        "\n",
        "# Tasa de aprendizaje inicial para el gradiente descendente\n",
        "learning_rate = 0.00001\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_features, size_hidden, n_output):\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden1 = .....\n",
        "        self.hidden2 = .....\n",
        "        self.out = .....\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = .....      # activation function for hidden layer\n",
        "                     # linear output\n",
        "        return x\n",
        "\n",
        "# Definimos el modelo del perceptrón\n",
        "net = Net( .... , ..... , ......)\n",
        "\n",
        "# Construimos el optimizador, y le indicamos que los parámetros a optimizar \n",
        "# son los del modelo definido: net.parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam( ...... , lr=learning_rate)\n",
        "\n",
        "# Definimos también la función de pérdida a utilizar\n",
        "criterion = .....\n",
        "\n",
        "# Creamos el objeto dataset que empaqueta los array de numpy para que puedan \n",
        "# ser leidos por PyTorch\n",
        "dataset = TensorDataset(torch.from_numpy(X_train).clone(), torch.from_numpy(y_train).clone())\n",
        "\n",
        "# Creamos un loader iterable indicandole que debe leer los datos a partir de\n",
        "# del dataset creado en el paso anterior. Este objeto puede ser iterado\n",
        "# y nos devuelve de a un batch (x, y).\n",
        "loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Número de épocas\n",
        "num_epochs = 5000\n",
        "\n",
        "# Lista en la que iremos guardando el valor de la función de pérdida en cada \n",
        "# etapa de entrenamiento\n",
        "loss_list = []\n",
        "\n",
        "# Bucle de entrenamiento\n",
        "for i in range(num_epochs):\n",
        "\n",
        "    # Itero sobre todos los batches del dataset\n",
        "    for x, y in loader:\n",
        "        # Seteo en cero los gradientes de los parámetros a optimizar\n",
        "        optimizer.......\n",
        "\n",
        "        # Realizo la pasada forward computando la loss entre la salida de la red `net(x)` y las etiquetas `y`\n",
        "        loss = .......\n",
        "        \n",
        "        # Realizo la pasada backward por la red        \n",
        "        loss......\n",
        "        \n",
        "        # Actualizo los pesos de la red con el optimizador\n",
        "        optimizer.......\n",
        "\n",
        "        # Me guardo el valor actual de la función de pérdida para luego graficarlo\n",
        "        loss_list.append(loss.data.item())\n",
        "\n",
        "    # Muestro el valor de la función de pérdida cada 100 iteraciones        \n",
        "    if i > 0 and i % 100 == 0:\n",
        "        print('Epoch %d, loss = %g' % (i, loss))\n",
        "\n",
        "# Muestro la lista que contiene los valores de la función de pérdida\n",
        "# y una versión suavizada (rojo) para observar la tendencia\n",
        "plt.figure()\n",
        "loss_np_array = np.array(loss_list)\n",
        "plt.plot(loss_np_array, alpha = 0.3)\n",
        "N = 60\n",
        "running_avg_loss = np.convolve(loss_np_array, np.ones((N,))/N, mode='valid')\n",
        "plt.plot(running_avg_loss, color='red')\n",
        "plt.title(\"Función de pérdida durante el entrenamiento\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 100, loss = 923.106\n",
            "Epoch 200, loss = 715.819\n",
            "Epoch 300, loss = 879.679\n",
            "Epoch 400, loss = 725.334\n",
            "Epoch 500, loss = 673.847\n",
            "Epoch 600, loss = 347.52\n",
            "Epoch 700, loss = 527.921\n",
            "Epoch 800, loss = 283.484\n",
            "Epoch 900, loss = 529.495\n",
            "Epoch 1000, loss = 401.049\n",
            "Epoch 1100, loss = 201.275\n",
            "Epoch 1200, loss = 118.789\n",
            "Epoch 1300, loss = 226.777\n",
            "Epoch 1400, loss = 87.881\n",
            "Epoch 1500, loss = 63.0272\n",
            "Epoch 1600, loss = 100.337\n",
            "Epoch 1700, loss = 65.8468\n",
            "Epoch 1800, loss = 129.8\n",
            "Epoch 1900, loss = 46.6197\n",
            "Epoch 2000, loss = 95.6299\n",
            "Epoch 2100, loss = 60.0859\n",
            "Epoch 2200, loss = 37.3812\n",
            "Epoch 2300, loss = 63.4171\n",
            "Epoch 2400, loss = 123.206\n",
            "Epoch 2500, loss = 67.7605\n",
            "Epoch 2600, loss = 74.0231\n",
            "Epoch 2700, loss = 82.4982\n",
            "Epoch 2800, loss = 40.9975\n",
            "Epoch 2900, loss = 49.8836\n",
            "Epoch 3000, loss = 85.0704\n",
            "Epoch 3100, loss = 54.7555\n",
            "Epoch 3200, loss = 67.2247\n",
            "Epoch 3300, loss = 58.443\n",
            "Epoch 3400, loss = 121.901\n",
            "Epoch 3500, loss = 123.038\n",
            "Epoch 3600, loss = 59.1262\n",
            "Epoch 3700, loss = 100.994\n",
            "Epoch 3800, loss = 52.6257\n",
            "Epoch 3900, loss = 62.6242\n",
            "Epoch 4000, loss = 96.3365\n",
            "Epoch 4100, loss = 99.842\n",
            "Epoch 4200, loss = 37.2965\n",
            "Epoch 4300, loss = 64.553\n",
            "Epoch 4400, loss = 70.2143\n",
            "Epoch 4500, loss = 83.4168\n",
            "Epoch 4600, loss = 64.7246\n",
            "Epoch 4700, loss = 30.0104\n",
            "Epoch 4800, loss = 62.4811\n",
            "Epoch 4900, loss = 63.7695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Función de pérdida durante el entrenamiento')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhL0-b9J6eoD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "d17361f5-6986-4868-bf28-5f1ebb143172"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Definimos un método para mostrar las predicciones como un scatter plot \n",
        "# y graficamos la recta de regresión para esos datos.\n",
        "\n",
        "def plotScatter(x_data, y_data, title, fit_line=True):\n",
        "  plt.figure()\n",
        "  \n",
        "  plt.plot(x_data, y_data, '+')\n",
        "  plt.xlabel('Valor real')\n",
        "  plt.ylabel('Predicción')\n",
        "  plt.ylim((0,50))\n",
        "  plt.xlim((0,50))\n",
        "  plt.title(title)\n",
        "\n",
        "  if fit_line:\n",
        "    X, Y = x_data.reshape(-1,1), y_data.reshape(-1,1)\n",
        "    plt.plot( X, LinearRegression().fit(X, Y).predict(X) )\n",
        "\n",
        "# Dibujamos el ground truth vs las predicciones en los datos de entrenamiento\n",
        "py = net(torch.FloatTensor(X_train))\n",
        "y_pred_train = py.cpu().detach().numpy()\n",
        "plotScatter(y_train, y_pred_train, \"Training data\")\n",
        "\n",
        "# Dibujamos el ground truth vs las predicciones en los datos de test\n",
        "py = net(torch.FloatTensor(X_test))\n",
        "y_pred_test = py.cpu().detach().numpy()\n",
        "plotScatter(y_test, y_pred_test, \"Test data\")\n",
        "\n",
        "print (\"MSE medio en training: \" + str(((y_train - y_pred_train)**2).mean()))\n",
        "print (\"MSE medio en test: \" + str(((y_test - y_pred_test)**2).mean()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE medio en training: 75.63646254973968\n",
            "MSE medio en test: 64.81397696010436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc8fFGSm-D99"
      },
      "source": [
        "# Entregable \n",
        "1. Encontrar el mínimo de la función *f* definida en el apartado b). Para ello, deberán encontrar primero la derivada *f'(x)* de forma analítica, y utilizarla para computar el mínimo de la función. Posteriormente, deberán corrobarar que el valor coincida con el que obtuvieron optimizando la función con gradiente descendiente. \n",
        "\n",
        "2. Compara el rendimiento de 3 perceptrones multicapa que varíen en la cantidad de neuronas en sus capas intermedia. Probar colocando 2, 10 y 200 neuronas en dichas capas, al entrenar los perceptrones durante 5000 épocas. Mostrar los resultados utilizando:\n",
        "\n",
        "* los gráficos de dispersión con la recta de regresión\n",
        "* el error medio en los datos de entrenamiento y test\n",
        "\n",
        "  Analizar la relación entre dichos resultados y la cantidad de neuronas que posee el perceptrón.\n",
        " "
      ]
    }
  ]
}